{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/joyce-dubliners-similes-analysis/blob/main/04_nlp_validation_joyce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# POS Tagging Validation on Joyce’s *Dubliners* with a BNC Comparator\n",
        "\n",
        "This study evaluates contemporary POS taggers against expert CLAWS7 annotations for simile-bearing sentences in James Joyce’s *Dubliners*, and incorporates a 200-sentence sample from the British National Corpus (BNC) as a comparative reference. Gold labels are projected to Penn-style tags under a strict CLAWS→Penn mapping to ensure label comparability across tools.\n",
        "\n",
        "## Research Objectives\n",
        "- Quantify token-level performance of spaCy (sm, lg), Flair, and NLTK against CLAWS7 gold under **strict** CLAWS→Penn projection.\n",
        "- Characterize systematic error patterns (e.g., UNK-induced losses; function-word and verb morphology confusions).\n",
        "- Assess whether performance differences are **Joyce-specific** or reflect a broader ceiling induced by projection.\n",
        "\n",
        "## Corpora\n",
        "- **Dubliners (literary)**: 183 sentences with CLAWS7 gold.\n",
        "- **BNC (reference)**: 200 sentences reconstructed as `Left + Node + Right` (with `Left` possibly empty) to form a single sentence string; CLAWS7 gold provided.\n",
        "- **Total**: 383 sentences. All evaluations report pooled results, with per-dataset confidence intervals provided to disentangle corpus effects.\n",
        "\n",
        "## Preprocessing and Alignment\n",
        "- **Gold normalization**: CLAWS7 tags are projected to Penn under a **strict** policy; distinctions without Penn equivalents are mapped to `UNK`.  \n",
        "- **Per-tool alignment**: For each sentence and tool, comparisons are made over the **per-sentence minimum length** across (gold tokens, tool tokens, tool tags) to avoid spurious misalignments arising from tokenization differences.\n",
        "\n",
        "## Evaluation Protocol\n",
        "- **Primary metrics**: Token accuracy; micro/macro/weighted precision, recall, and F1.  \n",
        "- **Uncertainty & inference**: Wilson score intervals for single-proportion estimates; Newcombe (Wilson) intervals for differences in accuracy; McNemar’s tests with Holm–Bonferroni correction for paired token outcomes; sentence-cluster bootstrap CIs for accuracy differences.\n",
        "\n",
        "## Role of the BNC Comparator\n",
        "The BNC sample functions as a **non-Joycean reference**. If tools perform markedly better on BNC than on *Dubliners*, this supports a **Joyce-specific difficulty** beyond the information loss induced by strict projection. Conversely, comparable ceilings across both corpora indicate that the binding constraint is the **projection itself**, rather than literary idiosyncrasy alone.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# COMPLETE SETUP AND INSTALLATION (NLTK yes, TextBlob no)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "# -q to keep output tidy\n",
        "!pip install -q spacy nltk flair scikit-learn plotly seaborn\n",
        "\n",
        "print(\"\\nDownloading spaCy models...\")\n",
        "# Use python -m to ensure install into the current kernel env\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "print(\"\\nEnsuring NLTK POS tagger resource...\")\n",
        "import nltk\n",
        "\n",
        "def _ensure_nltk_resource(path, downloader_name, verbose=False):\n",
        "    try:\n",
        "        nltk.data.find(path)\n",
        "        return True\n",
        "    except LookupError:\n",
        "        try:\n",
        "            nltk.download(downloader_name, quiet=not verbose)\n",
        "            nltk.data.find(path)  # verify\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "# Ensure POS tagger (try *_eng then classic)\n",
        "tagger_ok = _ensure_nltk_resource('taggers/averaged_perceptron_tagger_eng', 'averaged_perceptron_tagger_eng') or \\\n",
        "            _ensure_nltk_resource('taggers/averaged_perceptron_tagger',     'averaged_perceptron_tagger')\n",
        "\n",
        "print(f\"NLTK averaged_perceptron_tagger available: {'✓' if tagger_ok else '✗'}\")\n",
        "print(\"\\nAll installations attempted. Proceeding to imports...\")\n",
        "\n",
        "# ==============================================================================\n",
        "# IMPORTS AND SETUP\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP libraries\n",
        "import spacy\n",
        "from nltk.tokenize import TreebankWordTokenizer  # avoids punkt/punkt_tab\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Optional: Flair (enabled if import + model load succeed)\n",
        "FLAIR_AVAILABLE = False\n",
        "try:\n",
        "    from flair.data import Sentence\n",
        "    from flair.models import SequenceTagger\n",
        "    FLAIR_AVAILABLE = True\n",
        "    print(\"✓ Flair imported successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Flair not available: {e}\")\n",
        "\n",
        "# Analysis libraries\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from collections import defaultdict, Counter\n",
        "import scipy.stats as stats\n",
        "from math import sqrt\n",
        "\n",
        "# Visualization libraries\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# ==============================================================================\n",
        "# Load NLP models\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\nLoading spaCy models...\")\n",
        "nlp_sm = None\n",
        "nlp_lg = None\n",
        "try:\n",
        "    nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"✓ en_core_web_sm loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to load en_core_web_sm: {e}\")\n",
        "\n",
        "try:\n",
        "    nlp_lg = spacy.load(\"en_core_web_lg\")\n",
        "    print(\"✓ en_core_web_lg loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to load en_core_web_lg: {e}\")\n",
        "\n",
        "# Load Flair model if available\n",
        "flair_tagger = None\n",
        "if FLAIR_AVAILABLE:\n",
        "    try:\n",
        "        print(\"Loading Flair POS tagger (this may take a moment)...\")\n",
        "        flair_tagger = SequenceTagger.load('pos')\n",
        "        print(\"✓ Flair model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to load Flair model: {e}\")\n",
        "        flair_tagger = None\n",
        "        FLAIR_AVAILABLE = False\n",
        "\n",
        "# Final setup summary\n",
        "print(\"\\nSetup Summary:\")\n",
        "print(f\"spaCy sm: {'✓ Available' if nlp_sm is not None else '✗ Not available'}\")\n",
        "print(f\"spaCy lg: {'✓ Available' if nlp_lg is not None else '✗ Not available'}\")\n",
        "print(f\"NLTK averaged_perceptron_tagger: {'✓' if tagger_ok else '✗'} (tokenizer: TreebankWordTokenizer)\")\n",
        "print(f\"Flair: {'✓ Available' if FLAIR_AVAILABLE and flair_tagger is not None else '✗ Not available'}\")\n",
        "print(\"\\nReady to proceed with analysis!\")\n"
      ],
      "metadata": {
        "id": "ftuyS9S31zCk",
        "outputId": "fcdd1ab3-b73e-4b55-8de7-8026dbe8ef95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CLEAN SETUP: CLAWS7 → Penn (STRICT), AUDIT + EVAL\n",
        "# ==============================================================================\n",
        "\n",
        "import sys, re, time, warnings, json\n",
        "warnings.filterwarnings('ignore')\n",
        "from collections import defaultdict, Counter\n",
        "from math import sqrt\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "print(\"Imports loaded.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# NLP libraries (optional ones guarded)\n",
        "# ------------------------------------------------------------------------------\n",
        "import spacy\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Flair is optional\n",
        "try:\n",
        "    from flair.data import Sentence\n",
        "    from flair.models import SequenceTagger\n",
        "    FLAIR_AVAILABLE = True\n",
        "except Exception:\n",
        "    FLAIR_AVAILABLE = False\n",
        "\n",
        "print(\"Loading spaCy models...\")\n",
        "try:\n",
        "    nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "except Exception as e:\n",
        "    print(\"spaCy sm load failed:\", e)\n",
        "    nlp_sm = None\n",
        "\n",
        "try:\n",
        "    nlp_lg = spacy.load(\"en_core_web_lg\")\n",
        "except Exception as e:\n",
        "    print(\"spaCy lg load failed:\", e)\n",
        "    nlp_lg = None\n",
        "\n",
        "flair_tagger = None\n",
        "if FLAIR_AVAILABLE:\n",
        "    try:\n",
        "        print(\"Loading Flair POS tagger...\")\n",
        "        flair_tagger = SequenceTagger.load('pos')\n",
        "    except Exception as e:\n",
        "        print(\"Flair load failed:\", e)\n",
        "        FLAIR_AVAILABLE = False\n",
        "        flair_tagger = None\n",
        "\n",
        "print(\"\\nSetup Summary:\")\n",
        "print(f\"  spaCy sm: {'✓' if nlp_sm else '✗'} | spaCy lg: {'✓' if nlp_lg else '✗'}\")\n",
        "print(f\"  NLTK: ✓ (TreebankWordTokenizer + averaged_perceptron_tagger)\")\n",
        "print(f\"  Flair: {'✓' if FLAIR_AVAILABLE and flair_tagger else '✗'}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# CLAWS7 parsing (from 'word_TAG' sequences)\n",
        "# ------------------------------------------------------------------------------\n",
        "def parse_claws_tags(claws_string):\n",
        "    \"\"\"Parse CLAWS7 format: 'word_TAG word_TAG ...' -> (tokens, tags)\"\"\"\n",
        "    if pd.isna(claws_string) or not str(claws_string).strip():\n",
        "        return [], []\n",
        "    tokens, tags = [], []\n",
        "    for item in str(claws_string).strip().split():\n",
        "        if '_' in item:\n",
        "            word, tag = item.rsplit('_', 1)\n",
        "            tokens.append(word)\n",
        "            tags.append(tag)\n",
        "    return tokens, tags\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# CLAWS7 → Penn mapping (closest base mapping) + STRICT UNTRANSLATABLE\n",
        "# ------------------------------------------------------------------------------\n",
        "_CLAWS_TO_PENN_BASE = {\n",
        "    # Determiners / wh-dets\n",
        "    'AT':'DT','DD':'DT','DDQ':'WDT','DDQGE':'WP$','DDQV':'WDT',\n",
        "    'DA':'DT','DA1':'DT','DA2':'DT','DAR':'JJR','DAT':'JJS',\n",
        "    'DB':'DT','DB2':'DT','DD1':'DT','DD2':'DT',\n",
        "\n",
        "    # Coords / subords / clause markers\n",
        "    'CC':'CC','CCB':'CC','CS':'IN','CSA':'IN','CSN':'IN','CST':'IN','CSW':'IN','BCL':'IN',\n",
        "\n",
        "    # Prepositions (base)\n",
        "    'II':'IN','IF':'IN','IO':'IN','IW':'IN',\n",
        "\n",
        "    # Adjectives\n",
        "    'JJ':'JJ','JJR':'JJR','JJT':'JJS','JK':'JJ',\n",
        "\n",
        "    # Adverbs\n",
        "    'RR':'RB','RRQ':'WRB','RRQV':'WRB','RGR':'RBR','RRT':'RBS','RG':'RB','RGQ':'WRB','RGQV':'WRB',\n",
        "    'REX':'RB','RL':'RB','RP':'RP','RPK':'RP','RA':'RB','RT':'RB',\n",
        "\n",
        "    # Nouns (basic)\n",
        "    'NN':'NN','NN1':'NN','NN2':'NNS','NNO':'NN','NNO2':'NNS',\n",
        "    'NP':'NNP','NP1':'NNP','NP2':'NNPS',\n",
        "\n",
        "    # Proper noun semantic subclasses (months/weekdays)\n",
        "    'NPM1':'NNP','NPM2':'NNPS','NPD1':'NNP','NPD2':'NNPS',\n",
        "\n",
        "    # Semantic noun subclasses (mapped loosely; strict will UNK them)\n",
        "    'ND1':'NN','NNL1':'NN','NNL2':'NNS','NNT1':'NN','NNT2':'NNS',\n",
        "    'NNU':'NN','NNU1':'NN','NNU2':'NNS','NNA':'NN','NNB':'NN',\n",
        "\n",
        "    # Numerals\n",
        "    'MC':'CD','MC1':'CD','MC2':'CD','MCGE':'CD','MCMC':'CD','MF':'CD','MD':'JJ',\n",
        "\n",
        "    # Pronouns (closest Penn)\n",
        "    'PPGE':'PRP$','PPH1':'PRP','PPHO1':'PRP','PPHO2':'PRP',\n",
        "    'PPHS1':'PRP','PPHS2':'PRP','PPIO1':'PRP','PPIO2':'PRP',\n",
        "    'PPIS1':'PRP','PPIS2':'PRP','PPX1':'PRP','PPX2':'PRP','PPY':'PRP',\n",
        "    'PN':'PRP','PN1':'PRP','PNQO':'WP','PNQS':'WP','PNQV':'WP',\n",
        "\n",
        "    # Verbs: lexical (good matches)\n",
        "    'VV0':'VB','VVD':'VBD','VVG':'VBG','VVI':'VB','VVN':'VBN','VVZ':'VBZ','VVGK':'VBG','VVNK':'VBN',\n",
        "\n",
        "    # Verbs: do/have/be (Penn loses aux identity; strict will UNK these)\n",
        "    'VD0':'VB','VDD':'VBD','VDG':'VBG','VDI':'VB','VDN':'VBN','VDZ':'VBZ',\n",
        "    'VH0':'VB','VHD':'VBD','VHG':'VBG','VHI':'VB','VHN':'VBN','VHZ':'VBZ',\n",
        "    'VB0':'VB','VBDR':'VBD','VBDZ':'VBD','VBG':'VBG','VBI':'VB','VBM':'VBP','VBN':'VBN','VBR':'VBP','VBZ':'VBZ',\n",
        "\n",
        "    # Modals\n",
        "    'VM':'MD','VMK':'MD',\n",
        "\n",
        "    # Other function words\n",
        "    'TO':'TO','UH':'UH','EX':'EX','GE':'POS','XX':'RB',\n",
        "\n",
        "    # Foreign/formula/unclassified/letters\n",
        "    'FW':'FW','FO':'FW','FU':'FW','ZZ1':'NN','ZZ2':'NNS',\n",
        "\n",
        "    # --- Punctuation / brackets / quotes (reduce UNK→. / , noise) ---\n",
        "    '.':'.', ',':',', ':':':', ';':';', '!':'.', '?':'.',\n",
        "    '(': '-LRB-', ')':'-RRB-',\n",
        "    '[':'-LSB-', ']':'-RSB-',\n",
        "    '{':'-LCB-', '}':'-RCB-',\n",
        "    '``':'``', \"''\":\"''\", '\"':\"''\", \"'\":\"''\",\n",
        "}\n",
        "\n",
        "# Ditto tags like II31 → II\n",
        "_DITTO_RE = re.compile(r'^(.*?)(\\d{2,3})$')\n",
        "def _strip_ditto(tag: str) -> str:\n",
        "    m = _DITTO_RE.match(tag or \"\")\n",
        "    return m.group(1) if m else (tag or \"\")\n",
        "\n",
        "# Tags whose CLAWS distinctions Penn CANNOT encode → mark UNK in STRICT mode\n",
        "# (Explicitly include APPGE to document possessive pronoun pre-nominal collapse.)\n",
        "_STRICT_UNTRANSLATABLE = set([\n",
        "    # Articles / number-specific determiners\n",
        "    'AT','AT1','DD1','DD2','DA','DA1','DA2','DAR','DAT','DB','DB2','DDQGE',\n",
        "    # Preposition subtypes\n",
        "    'IF','IO','IW',\n",
        "    # Semantic noun subclasses (temporal/locative/unit/direction/title/weekday/month)\n",
        "    'ND1','NNL1','NNL2','NNT1','NNT2','NNU','NNU1','NNU2','NNA','NNB',\n",
        "    'NPM1','NPM2','NPD1','NPD2',\n",
        "    # Person/number/case-marked pronouns (Penn POS lacks these features)\n",
        "    'APPGE','PPH1','PPHO1','PPHO2','PPHS1','PPHS2','PPIO1','PPIO2','PPIS1','PPIS2','PPX1','PPX2','PPY','PN1','PN',\n",
        "    # Auxiliary identity / catenatives (Penn POS doesn't encode identity/catenative)\n",
        "    'VBM','VBR','VBZ','VBDR','VBDZ','VBG','VBN','VBI','VB0',\n",
        "    'VD0','VDD','VDG','VDI','VDN','VDZ',\n",
        "    'VH0','VHD','VHG','VHI','VHN','VHZ',\n",
        "    'VVGK','VVNK','VMK','RPK','JK',\n",
        "    # Appositional adv marker / formula / unclassified\n",
        "    'REX','FO','FU',\n",
        "])\n",
        "\n",
        "def convert_claws_to_penn(tag: str, strict: bool = True) -> str:\n",
        "    \"\"\"CLAWS7 → Penn. STRICT=True marks Penn-uncapturable distinctions as UNK.\"\"\"\n",
        "    if not tag or not isinstance(tag, str):\n",
        "        return 'UNK'\n",
        "    base = _strip_ditto(tag)\n",
        "    if strict and base in _STRICT_UNTRANSLATABLE:\n",
        "        return 'UNK'\n",
        "    if base in _CLAWS_TO_PENN_BASE:\n",
        "        return _CLAWS_TO_PENN_BASE[base]\n",
        "\n",
        "    # Generic fallbacks (rare)\n",
        "    if base.startswith('NN'):\n",
        "        return 'UNK' if strict else ('NNS' if tag.endswith('2') else 'NN')\n",
        "    if base.startswith('NP'):\n",
        "        return 'UNK' if strict else ('NNPS' if tag.endswith('2') else 'NNP')\n",
        "    if base.startswith('VV'):\n",
        "        if strict:\n",
        "            return 'UNK'\n",
        "        if base in ('VV0','VVI'): return 'VB'\n",
        "        if base == 'VVD': return 'VBD'\n",
        "        if base == 'VVG': return 'VBG'\n",
        "        if base == 'VVN': return 'VBN'\n",
        "        if base == 'VVZ': return 'VBZ'\n",
        "    if base in {'CS','CSA','CSN','CST','CSW','BCL'}:\n",
        "        return 'IN' if not strict else ('IN' if base == 'CS' else 'UNK')\n",
        "    if base in {'CC','CCB'}:\n",
        "        return 'CC'\n",
        "    return 'UNK'\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Diagnostics for unmappable tags\n",
        "# ------------------------------------------------------------------------------\n",
        "def analyze_unmappable_tags(processed_items, strict: bool = True, top_n: int = 30):\n",
        "    counts = Counter(); examples = {}\n",
        "    total = 0; unk_total = 0\n",
        "    for row in processed_items:\n",
        "        claws_tags = row.get('claws_tags', [])\n",
        "        sent = row.get('sentence', '')\n",
        "        for t in claws_tags:\n",
        "            total += 1\n",
        "            base = _strip_ditto(t)\n",
        "            penn = convert_claws_to_penn(base, strict=strict)\n",
        "            if penn == 'UNK':\n",
        "                unk_total += 1\n",
        "                counts[base] += 1\n",
        "                if base not in examples:\n",
        "                    examples[base] = sent[:120] + ('...' if len(sent) > 120 else '')\n",
        "    print(\"=\"*70)\n",
        "    print(f\"CLAWS7 → Penn STRICT={strict}  | Total tags: {total:,} | UNK: {unk_total:,} ({(unk_total/total*100 if total else 0):.1f}%)\")\n",
        "    print(\"- Top UNTRANSLATABLE CLAWS tags -\")\n",
        "    for tag, c in counts.most_common(top_n):\n",
        "        print(f\"{tag:8s} : {c:6d}   e.g. {examples[tag]}\")\n",
        "    print(\"=\"*70)\n",
        "    return {\n",
        "        'total_tags': total,\n",
        "        'unk_tags': unk_total,\n",
        "        'unk_rate': (unk_total/total if total else 0.0),\n",
        "        'counts': counts,\n",
        "        'examples': examples\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Taggers returning Penn tags directly (for fair comparison against STRICT-projected GT)\n",
        "# ------------------------------------------------------------------------------\n",
        "def tag_with_spacy(sentence, model='sm'):\n",
        "    nlp_model = nlp_sm if model == 'sm' else nlp_lg\n",
        "    if nlp_model is None:\n",
        "        return []\n",
        "    doc = nlp_model(sentence)\n",
        "    return [(t.text, t.tag_) for t in doc]  # spaCy Penn-style tags\n",
        "\n",
        "def tag_with_flair(sentence):\n",
        "    if not (FLAIR_AVAILABLE and flair_tagger):\n",
        "        return []\n",
        "    s = Sentence(sentence); flair_tagger.predict(s)\n",
        "    return [(t.text, t.tag) for t in s]  # Penn-style tags\n",
        "\n",
        "_tb_tok = TreebankWordTokenizer()\n",
        "def tag_with_nltk(sentence):\n",
        "    try:\n",
        "        tokens = _tb_tok.tokenize(sentence)\n",
        "    except Exception:\n",
        "        tokens = sentence.split()\n",
        "    pos_tags = pos_tag(tokens)  # averaged_perceptron_tagger(_eng) ensured in setup cell\n",
        "    return [(w, tag) for w, tag in pos_tags]  # Penn-style tags\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Accuracy + stats (STRICT projection on GT)\n",
        "# ------------------------------------------------------------------------------\n",
        "def calculate_accuracy_fair(ground_truth_claws, predicted_penn, tokens, strict=True):\n",
        "    \"\"\"Compute token-level accuracy after STRICT projecting CLAWS gold to Penn.\"\"\"\n",
        "    if not ground_truth_claws or not predicted_penn or not tokens:\n",
        "        return 0.0\n",
        "    m = min(len(ground_truth_claws), len(predicted_penn), len(tokens))\n",
        "    if m == 0:\n",
        "        return 0.0\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=strict) for t in ground_truth_claws[:m]]\n",
        "    pred = predicted_penn[:m]\n",
        "    return sum(1 for i in range(m) if gt_penn[i] == pred[i]) / m\n",
        "\n",
        "def wilson_confidence_interval(successes, trials, confidence=0.95):\n",
        "    if trials == 0:\n",
        "        return 0, 0, 0\n",
        "    p = successes / trials\n",
        "    z = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
        "    denom = 1 + z**2 / trials\n",
        "    centre = (p + z**2 / (2 * trials)) / denom\n",
        "    half = z * sqrt((p * (1 - p) + z**2 / (4 * trials)) / trials) / denom\n",
        "    return p, max(0, centre - half), min(1, centre + half)\n",
        "\n",
        "def proportion_z_test(x1, n1, x2, n2):\n",
        "    p1, p2 = x1/n1, x2/n2\n",
        "    p_pool = (x1+x2)/(n1+n2)\n",
        "    se = sqrt(p_pool*(1-p_pool)*(1/n1+1/n2))\n",
        "    z = (p1-p2)/se\n",
        "    p_value = 2*(1 - stats.norm.cdf(abs(z)))\n",
        "    return z, p_value\n",
        "\n",
        "def cohens_h(p1, p2):\n",
        "    return 2*(np.arcsin(sqrt(p1)) - np.arcsin(sqrt(p2)))\n",
        "\n",
        "print(\"Setup cell ready: mapping, taggers, and metrics defined.\")\n"
      ],
      "metadata": {
        "id": "KX5yohRk3gbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Data upload + processing (handles BNC KWIC: Left/Node/Right + CLAWS)\n",
        "# ------------------------------------------------------------------------------\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\nUpload your Dubliners CSV file:\")\n",
        "dubliners_uploaded = files.upload()\n",
        "print(\"\\nUpload your BNC CSV file:\")\n",
        "bnc_uploaded = files.upload()\n",
        "\n",
        "def _read_csv_any_encoding(filename):\n",
        "    df = None\n",
        "    for enc in ('cp1252','latin1','utf-8'):\n",
        "        try:\n",
        "            df = pd.read_csv(filename, encoding=enc, dtype=str)\n",
        "            break\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "    if df is None:\n",
        "        raise ValueError(f\"Failed to read {filename} with tried encodings.\")\n",
        "    return df\n",
        "\n",
        "def _find_col(df, candidates):\n",
        "    \"\"\"Return first matching column (case-insensitive) or None.\"\"\"\n",
        "    lower_map = {c.lower(): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        if cand in lower_map:\n",
        "            return lower_map[cand]\n",
        "    # also allow substring match like 'left context'\n",
        "    for c in df.columns:\n",
        "        lc = c.lower()\n",
        "        if any(cand in lc for cand in candidates):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def load_dataset_with_kwic(filename, dataset_name, preview=3):\n",
        "    df = _read_csv_any_encoding(filename)\n",
        "    print(f\"{dataset_name}: Loaded {len(df)} rows with columns: {list(df.columns)}\")\n",
        "    try:\n",
        "        print(df.head(preview).to_string(index=False))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Detects CLAWS tag string column (any 'claws' match)\n",
        "    claws_col = None\n",
        "    for c in df.columns:\n",
        "        if 'claws' in c.lower():\n",
        "            claws_col = c\n",
        "            break\n",
        "\n",
        "    # Detects KWIC columns\n",
        "    left_col = _find_col(df, ['left'])\n",
        "    node_col = _find_col(df, ['node'])\n",
        "    right_col = _find_col(df, ['right'])\n",
        "\n",
        "    # Detects sentence/context text columns (for non-KWIC datasets)\n",
        "    sentence_col = _find_col(df, ['sentence','context','text','sent','snippet','content','line','string'])\n",
        "\n",
        "    processed = []\n",
        "\n",
        "    if claws_col and left_col and node_col and right_col:\n",
        "        print(f\"✓ Detected KWIC columns: {left_col} | {node_col} | {right_col} and CLAWS column: {claws_col}\")\n",
        "        subset = df[[left_col, node_col, right_col, claws_col]].dropna(subset=[claws_col])\n",
        "        for _, row in subset.iterrows():\n",
        "            # Reconstructs sentence text from KWIC\n",
        "            L = str(row[left_col]).strip() if pd.notna(row[left_col]) else \"\"\n",
        "            N = str(row[node_col]).strip() if pd.notna(row[node_col]) else \"\"\n",
        "            R = str(row[right_col]).strip() if pd.notna(row[right_col]) else \"\"\n",
        "            sent_text = \" \".join([s for s in [L, N, R] if s]).strip()\n",
        "            # Parse CLAWS 'word_TAG' stream\n",
        "            tokens, tags = parse_claws_tags(row[claws_col])\n",
        "            if tokens and tags and len(tokens) == len(tags):\n",
        "                processed.append({\n",
        "                    'sentence': sent_text if sent_text else \" \".join(tokens),\n",
        "                    'tokens': tokens,\n",
        "                    'claws_tags': tags,\n",
        "                    'dataset': dataset_name\n",
        "                })\n",
        "        print(f\"{dataset_name}: Processed {len(processed)} valid KWIC rows.\")\n",
        "        return processed\n",
        "\n",
        "    # Fallback: sentence + CLAWS on the same row (non-KWIC)\n",
        "    if claws_col and sentence_col:\n",
        "        print(f\"✓ Using sentence column: {sentence_col} and CLAWS column: {claws_col}\")\n",
        "        subset = df[[sentence_col, claws_col]].dropna(subset=[claws_col])\n",
        "        for _, row in subset.iterrows():\n",
        "            tokens, tags = parse_claws_tags(row[claws_col])\n",
        "            if tokens and tags and len(tokens) == len(tags):\n",
        "                processed.append({\n",
        "                    'sentence': row[sentence_col],\n",
        "                    'tokens': tokens,\n",
        "                    'claws_tags': tags,\n",
        "                    'dataset': dataset_name\n",
        "                })\n",
        "        print(f\"{dataset_name}: Processed {len(processed)} valid sentences.\")\n",
        "        return processed\n",
        "\n",
        "    # Last fallback: CLAWS only → reconstruct sentence from tokens\n",
        "    if claws_col:\n",
        "        print(f\"⚠ No text column found; reconstructing sentence from CLAWS tokens. Using: {claws_col}\")\n",
        "        subset = df[[claws_col]].dropna(subset=[claws_col])\n",
        "        for _, row in subset.iterrows():\n",
        "            tokens, tags = parse_claws_tags(row[claws_col])\n",
        "            if tokens and tags and len(tokens) == len(tags):\n",
        "                processed.append({\n",
        "                    'sentence': \" \".join(tokens),\n",
        "                    'tokens': tokens,\n",
        "                    'claws_tags': tags,\n",
        "                    'dataset': dataset_name\n",
        "                })\n",
        "        print(f\"{dataset_name}: Processed {len(processed)} reconstructed sentences.\")\n",
        "        return processed\n",
        "\n",
        "    print(f\"✗ Could not locate CLAWS or usable sentence/KWIC columns in {dataset_name}.\")\n",
        "    return []\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Load both datasets now\n",
        "# ------------------------------------------------------------------------------\n",
        "dubliners_filename = list(dubliners_uploaded.keys())[0]\n",
        "bnc_filename       = list(bnc_uploaded.keys())[0]\n",
        "\n",
        "dubliners_data = load_dataset_with_kwic(dubliners_filename, \"Dubliners\")\n",
        "bnc_data       = load_dataset_with_kwic(bnc_filename, \"BNC\")\n",
        "\n",
        "all_processed_data = dubliners_data + bnc_data\n",
        "\n",
        "print(f\"\\nTotal processed sentences: {len(all_processed_data)}\")\n",
        "print(f\"Dubliners: {len(dubliners_data)} | BNC: {len(bnc_data)}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# STRICT unmappable audit\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\nRunning STRICT unmappable audit (CLAWS distinctions Penn can't encode)...\")\n",
        "strict_report = analyze_unmappable_tags(all_processed_data, strict=True, top_n=40)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Batch POS tagging across tools (Penn output) + STRICT accuracy against GT\n",
        "# ------------------------------------------------------------------------------\n",
        "def process_sentence_with_all_tools(sentence):\n",
        "    tools = {\n",
        "        'spacy_sm': (lambda s: tag_with_spacy(s, 'sm')),\n",
        "        'spacy_lg': (lambda s: tag_with_spacy(s, 'lg')),\n",
        "        'nltk':      tag_with_nltk,\n",
        "    }\n",
        "    if FLAIR_AVAILABLE and flair_tagger:\n",
        "        tools['flair'] = tag_with_flair\n",
        "\n",
        "    results = {}\n",
        "    for name, fn in tools.items():\n",
        "        try:\n",
        "            t0 = time.time()\n",
        "            tagged = fn(sentence)  # list of (token, PennTag)\n",
        "            dt = time.time() - t0\n",
        "            results[name] = {\n",
        "                'tags':   [tag for _, tag in tagged],   # Penn\n",
        "                'tokens': [tok for tok, _ in tagged],\n",
        "                'processing_time': dt\n",
        "            }\n",
        "        except Exception as e:\n",
        "            results[name] = {'error': str(e)}\n",
        "    return results\n",
        "\n",
        "print(\"\\nBatch tagging sentences with available tools...\")\n",
        "expanded_batch_results = []\n",
        "for i, data in enumerate(all_processed_data):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"  Progress: {i}/{len(all_processed_data)}\")\n",
        "        sys.stdout.flush()\n",
        "    tool_results = process_sentence_with_all_tools(data['sentence'])\n",
        "    expanded_batch_results.append({\n",
        "        'sentence': data['sentence'],\n",
        "        'ground_truth': data['claws_tags'],  # CLAWS7 tags\n",
        "        'dataset': data['dataset'],\n",
        "        'tool_results': tool_results\n",
        "    })\n",
        "print(\"Batch tagging complete.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Post-batch sanity summary (coverage + alignment health)\n",
        "# ------------------------------------------------------------------------------\n",
        "tool_names = set()\n",
        "for r in expanded_batch_results:\n",
        "    tool_names.update(r['tool_results'].keys())\n",
        "tool_names = sorted(tool_names)\n",
        "\n",
        "coverage = {t: 0 for t in tool_names}\n",
        "mean_len = {t: [] for t in tool_names}\n",
        "zero_min = {t: 0 for t in tool_names}  # count of cases where min alignment length was 0\n",
        "\n",
        "for r in expanded_batch_results:\n",
        "    gt = r['ground_truth']\n",
        "    for t in tool_names:\n",
        "        tri = r['tool_results'].get(t, {})\n",
        "        if tri and 'error' not in tri:\n",
        "            coverage[t] += 1\n",
        "            pred = tri['tags']\n",
        "            toks = tri['tokens']\n",
        "            mean_len[t].append(len(pred))\n",
        "            m = min(len(gt), len(pred), len(toks))\n",
        "            if m == 0:\n",
        "                zero_min[t] += 1\n",
        "\n",
        "print(\"\\nTagger coverage summary:\")\n",
        "for t in tool_names:\n",
        "    n = coverage[t]\n",
        "    ml = (np.mean(mean_len[t]) if mean_len[t] else 0)\n",
        "    zm = zero_min[t]\n",
        "    print(f\"  {t:10s} : sentences={n:4d} | mean_pred_len={ml:5.1f} | zero_min_align={zm}\")\n"
      ],
      "metadata": {
        "id": "b6GfGp8xgcWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Tool coverage + alignment audit (with optional include filter and per-dataset view)\n",
        "# ==============================================================================\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def audit_tool_coverage(results, include_tools=None):\n",
        "    tools_seen = set()\n",
        "    cov_sentences = Counter()      # overall sentences with non-error result\n",
        "    had_error = Counter()          # overall sentences with 'error'\n",
        "    zero_min = Counter()           # sentences where min alignment length == 0\n",
        "    pred_len_sum = Counter()\n",
        "    pred_len_cnt = Counter()\n",
        "    sample_errors = defaultdict(list)\n",
        "\n",
        "    # Per-dataset breakdown\n",
        "    cov_by_ds = defaultdict(Counter)   # dataset -> tool -> count\n",
        "    err_by_ds = defaultdict(Counter)   # dataset -> tool -> count\n",
        "\n",
        "    for res in results:\n",
        "        gt = res.get('ground_truth', [])\n",
        "        ds = res.get('dataset', 'UNKNOWN')\n",
        "        tri_map = res.get('tool_results', {})\n",
        "\n",
        "        tools_seen.update(tri_map.keys())\n",
        "        for tool, tri in tri_map.items():\n",
        "            if include_tools and tool not in include_tools:\n",
        "                continue\n",
        "            if not isinstance(tri, dict):\n",
        "                continue\n",
        "\n",
        "            if 'error' in tri:\n",
        "                had_error[tool] += 1\n",
        "                err_by_ds[ds][tool] += 1\n",
        "                if len(sample_errors[tool]) < 3:\n",
        "                    sample_errors[tool].append(tri.get('error'))\n",
        "                continue\n",
        "\n",
        "            preds = tri.get('tags') or []\n",
        "            toks  = tri.get('tokens') or []\n",
        "            if preds:\n",
        "                cov_sentences[tool] += 1\n",
        "                cov_by_ds[ds][tool] += 1\n",
        "                pred_len_sum[tool] += len(preds)\n",
        "                pred_len_cnt[tool] += 1\n",
        "\n",
        "            m = min(len(gt), len(preds), len(toks))\n",
        "            if m == 0:\n",
        "                zero_min[tool] += 1\n",
        "\n",
        "    print(\"=== Tool coverage & alignment audit ===\")\n",
        "    shown_tools = sorted(t for t in tools_seen if (not include_tools or t in include_tools))\n",
        "    for tool in shown_tools:\n",
        "        n_sent = cov_sentences[tool]\n",
        "        n_err  = had_error[tool]\n",
        "        zm     = zero_min[tool]\n",
        "        avg_len = (pred_len_sum[tool]/pred_len_cnt[tool]) if pred_len_cnt[tool] else 0.0\n",
        "        print(f\"{tool:10s} | sentences_ok={n_sent:4d} | errors={n_err:3d} | zero_min={zm:3d} | mean_pred_len={avg_len:5.1f}\")\n",
        "        if sample_errors[tool]:\n",
        "            for e in sample_errors[tool]:\n",
        "                print(f\"   ↪ error sample: {e}\")\n",
        "\n",
        "    # Optional per-dataset breakdown\n",
        "    print(\"\\n--- Per-dataset coverage ---\")\n",
        "    for ds in sorted(cov_by_ds.keys() | err_by_ds.keys()):\n",
        "        print(f\"[{ds}]\")\n",
        "        for tool in shown_tools:\n",
        "            ok = cov_by_ds[ds][tool]\n",
        "            er = err_by_ds[ds][tool]\n",
        "            print(f\"  {tool:10s} ok={ok:4d} | errors={er:3d}\")\n",
        "    return {\n",
        "        \"tools_seen\": shown_tools,\n",
        "        \"sentences_ok\": dict(cov_sentences),\n",
        "        \"errors\": dict(had_error),\n",
        "        \"zero_min\": dict(zero_min),\n",
        "        \"per_dataset_ok\": {ds: dict(cov_by_ds[ds]) for ds in cov_by_ds},\n",
        "        \"per_dataset_err\": {ds: dict(err_by_ds[ds]) for ds in err_by_ds},\n",
        "    }\n",
        "\n",
        "# Example: exclude textblob explicitly\n",
        "_coverage = audit_tool_coverage(expanded_batch_results, include_tools={'spacy_sm','spacy_lg','nltk','flair'})\n"
      ],
      "metadata": {
        "id": "nks3ItJc8d56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Minimal STRICT error analysis (TextBlob excluded)\n",
        "# ==============================================================================\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def run_min_error_analysis_all_tools(results, top_n=5):\n",
        "    # discover all tool names first\n",
        "    tools_seen = set()\n",
        "    for r in results:\n",
        "        tools_seen.update(r.get('tool_results', {}).keys())\n",
        "    # exclude TextBlob\n",
        "    tools_seen.discard('textblob')\n",
        "\n",
        "    error_counts = {t: Counter() for t in tools_seen}  # include empty counters\n",
        "    confusions_by_gold = {t: defaultdict(Counter) for t in tools_seen}\n",
        "    totals = Counter()\n",
        "    errors = Counter()\n",
        "\n",
        "    for res in results:\n",
        "        gt_claws = res.get('ground_truth', [])\n",
        "        gt_penn_full = [convert_claws_to_penn(t, strict=True) for t in gt_claws]\n",
        "\n",
        "        for tool in tools_seen:\n",
        "            tri = res.get('tool_results', {}).get(tool, {})\n",
        "            if not isinstance(tri, dict) or ('error' in tri):\n",
        "                continue\n",
        "            pred = tri.get('tags') or []\n",
        "            toks = tri.get('tokens') or []\n",
        "            m = min(len(gt_penn_full), len(pred), len(toks))\n",
        "            if m <= 0:\n",
        "                continue\n",
        "            gt_penn = gt_penn_full[:m]\n",
        "            for i in range(m):\n",
        "                g, p = gt_penn[i], pred[i]\n",
        "                totals[tool] += 1\n",
        "                if g != p:\n",
        "                    errors[tool] += 1\n",
        "                    error_counts[tool][f\"{g}->{p}\"] += 1\n",
        "                    confusions_by_gold[tool][g][p] += 1\n",
        "\n",
        "    # Report\n",
        "    print(\"Most common error patterns (STRICT):\")\n",
        "    print(\"=\"*60)\n",
        "    for tool in sorted(tools_seen):\n",
        "        err = errors.get(tool, 0)\n",
        "        tot = totals.get(tool, 0)\n",
        "        err_rate = (err / tot) if tot else 0.0\n",
        "        print(f\"\\n{tool} (errors={err:,} / {tot:,} | error rate={err_rate:.3f}):\")\n",
        "        for pat, c in error_counts[tool].most_common(top_n):\n",
        "            print(f\"  {pat:12s} : {c}\")\n",
        "        if tot == 0:\n",
        "            print(\"  (no aligned tokens; check coverage/zero_min above)\")\n",
        "\n",
        "    print(\"\\nTop confusions by GOLD (STRICT):\")\n",
        "    print(\"=\"*60)\n",
        "    default_order = ['UNK','NN','NNS','JJ','RB','IN','VB','VBD','VBG','VBN','VBZ','PRP','DT', ',', '.']\n",
        "    for tool in sorted(tools_seen):\n",
        "        cg = confusions_by_gold[tool]\n",
        "        extras = [g for g in cg.keys() if g not in default_order]\n",
        "        key_gold = default_order + sorted(extras)\n",
        "        print(f\"\\n{tool}:\")\n",
        "        any_line = False\n",
        "        for g in key_gold:\n",
        "            if g in cg and cg[g]:\n",
        "                top = \", \".join([f\"{p}×{c}\" for p, c in cg[g].most_common(3)])\n",
        "                print(f\"  {g:>4s} → {top}\")\n",
        "                any_line = True\n",
        "        if not any_line:\n",
        "            print(\"  (no confusions recorded)\")\n",
        "\n",
        "    return {\n",
        "        \"tools_seen\": sorted(tools_seen),\n",
        "        \"error_counts\": error_counts,\n",
        "        \"confusions_by_gold\": confusions_by_gold,\n",
        "        \"totals\": dict(totals),\n",
        "        \"errors\": dict(errors)\n",
        "    }\n",
        "\n",
        "_min_err = run_min_error_analysis_all_tools(expanded_batch_results, top_n=5)\n"
      ],
      "metadata": {
        "id": "TNCR8CAh-Jfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Drilldown: Which CLAWS tags become UNK (STRICT) and what do tools predict?\n",
        "# ==============================================================================\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def unk_source_analysis(results, tool_names=None, top_n=12, strip_ditto=True, return_data=True):\n",
        "    \"\"\"\n",
        "    For tokens where STRICT-projected gold == 'UNK', count:\n",
        "      - aggregate: CLAWS source -> predicted Penn\n",
        "      - per tool: tool -> CLAWS source -> predicted Penn\n",
        "    Parameters\n",
        "      results      : expanded_batch_results\n",
        "      tool_names   : iterable of tool names to include (None = all discovered)\n",
        "      top_n        : top CLAWS sources to print\n",
        "      strip_ditto  : collapse ditto tags (e.g., II31 -> II)\n",
        "      return_data  : return dicts with flat rows for export\n",
        "    \"\"\"\n",
        "    per_tool = defaultdict(lambda: defaultdict(Counter))   # tool -> CLAWS -> Counter(pred)\n",
        "    all_tools = defaultdict(Counter)                       # CLAWS -> Counter(pred)\n",
        "    totals_by_source = Counter()                           # CLAWS -> total UNK obs (all tools)\n",
        "    totals_by_tool = Counter()                             # tool -> total UNK obs\n",
        "    discovered_tools = set()\n",
        "\n",
        "    for res in results:\n",
        "        gt_claws_full = res.get('ground_truth', [])\n",
        "        gt_penn_full = [convert_claws_to_penn(t, strict=True) for t in gt_claws_full]\n",
        "        tri_map = res.get('tool_results', {})\n",
        "        discovered_tools.update(tri_map.keys())\n",
        "\n",
        "        for tool, tri in tri_map.items():\n",
        "            if tool_names and tool not in tool_names:\n",
        "                continue\n",
        "            if not isinstance(tri, dict) or ('error' in tri):\n",
        "                continue\n",
        "            pred = tri.get('tags') or []\n",
        "            toks = tri.get('tokens') or []\n",
        "            m = min(len(gt_claws_full), len(gt_penn_full), len(pred), len(toks))\n",
        "            if m <= 0:\n",
        "                continue\n",
        "\n",
        "            for i in range(m):\n",
        "                if gt_penn_full[i] == 'UNK':\n",
        "                    claws_src = gt_claws_full[i]\n",
        "                    if strip_ditto:\n",
        "                        claws_src = _strip_ditto(claws_src)\n",
        "                    per_tool[tool][claws_src][pred[i]] += 1\n",
        "                    all_tools[claws_src][pred[i]] += 1\n",
        "                    totals_by_source[claws_src] += 1\n",
        "                    totals_by_tool[tool] += 1\n",
        "\n",
        "    # Resolve default tool list\n",
        "    if tool_names is None:\n",
        "        tool_names = sorted(discovered_tools)\n",
        "\n",
        "    # ---- Aggregate printout ---------------------------------------------------\n",
        "    print(\"Top CLAWS sources of UNK (all tools combined):\")\n",
        "    print(\"=\"*70)\n",
        "    total_unk = sum(totals_by_source.values())\n",
        "    for claws_src, cnts in sorted(all_tools.items(),\n",
        "                                  key=lambda x: sum(x[1].values()),\n",
        "                                  reverse=True)[:top_n]:\n",
        "        src_total = sum(cnts.values())\n",
        "        share = (src_total / total_unk) if total_unk else 0.0\n",
        "        top_preds = \", \".join(f\"{p}×{c}\" for p, c in cnts.most_common(3))\n",
        "        print(f\"{claws_src:8s}  total={src_total:4d}  ({share:5.1%})  →  {top_preds}\")\n",
        "\n",
        "    # ---- Per-tool printout ----------------------------------------------------\n",
        "    for tool in tool_names:\n",
        "        print(f\"\\n{tool}: Top CLAWS sources of UNK\")\n",
        "        print(\"-\"*70)\n",
        "        tool_map = per_tool.get(tool, {})\n",
        "        tool_tot = sum(sum(c.values()) for c in tool_map.values())\n",
        "        if tool_tot == 0:\n",
        "            print(\"  (no UNK observations for this tool)\")\n",
        "            continue\n",
        "        items = sorted(tool_map.items(),\n",
        "                       key=lambda x: sum(x[1].values()),\n",
        "                       reverse=True)[:top_n]\n",
        "        for claws_src, cnts in items:\n",
        "            src_total = sum(cnts.values())\n",
        "            share = src_total / tool_tot if tool_tot else 0.0\n",
        "            top_preds = \", \".join(f\"{p}×{c}\" for p, c in cnts.most_common(3))\n",
        "            print(f\"{claws_src:8s}  total={src_total:4d}  ({share:5.1%})  →  {top_preds}\")\n",
        "\n",
        "    if not return_data:\n",
        "        return None\n",
        "\n",
        "    # ---- Build flat rows for downstream export --------------------------------\n",
        "    agg_rows = []\n",
        "    for claws_src, cnts in all_tools.items():\n",
        "        src_total = sum(cnts.values())\n",
        "        for pred, c in cnts.items():\n",
        "            agg_rows.append({\n",
        "                'CLAWS_source': claws_src,\n",
        "                'pred_penn': pred,\n",
        "                'count': int(c),\n",
        "                'source_total': int(src_total),\n",
        "                'source_share_all': (src_total / total_unk) if total_unk else 0.0\n",
        "            })\n",
        "\n",
        "    per_tool_rows = []\n",
        "    for tool, m in per_tool.items():\n",
        "        tool_tot = sum(sum(c.values()) for c in m.values())\n",
        "        for claws_src, cnts in m.items():\n",
        "            src_total = sum(cnts.values())\n",
        "            for pred, c in cnts.items():\n",
        "                per_tool_rows.append({\n",
        "                    'tool': tool,\n",
        "                    'CLAWS_source': claws_src,\n",
        "                    'pred_penn': pred,\n",
        "                    'count': int(c),\n",
        "                    'source_total_tool': int(src_total),\n",
        "                    'source_share_tool': (src_total / tool_tot) if tool_tot else 0.0\n",
        "                })\n",
        "\n",
        "    return {\n",
        "        'aggregate': agg_rows,\n",
        "        'per_tool': per_tool_rows,\n",
        "        'totals_by_source': dict(totals_by_source),\n",
        "        'totals_by_tool': dict(totals_by_tool),\n",
        "        'tools_seen': tool_names,\n",
        "        'total_unk': int(total_unk)\n",
        "    }\n",
        "\n",
        "# Run — include nltk explicitly (or leave tool_names=None to include all seen tools)\n",
        "_unk = unk_source_analysis(expanded_batch_results,\n",
        "                           tool_names=['spacy_sm','spacy_lg','flair','nltk'],\n",
        "                           top_n=12,\n",
        "                           strip_ditto=True,\n",
        "                           return_data=True)\n"
      ],
      "metadata": {
        "id": "PMzRRByNnd7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Build performance_summary directly from expanded_batch_results\n",
        "# ==============================================================================\n",
        "\n",
        "EXCLUDE_TOOLS = {'textblob'}  # ensure TextBlob is not considered\n",
        "\n",
        "performance_summary = {}\n",
        "\n",
        "for res in expanded_batch_results:\n",
        "    gt_claws = res.get('ground_truth', [])\n",
        "    # Strictly project CLAWS → Penn for gold\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt_claws]\n",
        "\n",
        "    tri_map = res.get('tool_results', {})\n",
        "    for tool, tri in tri_map.items():\n",
        "        # drop excluded tools (e.g., textblob)\n",
        "        if tool in EXCLUDE_TOOLS:\n",
        "            continue\n",
        "        # skip errors / malformed entries\n",
        "        if not isinstance(tri, dict) or ('error' in tri):\n",
        "            continue\n",
        "\n",
        "        pred = tri.get('tags') or []\n",
        "        toks = tri.get('tokens') or []\n",
        "        # align by minimum length across gold, predictions, and tokens\n",
        "        m = min(len(gt_penn), len(pred), len(toks))\n",
        "        if m == 0:\n",
        "            continue\n",
        "\n",
        "        acc = sum(1 for i in range(m) if gt_penn[i] == pred[i]) / m\n",
        "\n",
        "        # init bucket and record\n",
        "        if tool not in performance_summary:\n",
        "            performance_summary[tool] = {\n",
        "                'accuracies': [],\n",
        "                'total_sentences': 0,\n",
        "                'perfect_sentences': 0\n",
        "            }\n",
        "        performance_summary[tool]['accuracies'].append(acc)\n",
        "        performance_summary[tool]['total_sentences'] += 1\n",
        "        if acc == 1.0:\n",
        "            performance_summary[tool]['perfect_sentences'] += 1\n",
        "\n",
        "# Finalize stats\n",
        "for tool, d in list(performance_summary.items()):\n",
        "    accs = d['accuracies']\n",
        "    performance_summary[tool] = {\n",
        "        'mean_accuracy': float(np.mean(accs)) if accs else 0.0,\n",
        "        'std_accuracy': float(np.std(accs)) if accs else 0.0,\n",
        "        'total_sentences': int(d['total_sentences']),\n",
        "        'perfect_sentences': int(d['perfect_sentences'])\n",
        "    }\n",
        "\n",
        "print(\"Tools available for pairwise tests (excluding TextBlob):\", sorted(performance_summary.keys()))\n"
      ],
      "metadata": {
        "id": "EXx_Xn0X-6gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# WILSON CONFIDENCE INTERVALS (STRICT) + SIGNIFICANCE (OVERALL, no TextBlob)\n",
        "# ==============================================================================\n",
        "\n",
        "import scipy.stats as stats\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "EXCLUDE_TOOLS = {'textblob'}\n",
        "\n",
        "def wilson_confidence_interval(successes, trials, confidence=0.95):\n",
        "    if trials == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    p = successes / trials\n",
        "    z = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
        "    denom = 1 + z**2 / trials\n",
        "    centre = (p + z**2 / (2 * trials)) / denom\n",
        "    half = z * sqrt((p * (1 - p) + z**2 / (4 * trials)) / trials) / denom\n",
        "    lower = max(0, centre - half)\n",
        "    upper = min(1, centre + half)\n",
        "    return p, lower, upper\n",
        "\n",
        "def proportion_z_test(x1, n1, x2, n2):\n",
        "    p1, p2 = x1 / n1, x2 / n2\n",
        "    p_pool = (x1 + x2) / (n1 + n2)\n",
        "    se = sqrt(p_pool * (1 - p_pool) * (1/n1 + 1/n2))\n",
        "    z = (p1 - p2) / se\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "    return z, p_value\n",
        "\n",
        "def cohens_h(p1, p2):\n",
        "    return 2 * (np.arcsin(sqrt(p1)) - np.arcsin(sqrt(p2)))\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Ensure performance_summary exists (build from expanded_batch_results if not)\n",
        "# --------------------------------------------------------------------------\n",
        "if 'performance_summary' not in globals() or not performance_summary:\n",
        "    performance_summary = {}\n",
        "    for res in expanded_batch_results:\n",
        "        gt_claws = res.get('ground_truth', [])\n",
        "        gt_penn  = [convert_claws_to_penn(t, strict=True) for t in gt_claws]\n",
        "        tri_map  = res.get('tool_results', {})\n",
        "        for tool, tri in tri_map.items():\n",
        "            if tool in EXCLUDE_TOOLS:\n",
        "                continue\n",
        "            if not isinstance(tri, dict) or ('error' in tri):\n",
        "                continue\n",
        "            pred = tri.get('tags') or []\n",
        "            toks = tri.get('tokens') or []\n",
        "            m = min(len(gt_penn), len(pred), len(toks))\n",
        "            if m == 0:\n",
        "                continue\n",
        "            acc = sum(1 for i in range(m) if gt_penn[i] == pred[i]) / m\n",
        "            if tool not in performance_summary:\n",
        "                performance_summary[tool] = {\n",
        "                    'accuracies': [],\n",
        "                    'total_sentences': 0,\n",
        "                    'perfect_sentences': 0\n",
        "                }\n",
        "            performance_summary[tool]['accuracies'].append(acc)\n",
        "            performance_summary[tool]['total_sentences'] += 1\n",
        "            if acc == 1.0:\n",
        "                performance_summary[tool]['perfect_sentences'] += 1\n",
        "\n",
        "    for tool, d in list(performance_summary.items()):\n",
        "        accs = d['accuracies']\n",
        "        performance_summary[tool] = {\n",
        "            'mean_accuracy': float(np.mean(accs)) if accs else 0.0,\n",
        "            'std_accuracy': float(np.std(accs)) if accs else 0.0,\n",
        "            'total_sentences': int(d['total_sentences']),\n",
        "            'perfect_sentences': int(d['perfect_sentences'])\n",
        "        }\n",
        "\n",
        "print(\"Tool Performance with Wilson 95% Confidence Intervals (STRICT):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "wilson_results = {}\n",
        "aligned_sentence_counts = {}  # per-tool count of sentences with m>0\n",
        "\n",
        "for tool_name, stats_data in performance_summary.items():\n",
        "    if tool_name in EXCLUDE_TOOLS:\n",
        "        continue\n",
        "\n",
        "    total_sentences = stats_data['total_sentences']\n",
        "    perfect_sentences = stats_data['perfect_sentences']\n",
        "\n",
        "    # Perfect-sentence Wilson CI\n",
        "    perfect_rate, perfect_lower, perfect_upper = wilson_confidence_interval(\n",
        "        perfect_sentences, total_sentences\n",
        "    )\n",
        "\n",
        "    # Token-level totals + aligned sentence count\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "    aligned_sents = 0\n",
        "\n",
        "    for res in expanded_batch_results:\n",
        "        tri = res.get('tool_results', {}).get(tool_name, {})\n",
        "        if not tri or 'error' in tri:\n",
        "            continue\n",
        "        gt = res.get('ground_truth', [])\n",
        "        pred = tri.get('tags', [])\n",
        "        toks = tri.get('tokens', [])\n",
        "        m = min(len(gt), len(pred), len(toks))\n",
        "        if m == 0:\n",
        "            continue\n",
        "        aligned_sents += 1\n",
        "        gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "        correct_tokens += sum(1 for i in range(m) if gt_penn[i] == pred[i])\n",
        "        total_tokens += m\n",
        "\n",
        "    aligned_sentence_counts[tool_name] = aligned_sents\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        # No usable tokens for this tool; skip it\n",
        "        continue\n",
        "\n",
        "    token_accuracy, token_lower, token_upper = wilson_confidence_interval(\n",
        "        correct_tokens, total_tokens\n",
        "    )\n",
        "\n",
        "    wilson_results[tool_name] = {\n",
        "        'token_accuracy': token_accuracy,\n",
        "        'token_ci_lower': token_lower,\n",
        "        'token_ci_upper': token_upper,\n",
        "        'perfect_rate': perfect_rate,\n",
        "        'perfect_ci_lower': perfect_lower,\n",
        "        'perfect_ci_upper': perfect_upper,\n",
        "        'total_tokens': total_tokens,\n",
        "        'correct_tokens': correct_tokens,\n",
        "        'total_sentences': total_sentences,\n",
        "        'perfect_sentences': perfect_sentences,\n",
        "        'aligned_sentences': aligned_sents\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{tool_name.upper()}:\")\n",
        "    print(f\"  Token Accuracy: {token_accuracy:.3f} [{token_lower:.3f}, {token_upper:.3f}]\")\n",
        "    print(f\"  Perfect Sentences: {perfect_rate:.3f} [{perfect_lower:.3f}, {perfect_upper:.3f}]\")\n",
        "    print(f\"  Sample size: {total_tokens:,} tokens, {total_sentences} sentences \"\n",
        "          f\"(aligned sentences used: {aligned_sents})\")\n",
        "\n",
        "# Guard and significance test\n",
        "if not wilson_results:\n",
        "    print(\"\\nNo token-level data available to compute Wilson intervals. \"\n",
        "          \"Check that expanded_batch_results is populated and tool predictions exist.\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    tools_by_accuracy = sorted(wilson_results.items(),\n",
        "                               key=lambda x: x[1]['token_accuracy'],\n",
        "                               reverse=True)\n",
        "    best_tool, best_stats = tools_by_accuracy[0]\n",
        "    worst_tool, worst_stats = tools_by_accuracy[-1]\n",
        "\n",
        "    z_stat, p_value = proportion_z_test(\n",
        "        best_stats['correct_tokens'], best_stats['total_tokens'],\n",
        "        worst_stats['correct_tokens'], worst_stats['total_tokens']\n",
        "    )\n",
        "\n",
        "    print(f\"Comparison: {best_tool} vs {worst_tool}\")\n",
        "    print(f\"Accuracy difference: {best_stats['token_accuracy'] - worst_stats['token_accuracy']:.3f}\")\n",
        "    print(f\"Z-statistic: {z_stat:.3f}\")\n",
        "    print(f\"P-value: {p_value:.3f}\")\n",
        "    sig = \"Yes\" if p_value < 0.05 else \"No\"\n",
        "    print(f\"Significant at α=0.05: {sig}\")\n",
        "\n",
        "    effect_size = cohens_h(best_stats['token_accuracy'], worst_stats['token_accuracy'])\n",
        "    if abs(effect_size) < 0.2:\n",
        "        magnitude = \"negligible\"\n",
        "    elif abs(effect_size) < 0.5:\n",
        "        magnitude = \"small\"\n",
        "    elif abs(effect_size) < 0.8:\n",
        "        magnitude = \"medium\"\n",
        "    else:\n",
        "        magnitude = \"large\"\n",
        "    print(f\"Effect size (Cohen's h): {effect_size:.3f} ({magnitude})\")\n",
        "\n",
        "# Optional sanity check: show aligned sentence counts per tool\n",
        "print(\"\\nAligned sentences used per tool (should be close to 383 if all align):\")\n",
        "for t, n in sorted(aligned_sentence_counts.items()):\n",
        "    print(f\"  {t:10s}: {n}\")\n"
      ],
      "metadata": {
        "id": "ShkhL401_8b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Pairwise McNemar’s Tests (with Holm–Bonferroni) + Cluster Bootstrap CIs\n",
        "# ==============================================================================\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "from scipy.stats import binomtest\n",
        "\n",
        "# Try to import statsmodels' McNemar; fall back to an exact/binomial implementation\n",
        "try:\n",
        "    from statsmodels.stats.contingency_tables import mcnemar as _sm_mcnemar\n",
        "    HAVE_STATSMODELS = True\n",
        "except Exception:\n",
        "    HAVE_STATSMODELS = False\n",
        "\n",
        "def _mcnemar_from_table(table, exact):\n",
        "    \"\"\"\n",
        "    Return (statistic, pvalue) for McNemar from 2x2 table:\n",
        "      [[both_correct, a_correct_b_wrong],\n",
        "       [b_correct_a_wrong, both_wrong]]\n",
        "    If statsmodels is present, use it; otherwise compute:\n",
        "      - exact: two-sided binomial on min(b01,b10) with n=b01+b10, p=0.5\n",
        "      - chi^2 (no continuity) when exact=False   (continuity not added in fallback)\n",
        "    \"\"\"\n",
        "    b01 = table[0][1]\n",
        "    b10 = table[1][0]\n",
        "    discordant = b01 + b10\n",
        "    if discordant == 0:\n",
        "        # no information to test; define stat=0, p=1\n",
        "        return 0.0, 1.0\n",
        "\n",
        "    if HAVE_STATSMODELS:\n",
        "        res = _sm_mcnemar(table, exact=exact, correction=(not exact))\n",
        "        stat = float(res.statistic) if res.statistic is not None else np.nan\n",
        "        return stat, float(res.pvalue)\n",
        "\n",
        "    # Fallbacks\n",
        "    if exact:\n",
        "        # Two-sided exact binomial test under H0: p = 0.5\n",
        "        k = min(b01, b10)\n",
        "        p = binomtest(k, n=discordant, p=0.5, alternative='two-sided').pvalue\n",
        "        # A chi^2-like descriptive stat (not used for decision when exact=True)\n",
        "        stat = (b01 - b10) ** 2 / discordant\n",
        "        return float(stat), float(p)\n",
        "    else:\n",
        "        # Large-sample chi-square without continuity (approx.)\n",
        "        stat = (b01 - b10) ** 2 / discordant\n",
        "        # Two-sided p-value from chi-square(1)\n",
        "        from scipy.stats import chi2\n",
        "        p = 1 - chi2.cdf(stat, df=1)\n",
        "        return float(stat), float(p)\n",
        "\n",
        "def build_mcnemar_table(tool_a, tool_b, results):\n",
        "    \"\"\"\n",
        "    Build 2x2 contingency for paired token outcomes:\n",
        "      [[ both_correct,  a_correct_b_wrong ],\n",
        "       [ b_correct_a_wrong, both_wrong     ]]\n",
        "    STRICT mapping for gold.\n",
        "    \"\"\"\n",
        "    both_correct = both_wrong = a_correct_b_wrong = b_correct_a_wrong = 0\n",
        "\n",
        "    for res in results:\n",
        "        gt = res['ground_truth']\n",
        "        tri_a = res['tool_results'].get(tool_a, {})\n",
        "        tri_b = res['tool_results'].get(tool_b, {})\n",
        "        if not isinstance(tri_a, dict) or not isinstance(tri_b, dict):\n",
        "            continue\n",
        "        if 'error' in tri_a or 'error' in tri_b:\n",
        "            continue\n",
        "\n",
        "        pred_a = tri_a.get('tags', [])\n",
        "        pred_b = tri_b.get('tags', [])\n",
        "        toks_a = tri_a.get('tokens', [])\n",
        "        toks_b = tri_b.get('tokens', [])\n",
        "\n",
        "        m = min(len(gt), len(pred_a), len(pred_b), len(toks_a), len(toks_b))\n",
        "        if m == 0:\n",
        "            continue\n",
        "\n",
        "        gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "\n",
        "        for i in range(m):\n",
        "            a_correct = (gt_penn[i] == pred_a[i])\n",
        "            b_correct = (gt_penn[i] == pred_b[i])\n",
        "            if a_correct and b_correct:\n",
        "                both_correct += 1\n",
        "            elif (not a_correct) and (not b_correct):\n",
        "                both_wrong += 1\n",
        "            elif a_correct and (not b_correct):\n",
        "                a_correct_b_wrong += 1\n",
        "            else:  # b_correct and not a_correct\n",
        "                b_correct_a_wrong += 1\n",
        "\n",
        "    return [[both_correct, a_correct_b_wrong],\n",
        "            [b_correct_a_wrong, both_wrong]]\n",
        "\n",
        "def run_mcnemar_for_all_pairs(results, tool_names):\n",
        "    \"\"\"\n",
        "    Runs McNemar for each unordered tool pair.\n",
        "    Uses exact test when discordant count < 25, else chi^2 with continuity (if statsmodels).\n",
        "    Returns list of dicts with stats and raw p-values.\n",
        "    \"\"\"\n",
        "    outcomes = []\n",
        "    for a, b in itertools.combinations(tool_names, 2):\n",
        "        table = build_mcnemar_table(a, b, results)\n",
        "        b01 = table[0][1]\n",
        "        b10 = table[1][0]\n",
        "        discordant = b01 + b10\n",
        "        exact = discordant < 25  # common rule-of-thumb\n",
        "        chi2, p = _mcnemar_from_table(table, exact=exact)\n",
        "        outcomes.append({\n",
        "            'tool_a': a, 'tool_b': b,\n",
        "            'table': table,\n",
        "            'discordant': discordant,\n",
        "            'exact': exact,\n",
        "            'chi2': chi2,\n",
        "            'p_value': p\n",
        "        })\n",
        "    return outcomes\n",
        "\n",
        "def holm_bonferroni_adjust(pvals):\n",
        "    \"\"\"\n",
        "    Holm–Bonferroni step-down procedure for FWER control.\n",
        "    Returns adjusted p-values in the original order.\n",
        "    \"\"\"\n",
        "    m = len(pvals)\n",
        "    if m == 0:\n",
        "        return np.array([])\n",
        "    order = np.argsort(pvals)\n",
        "    adj = np.empty(m, dtype=float)\n",
        "    running_max = 0.0\n",
        "    for rank, idx in enumerate(order, start=1):\n",
        "        p = pvals[idx]\n",
        "        adj_p = (m - rank + 1) * p\n",
        "        running_max = max(running_max, adj_p)\n",
        "        adj[idx] = min(1.0, running_max)\n",
        "    return adj\n",
        "\n",
        "def sentence_accuracy(tool_name, res):\n",
        "    \"\"\"\n",
        "    Returns (#correct, #total) tokens for a single sentence result and tool,\n",
        "    under STRICT mapping.\n",
        "    \"\"\"\n",
        "    tri = res['tool_results'].get(tool_name, {})\n",
        "    if not isinstance(tri, dict) or 'error' in tri:\n",
        "        return 0, 0\n",
        "    gt = res['ground_truth']\n",
        "    pred = tri.get('tags', [])\n",
        "    toks = tri.get('tokens', [])\n",
        "    m = min(len(gt), len(pred), len(toks))\n",
        "    if m == 0:\n",
        "        return 0, 0\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "    correct = sum(1 for i in range(m) if gt_penn[i] == pred[i])\n",
        "    return correct, m\n",
        "\n",
        "def bootstrap_accuracy_diff_sentence_cluster(tool_a, tool_b, results, n_iter=3000, seed=123):\n",
        "    \"\"\"\n",
        "    Cluster bootstrap by sentence: resample sentences with replacement,\n",
        "    compute token-level accuracy per tool, then take difference acc_a - acc_b.\n",
        "    Returns mean diff and 95% percentile CI.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(results)\n",
        "    diffs = np.empty(n_iter, dtype=float)\n",
        "\n",
        "    # Precompute per-sentence (correct, total) for speed\n",
        "    per_sent = []\n",
        "    for res in results:\n",
        "        ca, na = sentence_accuracy(tool_a, res)\n",
        "        cb, nb = sentence_accuracy(tool_b, res)\n",
        "        per_sent.append((ca, na, cb, nb))\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        ca_sum = na_sum = cb_sum = nb_sum = 0\n",
        "        for j in idx:\n",
        "            ca, na, cb, nb = per_sent[j]\n",
        "            ca_sum += ca; na_sum += na\n",
        "            cb_sum += cb; nb_sum += nb\n",
        "        acc_a = ca_sum / na_sum if na_sum else 0.0\n",
        "        acc_b = cb_sum / nb_sum if nb_sum else 0.0\n",
        "        diffs[i] = acc_a - acc_b\n",
        "\n",
        "    mean_diff = float(np.mean(diffs))\n",
        "    low, high = np.percentile(diffs, [2.5, 97.5])\n",
        "    return mean_diff, float(low), float(high)\n",
        "\n",
        "# --- Choose tools from your performance_summary; drop any with no token data ---\n",
        "all_tools = sorted(performance_summary.keys())\n",
        "# If you ever want to explicitly exclude a tool (e.g., 'textblob'), uncomment:\n",
        "# all_tools = [t for t in all_tools if t != 'textblob']\n",
        "\n",
        "if len(all_tools) < 2:\n",
        "    print(\"Not enough tools for pairwise tests.\")\n",
        "else:\n",
        "    # 1) McNemar for all pairs\n",
        "    mcnemar_outcomes = run_mcnemar_for_all_pairs(expanded_batch_results, all_tools)\n",
        "    pvals = np.array([o['p_value'] for o in mcnemar_outcomes])\n",
        "    adj_pvals = holm_bonferroni_adjust(pvals)\n",
        "\n",
        "    print(\"\\nPAIRWISE McNemar’s tests (token-level, STRICT gold projection):\")\n",
        "    print(\"=\" * 80)\n",
        "    for o, adjp in zip(mcnemar_outcomes, adj_pvals):\n",
        "        a, b = o['tool_a'], o['tool_b']\n",
        "        t = o['table']\n",
        "        print(f\"\\n{a} vs {b}\")\n",
        "        print(f\"  Table [[both_correct, a_correct_b_wrong], [b_correct_a_wrong, both_wrong]] = {t}\")\n",
        "        print(f\"  Discordant = {o['discordant']} | {'exact' if o['exact'] else 'chi^2'} test\")\n",
        "        print(f\"  McNemar χ² = {o['chi2']:.3f} | p = {o['p_value']:.4g} | Holm-adjusted p = {adjp:.4g}\")\n",
        "        if adjp < 0.05:\n",
        "            print(\"  → Significant asymmetry in disagreements (after Holm–Bonferroni).\")\n",
        "        else:\n",
        "            print(\"  → No significant asymmetry in disagreements (after correction).\")\n",
        "\n",
        "    # 2) Cluster bootstrap CIs for accuracy differences for all pairs\n",
        "    print(\"\\nPAIRWISE Bootstrap 95% CIs for accuracy differences (acc_A − acc_B):\")\n",
        "    print(\"=\" * 80)\n",
        "    for a, b in itertools.combinations(all_tools, 2):\n",
        "        mean_diff, lo, hi = bootstrap_accuracy_diff_sentence_cluster(a, b, expanded_batch_results,\n",
        "                                                                     n_iter=3000, seed=123)\n",
        "        note = \" (A>B)\" if (lo > 0) else (\" (B>A)\" if (hi < 0) else \" (no clear difference)\")\n",
        "        print(f\"{a:10s} − {b:10s}: mean = {mean_diff:.3f}, 95% CI = [{lo:.3f}, {hi:.3f}]{note}\")\n"
      ],
      "metadata": {
        "id": "oIA9qfQUAf1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STRICT metrics: Precision/Recall/F1 (micro/macro/weighted)\n",
        "# + Newcombe (Wilson) CIs for differences in accuracy\n",
        "# (filters out TextBlob; includes spaCy_sm, spaCy_lg, Flair, NLTK)\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def collect_token_outcomes_strict(results, allowed_tools=None):\n",
        "    \"\"\"\n",
        "    Build per-tool gold/pred arrays under STRICT projection.\n",
        "    Only tools in allowed_tools are kept (if provided).\n",
        "    Returns:\n",
        "      per_tool = {\n",
        "         tool: {\n",
        "            'y_true': [... Penn tags ...],\n",
        "            'y_pred': [... Penn tags ...],\n",
        "            'correct_tokens': int,\n",
        "            'total_tokens': int\n",
        "         }, ...\n",
        "      }\n",
        "    \"\"\"\n",
        "    per_tool = defaultdict(lambda: {'y_true': [], 'y_pred': [], 'correct_tokens': 0, 'total_tokens': 0})\n",
        "\n",
        "    for res in results:\n",
        "        gt_claws = res.get('ground_truth', [])\n",
        "        tri_map = res.get('tool_results', {})\n",
        "        for tool, tri in tri_map.items():\n",
        "            if allowed_tools is not None and tool not in allowed_tools:\n",
        "                continue\n",
        "            if not isinstance(tri, dict) or ('error' in tri):\n",
        "                continue\n",
        "\n",
        "            pred = tri.get('tags', []) or []\n",
        "            toks = tri.get('tokens', []) or []\n",
        "            m = min(len(gt_claws), len(pred), len(toks))\n",
        "            if m <= 0:\n",
        "                continue\n",
        "\n",
        "            gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt_claws[:m]]\n",
        "            y_true = gt_penn\n",
        "            y_pred = pred[:m]\n",
        "\n",
        "            per_tool[tool]['y_true'].extend(y_true)\n",
        "            per_tool[tool]['y_pred'].extend(y_pred)\n",
        "            per_tool[tool]['total_tokens'] += m\n",
        "            per_tool[tool]['correct_tokens'] += sum(1 for i in range(m) if y_true[i] == y_pred[i])\n",
        "\n",
        "    return per_tool\n",
        "\n",
        "def wilson_interval(successes, n, confidence=0.95):\n",
        "    \"\"\"Wilson score interval for a single proportion.\"\"\"\n",
        "    if n == 0:\n",
        "        return (0.0, 0.0, 0.0)\n",
        "    from scipy.stats import norm\n",
        "    z = norm.ppf(1 - (1 - confidence)/2)\n",
        "    p = successes / n\n",
        "    denom = 1 + z*z/n\n",
        "    centre = (p + z*z/(2*n)) / denom\n",
        "    half = z * np.sqrt((p*(1-p) + z*z/(4*n))/n) / denom\n",
        "    return (p, max(0.0, centre - half), min(1.0, centre + half))\n",
        "\n",
        "def newcombe_wilson_diff(x1, n1, x2, n2, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Newcombe method 10 (1998): CI for difference of two independent proportions.\n",
        "    This complements paired analyses (McNemar / bootstrap) with a descriptive CI.\n",
        "    \"\"\"\n",
        "    p1, L1, U1 = wilson_interval(x1, n1, confidence)\n",
        "    p2, L2, U2 = wilson_interval(x2, n2, confidence)\n",
        "    diff = p1 - p2\n",
        "    lower = L1 - U2\n",
        "    upper = U1 - L2\n",
        "    return diff, lower, upper, (p1, L1, U1), (p2, L2, U2)\n",
        "\n",
        "# Choose tools explicitly (exclude textblob)\n",
        "discovered = {t for r in expanded_batch_results for t in r.get('tool_results', {}).keys()}\n",
        "tools_for_eval = [t for t in ('spacy_sm','spacy_lg','flair','nltk') if t in discovered]\n",
        "\n",
        "per_tool = collect_token_outcomes_strict(expanded_batch_results, allowed_tools=tools_for_eval)\n",
        "\n",
        "if not per_tool:\n",
        "    print(\"No token-level data found. Make sure expanded_batch_results is populated.\")\n",
        "else:\n",
        "    # Build a consistent label set across the included tools\n",
        "    label_set = set()\n",
        "    for d in per_tool.values():\n",
        "        label_set.update(d['y_true'])\n",
        "        label_set.update(d['y_pred'])\n",
        "    labels = sorted(label_set)\n",
        "\n",
        "    print(\"Token-level Precision / Recall / F1 under STRICT gold\")\n",
        "    print(\"=\"*70)\n",
        "    metrics_summary = {}\n",
        "\n",
        "    # Print tools in a stable order (by name)\n",
        "    for tool in sorted(per_tool.keys()):\n",
        "        d = per_tool[tool]\n",
        "        y_true = np.array(d['y_true'])\n",
        "        y_pred = np.array(d['y_pred'])\n",
        "        if y_true.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Averages\n",
        "        prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, labels=labels, average='micro', zero_division=0\n",
        "        )\n",
        "        prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, labels=labels, average='macro', zero_division=0\n",
        "        )\n",
        "        prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, labels=labels, average='weighted', zero_division=0\n",
        "        )\n",
        "\n",
        "        # Per-class (optional): show top classes by support\n",
        "        _, _, f1_per_class, support = precision_recall_fscore_support(\n",
        "            y_true, y_pred, labels=labels, average=None, zero_division=0\n",
        "        )\n",
        "        support_idx = np.argsort(support)[::-1]\n",
        "        topK = 10\n",
        "        top_rows = [(labels[i], int(support[i]), float(f1_per_class[i])) for i in support_idx[:topK]]\n",
        "\n",
        "        acc = d['correct_tokens'] / d['total_tokens'] if d['total_tokens'] else 0.0\n",
        "        metrics_summary[tool] = {\n",
        "            'accuracy': acc,\n",
        "            'micro':  (prec_micro, rec_micro, f1_micro),\n",
        "            'macro':  (prec_macro, rec_macro, f1_macro),\n",
        "            'weighted': (prec_weighted, rec_weighted, f1_weighted),\n",
        "            'top_classes': top_rows,\n",
        "            'total_tokens': d['total_tokens'],\n",
        "            'correct_tokens': d['correct_tokens']\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{tool.upper()}:\")\n",
        "        print(f\"  Tokens: {d['total_tokens']:,} | Accuracy: {acc:.3f}\")\n",
        "        print(f\"  Micro  P/R/F1: {prec_micro:.3f} / {rec_micro:.3f} / {f1_micro:.3f}\")\n",
        "        print(f\"  Macro  P/R/F1: {prec_macro:.3f} / {rec_macro:.3f} / {f1_macro:.3f}\")\n",
        "        print(f\"  Weight P/R/F1: {prec_weighted:.3f} / {rec_weighted:.3f} / {f1_weighted:.3f}\")\n",
        "        print(f\"  Top {topK} classes by support (label, support, F1):\")\n",
        "        for lbl, sup, f1c in top_rows:\n",
        "            print(f\"    {lbl:>4s}  {sup:6d}  F1={f1c:.3f}\")\n",
        "\n",
        "    # Newcombe (Wilson) CIs for accuracy differences between every pair of tools\n",
        "    if metrics_summary:\n",
        "        print(\"\\nNewcombe (Wilson) 95% CIs for differences in token accuracy (acc_A − acc_B)\")\n",
        "        print(\"=\"*70)\n",
        "        tools = sorted(metrics_summary.keys())\n",
        "        for a, b in itertools.combinations(tools, 2):\n",
        "            ca, na = metrics_summary[a]['correct_tokens'], metrics_summary[a]['total_tokens']\n",
        "            cb, nb = metrics_summary[b]['correct_tokens'], metrics_summary[b]['total_tokens']\n",
        "            diff, lo, hi, p1_info, p2_info = newcombe_wilson_diff(ca, na, cb, nb, confidence=0.95)\n",
        "            print(f\"{a:10s} − {b:10s}: Δ = {diff:.3f}, 95% CI = [{lo:.3f}, {hi:.3f}]  \"\n",
        "                  f\"(p1={p1_info[0]:.3f}, p2={p2_info[0]:.3f})\")\n",
        "    else:\n",
        "        print(\"No metrics to compare.\")\n"
      ],
      "metadata": {
        "id": "mTyI7cchBnM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encodable-only (exclude UNK gold) token accuracy per tool\n",
        "def encodable_accuracy(results, tools):\n",
        "    acc = {}\n",
        "    for tool in tools:\n",
        "        correct = total = 0\n",
        "        for res in results:\n",
        "            tri = res['tool_results'].get(tool, {})\n",
        "            if 'error' in tri:\n",
        "                continue\n",
        "            gt = res['ground_truth']\n",
        "            pred = tri.get('tags', []) or []\n",
        "            toks = tri.get('tokens', []) or []\n",
        "            m = min(len(gt), len(pred), len(toks))\n",
        "            if m == 0:\n",
        "                continue\n",
        "            gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "            for i in range(m):\n",
        "                if gt_penn[i] == 'UNK':\n",
        "                    continue\n",
        "                total += 1\n",
        "                if gt_penn[i] == pred[i]:\n",
        "                    correct += 1\n",
        "        acc[tool] = (correct / total) if total else 0.0\n",
        "    return acc\n",
        "\n",
        "enc_only = encodable_accuracy(expanded_batch_results, ['flair','spacy_lg','spacy_sm','nltk'])\n",
        "for k, v in sorted(enc_only.items()):\n",
        "    print(f\"{k:9s}: {v:.3f}\")\n"
      ],
      "metadata": {
        "id": "QjWMJ83cCV5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Tests under **Strict** CLAWS→Penn Projection\n",
        "\n",
        "#### Wilson 95% Confidence Intervals (token accuracy)\n",
        "*(383 sentences; ~9,994 tokens for spaCy/Flair; ~9,941 for NLTK)*\n",
        "\n",
        "- **Flair**: **0.559**  [0.549, 0.569]  \n",
        "- **NLTK**: **0.531**  [0.521, 0.540]  \n",
        "- **spaCy (lg)**: **0.530**  [0.520, 0.540]  \n",
        "- **spaCy (sm)**: **0.527**  [0.518, 0.537]  \n",
        "\n",
        "_Perfect-sentence rate ≈ 0% for all models (95% CI upper bound ≈ 1%)._\n",
        "\n",
        "#### Two-Proportion Z-Test (best vs comparator)\n",
        "- **Flair vs spaCy (sm)**: Δ = **0.032**, z = **4.487**, p < **0.001**, **significant**  \n",
        "  Cohen’s *h* = **0.063** → *negligible* effect size.\n",
        "\n",
        "#### Pairwise McNemar’s Tests (token-level; Holm–Bonferroni corrected)\n",
        "- **Flair vs NLTK**: χ² = **66.325**, p = **3.823e-16** → **significant**  \n",
        "- **Flair vs spaCy (lg)**: χ² = **113.144**, p = **2.006e-26** → **significant**  \n",
        "- **Flair vs spaCy (sm)**: χ² = **132.300**, p = **1.286e-30** → **significant**  \n",
        "- **NLTK vs spaCy (lg)**: χ² = **0.000**, p = **1.000** → **no difference**  \n",
        "- **NLTK vs spaCy (sm)**: χ² = **0.594**, p = **0.4407** → **no difference**  \n",
        "- **spaCy (lg) vs spaCy (sm)**: χ² = **5.879**, p = **0.0153** → **significant** (small, after correction)\n",
        "\n",
        "_Example contingency (Flair vs spaCy-lg): [[both correct, A-correct/B-wrong], [B-correct/A-wrong, both wrong]] = [[5079, 508], [220, 4186]] (discordant = 728)._\n",
        "\n",
        "#### Pairwise Cluster Bootstrap 95% CIs for Accuracy Differences (accₐ − accᵦ)\n",
        "- **Flair − NLTK**: mean = **+0.028**, CI = **[+0.002, +0.054]** → A>B  \n",
        "- **Flair − spaCy (lg)**: mean = **+0.029**, CI = **[+0.009, +0.052]** → A>B  \n",
        "- **Flair − spaCy (sm)**: mean = **+0.031**, CI = **[+0.012, +0.054]** → A>B  \n",
        "- **NLTK − spaCy (lg)**: mean = **+0.001**, CI = **[−0.026, +0.026]** → no clear diff  \n",
        "- **NLTK − spaCy (sm)**: mean = **+0.003**, CI = **[−0.024, +0.029]** → no clear diff  \n",
        "- **spaCy (lg) − spaCy (sm)**: mean = **+0.003**, CI = **[+0.001, +0.005]** → A>B\n",
        "\n",
        "#### Alignment/coverage (sentences used)\n",
        "- **Flair**: 383  \n",
        "- **NLTK**: 383  \n",
        "- **spaCy (lg)**: 383  \n",
        "- **spaCy (sm)**: 383  \n",
        "\n",
        "**Takeaway:** Flair is **consistently but modestly** better than NLTK and both spaCy variants under strict projection; spaCy-lg and spaCy-sm are nearly indistinguishable. Effects are statistically reliable yet **practically small**, reflecting the structural loss from CLAWS→Penn (`UNK`) rather than large modeling differences.\n"
      ],
      "metadata": {
        "id": "hxJ1HgufD8Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token-level performance under **strict** CLAWS→Penn projection\n",
        "\n",
        "| Tool        | Accuracy | Micro-F1 | Macro-F1 | Weighted-F1 |\n",
        "|-------------|:--------:|:--------:|:--------:|:-----------:|\n",
        "| spaCy (sm)  | 0.527    | 0.527    | 0.402    | 0.494       |\n",
        "| spaCy (lg)  | 0.530    | 0.530    | 0.397    | 0.497       |\n",
        "| NLTK        | 0.531    | 0.531    | 0.396    | 0.494       |\n",
        "| **Flair**   | **0.559**| **0.559**| **0.437**| **0.523**   |\n",
        "\n",
        "*Notes (selected, high-support tags):* Under strict projection, `UNK`—which collapses CLAWS distinctions invisible to Penn—accounts for ≈31% of tokens and has F1 = 0.000 for all tools. Frequent tractable tags achieve materially better F1; e.g., with Flair: `IN` ≈ 0.77, `NN` ≈ 0.81, `JJ` ≈ 0.77, `RB` ≈ 0.78, `NNS` ≈ 0.82, `CC` ≈ 0.90.\n",
        "\n",
        "#### Pairwise accuracy differences (Newcombe Wilson 95% CIs; Δ = Acc\\_A − Acc\\_B)\n",
        "\n",
        "| Comparison          | Δ        | 95% CI            | Interpretation               |\n",
        "|---------------------|---------:|-------------------|------------------------------|\n",
        "| **Flair − NLTK**    | +0.029   | [0.009, 0.048]    | Flair significantly better   |\n",
        "| **Flair − spaCy (lg)** | +0.029| [0.009, 0.048]    | Flair significantly better   |\n",
        "| **Flair − spaCy (sm)** | +0.032| [0.012, 0.051]    | Flair significantly better   |\n",
        "| NLTK − spaCy (lg)   | +0.000   | [−0.019, 0.020]   | No clear difference          |\n",
        "| NLTK − spaCy (sm)   | +0.003   | [−0.016, 0.023]   | No clear difference          |\n",
        "| spaCy (lg) − (sm)   | +0.003   | [−0.017, 0.022]   | No clear difference          |\n",
        "\n",
        "#### Paired significance (McNemar) and robustness (bootstrap)\n",
        "\n",
        "- **McNemar (Holm–Bonferroni):** Disagreements are asymmetric in favor of **Flair** vs each of NLTK, spaCy-lg, and spaCy-sm (all adjusted *p* < 0.05). NLTK vs spaCy models show no asymmetry; spaCy-lg vs spaCy-sm shows a small but significant asymmetry (adjusted *p* ≈ 0.046).\n",
        "- **Cluster bootstrap CIs (sentence-level resampling):** Accuracy gaps of **Flair** over NLTK/spaCy are small but positive (≈ +0.02 to +0.03) with CIs excluding zero; differences among NLTK and spaCy models are not clearly different from zero.\n",
        "\n",
        "#### Interpretation under strict projection\n",
        "\n",
        "The strict CLAWS→Penn protocol intentionally collapses any CLAWS category that Penn cannot encode into `UNK`. The resulting `UNK` mass therefore quantifies **representational loss in Penn**, not model error per se. Within this constrained regime, **Flair** exhibits a consistent but **small** advantage (≈3 percentage points). Differences among NLTK and spaCy models are negligible. The central finding is thus **structural**: performance ceilings are dominated by the tagset mismatch—evidence that CLAWS captures fine-grained morphosyntactic distinctions that Penn-based evaluation cannot reward.\n"
      ],
      "metadata": {
        "id": "ouTJTd8JHMQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# VISUALIZATIONS — per dataset (Dubliners, BNC) + pooled\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# --- helper: strict, sentence-level accuracy for a tool on one sentence ---\n",
        "def _strict_sentence_acc(gt_claws, pred_tags, pred_tokens):\n",
        "    m = min(len(gt_claws), len(pred_tags), len(pred_tokens))\n",
        "    if m == 0:\n",
        "        return None\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt_claws[:m]]\n",
        "    correct = sum(1 for i in range(m) if gt_penn[i] == pred_tags[i])\n",
        "    return correct / m\n",
        "\n",
        "# --- build per-dataset stats from expanded_batch_results ---\n",
        "datasets = sorted({rec.get('dataset', 'Unknown') for rec in expanded_batch_results})\n",
        "tools_seen = sorted({t for rec in expanded_batch_results for t in rec.get('tool_results', {}).keys()})\n",
        "\n",
        "stats = {ds: {tool: {'accs': [], 'total_sentences': 0, 'perfect_sentences': 0}\n",
        "              for tool in tools_seen}\n",
        "         for ds in datasets}\n",
        "\n",
        "for rec in expanded_batch_results:\n",
        "    ds = rec.get('dataset', 'Unknown')\n",
        "    gt = rec.get('ground_truth', [])\n",
        "    tri_map = rec.get('tool_results', {})\n",
        "    for tool, tri in tri_map.items():\n",
        "        if not isinstance(tri, dict) or ('error' in tri):\n",
        "            continue\n",
        "        pred = tri.get('tags') or []\n",
        "        toks = tri.get('tokens') or []\n",
        "        acc = _strict_sentence_acc(gt, pred, toks)\n",
        "        if acc is None:\n",
        "            continue\n",
        "        s = stats[ds][tool]\n",
        "        s['accs'].append(acc)\n",
        "        s['total_sentences'] += 1\n",
        "        if acc == 1.0:\n",
        "            s['perfect_sentences'] += 1\n",
        "\n",
        "# finalize (replace accs list with summary numbers) and also compute pooled (\"All\")\n",
        "pooled = {tool: {'accs': [], 'total_sentences': 0, 'perfect_sentences': 0} for tool in tools_seen}\n",
        "for ds in datasets:\n",
        "    for tool in tools_seen:\n",
        "        d = stats[ds][tool]\n",
        "        accs = d['accs']\n",
        "        stats[ds][tool] = {\n",
        "            'mean_accuracy': float(np.mean(accs)) if accs else 0.0,\n",
        "            'std_accuracy': float(np.std(accs)) if accs else 0.0,\n",
        "            'total_sentences': int(d['total_sentences']),\n",
        "            'perfect_sentences': int(d['perfect_sentences'])\n",
        "        }\n",
        "        pooled[tool]['accs'].extend(accs)\n",
        "        pooled[tool]['total_sentences'] += d['total_sentences']\n",
        "        pooled[tool]['perfect_sentences'] += d['perfect_sentences']\n",
        "\n",
        "stats['All (pooled)'] = {}\n",
        "for tool in tools_seen:\n",
        "    p = pooled[tool]\n",
        "    accs = p['accs']\n",
        "    stats['All (pooled)'][tool] = {\n",
        "        'mean_accuracy': float(np.mean(accs)) if accs else 0.0,\n",
        "        'std_accuracy': float(np.std(accs)) if accs else 0.0,\n",
        "        'total_sentences': int(p['total_sentences']),\n",
        "        'perfect_sentences': int(p['perfect_sentences'])\n",
        "    }\n",
        "\n",
        "# --- Figure 1: grouped bar of mean sentence accuracy per tool, by dataset ---\n",
        "ordered_datasets = [ds for ds in ['Dubliners', 'BNC', 'All (pooled)'] if ds in stats]\n",
        "tools = tools_seen\n",
        "\n",
        "fig_acc = go.Figure()\n",
        "for ds in ordered_datasets:\n",
        "    y_vals = [stats[ds].get(t, {}).get('mean_accuracy', 0.0) for t in tools]\n",
        "    y_errs = [stats[ds].get(t, {}).get('std_accuracy', 0.0) for t in tools]\n",
        "    fig_acc.add_trace(go.Bar(\n",
        "        name=ds,\n",
        "        x=tools,\n",
        "        y=y_vals,\n",
        "        error_y=dict(type='data', array=y_errs),\n",
        "        text=[f\"{v:.3f}\" for v in y_vals],\n",
        "        textposition='auto'\n",
        "    ))\n",
        "\n",
        "fig_acc.update_layout(\n",
        "    barmode='group',\n",
        "    title=\"Mean sentence accuracy (STRICT) by tool and dataset\",\n",
        "    xaxis_title=\"Tool\",\n",
        "    yaxis_title=\"Mean sentence accuracy\",\n",
        "    yaxis=dict(range=[0, 1])\n",
        ")\n",
        "fig_acc.show()\n",
        "\n",
        "# --- Conditionally render perfect-sentence plot only if any rate > 0 ---\n",
        "any_perfect = False\n",
        "rates_by_ds = {}\n",
        "for ds in ordered_datasets:\n",
        "    ds_rates = []\n",
        "    for t in tools:\n",
        "        d = stats[ds].get(t, {})\n",
        "        tot = d.get('total_sentences', 0)\n",
        "        perf = d.get('perfect_sentences', 0)\n",
        "        rate = (perf / tot) if tot else 0.0\n",
        "        ds_rates.append(rate)\n",
        "        if rate > 0:\n",
        "            any_perfect = True\n",
        "    rates_by_ds[ds] = ds_rates\n",
        "\n",
        "if any_perfect:\n",
        "    fig_perf = go.Figure()\n",
        "    for ds in ordered_datasets:\n",
        "        fig_perf.add_trace(go.Bar(\n",
        "            name=ds,\n",
        "            x=tools,\n",
        "            y=rates_by_ds[ds],\n",
        "            text=[f\"{r:.1%}\" for r in rates_by_ds[ds]],\n",
        "            textposition='auto'\n",
        "        ))\n",
        "    fig_perf.update_layout(\n",
        "        barmode='group',\n",
        "        title=\"Perfect-sentence tagging rate (STRICT) by tool and dataset\",\n",
        "        xaxis_title=\"Tool\",\n",
        "        yaxis_title=\"Perfect sentences\",\n",
        "        yaxis=dict(range=[0, 1])\n",
        "    )\n",
        "    fig_perf.show()\n",
        "else:\n",
        "    print(\"Perfect-sentence tagging rate is 0% for all tools/datasets under STRICT; plot omitted.\")\n",
        "\n",
        "print(\"Visualizations complete.\")\n"
      ],
      "metadata": {
        "id": "mi70I58hI1el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ADDITIONAL DATA VISUALIZATIONS (STRICT, pooled Dubliners + BNC)\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# --- ensure wilson_results exists (fallback compute if missing) ----------------\n",
        "def _wilson_interval(successes, n, confidence=0.95):\n",
        "    if n == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    from math import sqrt\n",
        "    from scipy.stats import norm\n",
        "    z = norm.ppf(1 - (1 - confidence) / 2)\n",
        "    p = successes / n\n",
        "    denom = 1 + z*z/n\n",
        "    centre = (p + z*z/(2*n)) / denom\n",
        "    half = z * np.sqrt((p*(1-p) + z*z/(4*n))/n) / denom\n",
        "    return p, max(0.0, centre - half), min(1.0, centre + half)\n",
        "\n",
        "if 'wilson_results' not in globals() or not wilson_results:\n",
        "    # build from expanded_batch_results\n",
        "    wilson_results = {}\n",
        "    discovered_tools = sorted({t for r in expanded_batch_results for t in r.get('tool_results', {})})\n",
        "    for tool_name in discovered_tools:\n",
        "        total_tokens = 0\n",
        "        correct_tokens = 0\n",
        "        total_sentences = 0\n",
        "        perfect_sentences = 0\n",
        "        for res in expanded_batch_results:\n",
        "            tri = res['tool_results'].get(tool_name, {})\n",
        "            if not isinstance(tri, dict) or ('error' in tri):\n",
        "                continue\n",
        "            gt = res['ground_truth']\n",
        "            pred = tri.get('tags', [])\n",
        "            toks = tri.get('tokens', [])\n",
        "            m = min(len(gt), len(pred), len(toks))\n",
        "            if m == 0:\n",
        "                continue\n",
        "            gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "            corr = sum(1 for i in range(m) if gt_penn[i] == pred[i])\n",
        "            correct_tokens += corr\n",
        "            total_tokens += m\n",
        "            total_sentences += 1\n",
        "            if corr == m:\n",
        "                perfect_sentences += 1\n",
        "        if total_tokens > 0:\n",
        "            p, lo, hi = _wilson_interval(correct_tokens, total_tokens, 0.95)\n",
        "            pr, pr_lo, pr_hi = _wilson_interval(perfect_sentences, total_sentences, 0.95)\n",
        "            wilson_results[tool_name] = {\n",
        "                'token_accuracy': p,\n",
        "                'token_ci_lower': lo,\n",
        "                'token_ci_upper': hi,\n",
        "                'perfect_rate': pr,\n",
        "                'perfect_ci_lower': pr_lo,\n",
        "                'perfect_ci_upper': pr_hi,\n",
        "                'total_tokens': total_tokens,\n",
        "                'correct_tokens': correct_tokens,\n",
        "                'total_sentences': total_sentences,\n",
        "                'perfect_sentences': perfect_sentences\n",
        "            }\n",
        "\n",
        "# --- 1) Wilson CI comparison plot ---------------------------------------------\n",
        "tools_order = sorted(wilson_results.keys())\n",
        "ci_points = [(t, wilson_results[t]['token_accuracy'],\n",
        "              wilson_results[t]['token_ci_lower'],\n",
        "              wilson_results[t]['token_ci_upper']) for t in tools_order]\n",
        "\n",
        "x_min = min(lo for _, _, lo, _ in ci_points) if ci_points else 0.0\n",
        "x_max = max(hi for _, _, _, hi in ci_points) if ci_points else 1.0\n",
        "pad = max(0.01, (x_max - x_min) * 0.1)\n",
        "x_range = [max(0.0, x_min - pad), min(1.0, x_max + pad)]\n",
        "\n",
        "fig_ci = go.Figure()\n",
        "for tool, p, lo, hi in ci_points:\n",
        "    # point\n",
        "    fig_ci.add_trace(go.Scatter(\n",
        "        x=[p], y=[tool], mode='markers',\n",
        "        marker=dict(size=12),\n",
        "        name=tool, showlegend=False,\n",
        "        hovertemplate=f\"{tool}<br>Accuracy={p:.3%}<br>95% CI=[{lo:.3%}, {hi:.3%}]<extra></extra>\"\n",
        "    ))\n",
        "    # CI segment\n",
        "    fig_ci.add_trace(go.Scatter(\n",
        "        x=[lo, hi], y=[tool, tool], mode='lines',\n",
        "        line=dict(width=4), showlegend=False\n",
        "    ))\n",
        "\n",
        "fig_ci.update_layout(\n",
        "    title=\"Token Accuracy with 95% Wilson CIs (STRICT)<br><sub>Dubliners + BNC, pooled</sub>\",\n",
        "    xaxis_title=\"Token-level accuracy\",\n",
        "    yaxis_title=\"Tool\",\n",
        "    xaxis=dict(range=x_range, tickformat='.0%'),\n",
        "    height=420\n",
        ")\n",
        "fig_ci.show()\n",
        "\n",
        "# --- helper: strict sentence accuracy -----------------------------------------\n",
        "def _strict_sentence_acc(gt_claws, pred_tags, pred_tokens):\n",
        "    m = min(len(gt_claws), len(pred_tags), len(pred_tokens))\n",
        "    if m <= 0:\n",
        "        return None\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt_claws[:m]]\n",
        "    return sum(1 for i in range(m) if gt_penn[i] == pred_tags[i]) / m\n",
        "\n",
        "# --- 2) Error heatmap: most problematic CLAWS tags (by strict mismatch) -------\n",
        "# We compare strict-projected gold vs predicted Penn, but attribute errors to the\n",
        "# original CLAWS source tag (optionally collapsed via _strip_ditto if desired).\n",
        "strip_ditto = True  # set False if you want the raw CLAWS variants\n",
        "\n",
        "tag_errors = defaultdict(lambda: defaultdict(int))  # CLAWS -> tool -> count\n",
        "tools_for_heatmap = tools_order  # same tools\n",
        "\n",
        "for res in expanded_batch_results:\n",
        "    gt_claws = res.get('ground_truth', [])\n",
        "    # precompute strict-projected gold\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt_claws]\n",
        "    for tool in tools_for_heatmap:\n",
        "        tri = res.get('tool_results', {}).get(tool, {})\n",
        "        if not isinstance(tri, dict) or ('error' in tri):\n",
        "            continue\n",
        "        pred = tri.get('tags', []) or []\n",
        "        toks = tri.get('tokens', []) or []\n",
        "        m = min(len(gt_penn), len(pred), len(toks), len(gt_claws))\n",
        "        if m == 0:\n",
        "            continue\n",
        "        for i in range(m):\n",
        "            if gt_penn[i] != pred[i]:\n",
        "                claws_src = _strip_ditto(gt_claws[i]) if strip_ditto else gt_claws[i]\n",
        "                tag_errors[claws_src][tool] += 1\n",
        "\n",
        "# pick top 12 tags by total error across tools\n",
        "tag_totals = {tag: sum(cnts.values()) for tag, cnts in tag_errors.items()}\n",
        "top_tags = [t for t, _ in sorted(tag_totals.items(), key=lambda x: x[1], reverse=True)[:12]]\n",
        "\n",
        "heatmap_data = []\n",
        "for tag in top_tags:\n",
        "    row = [tag_errors[tag].get(tool, 0) for tool in tools_for_heatmap]\n",
        "    heatmap_data.append(row)\n",
        "\n",
        "fig_heatmap = go.Figure(data=go.Heatmap(\n",
        "    z=heatmap_data,\n",
        "    x=tools_for_heatmap,\n",
        "    y=top_tags,\n",
        "    colorscale='Reds',\n",
        "    text=heatmap_data,\n",
        "    texttemplate=\"%{text}\",\n",
        "    textfont={\"size\": 10},\n",
        "    hovertemplate=\"Tag=%{y}<br>Tool=%{x}<br>Errors=%{z}<extra></extra>\"\n",
        "))\n",
        "fig_heatmap.update_layout(\n",
        "    title=\"Most Problematic CLAWS Tags (STRICT mismatches attributed to CLAWS source)\",\n",
        "    xaxis_title=\"Tool\",\n",
        "    yaxis_title=\"CLAWS tag\",\n",
        "    height=520\n",
        ")\n",
        "fig_heatmap.show()\n",
        "\n",
        "# --- 3) Sentence length vs accuracy (STRICT), colored by tool -----------------\n",
        "points = []\n",
        "for res in expanded_batch_results:\n",
        "    ds = res.get('dataset', 'Unknown')\n",
        "    gt = res.get('ground_truth', [])\n",
        "    sent_len = len(gt)\n",
        "    for tool in tools_order:\n",
        "        tri = res.get('tool_results', {}).get(tool, {})\n",
        "        if not isinstance(tri, dict) or ('error' in tri):\n",
        "            continue\n",
        "        pred = tri.get('tags', []) or []\n",
        "        toks = tri.get('tokens', []) or []\n",
        "        acc = _strict_sentence_acc(gt, pred, toks)\n",
        "        if acc is not None:\n",
        "            points.append((sent_len, acc, tool, ds))\n",
        "\n",
        "import pandas as pd\n",
        "df_scatter = pd.DataFrame(points, columns=['length', 'accuracy', 'tool', 'dataset'])\n",
        "\n",
        "fig_scatter = go.Figure()\n",
        "palette = {\n",
        "    'flair': '#2ca02c',\n",
        "    'spacy_lg': '#ff7f0e',\n",
        "    'spacy_sm': '#1f77b4',\n",
        "    'nltk': '#9467bd'\n",
        "}\n",
        "for tool in sorted(df_scatter['tool'].unique()):\n",
        "    sub = df_scatter[df_scatter['tool'] == tool]\n",
        "    fig_scatter.add_trace(go.Scatter(\n",
        "        x=sub['length'], y=sub['accuracy'],\n",
        "        mode='markers', name=tool,\n",
        "        marker=dict(size=6, opacity=0.6, color=palette.get(tool, '#636EFA')),\n",
        "        hovertemplate=(\"Tool=\"+tool+\"<br>Len=%{x}<br>Acc=%{y:.1%}\"\n",
        "                       \"<br>Dataset=%{text}<extra></extra>\"),\n",
        "        text=sub['dataset']\n",
        "    ))\n",
        "\n",
        "# Trend line (overall)\n",
        "if not df_scatter.empty:\n",
        "    z = np.polyfit(df_scatter['length'], df_scatter['accuracy'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    x_trend = np.linspace(df_scatter['length'].min(), df_scatter['length'].max(), 100)\n",
        "    fig_scatter.add_trace(go.Scatter(\n",
        "        x=x_trend, y=p(x_trend), mode='lines', name='Trend',\n",
        "        line=dict(color='red', dash='dash')\n",
        "    ))\n",
        "\n",
        "fig_scatter.update_layout(\n",
        "    title=\"Sentence Length vs Token Accuracy (STRICT)<br><sub>Dubliners + BNC</sub>\",\n",
        "    xaxis_title=\"Sentence length (tokens in CLAWS gold)\",\n",
        "    yaxis_title=\"Token accuracy\",\n",
        "    yaxis=dict(range=[0, 1], tickformat='.0%'),\n",
        "    height=520\n",
        ")\n",
        "fig_scatter.show()\n",
        "\n",
        "# --- 4) Distribution of per-sentence accuracies (STRICT) ----------------------\n",
        "tool_performance = defaultdict(list)\n",
        "for res in expanded_batch_results:\n",
        "    gt = res.get('ground_truth', [])\n",
        "    tri_map = res.get('tool_results', {})\n",
        "    for tool in tools_order:\n",
        "        tri = tri_map.get(tool, {})\n",
        "        if not isinstance(tri, dict) or ('error' in tri):\n",
        "            continue\n",
        "        pred = tri.get('tags', []) or []\n",
        "        toks = tri.get('tokens', []) or []\n",
        "        acc = _strict_sentence_acc(gt, pred, toks)\n",
        "        if acc is not None:\n",
        "            tool_performance[tool].append(acc)\n",
        "\n",
        "fig_dist = go.Figure()\n",
        "for tool in tools_order:\n",
        "    accs = tool_performance.get(tool, [])\n",
        "    if not accs:\n",
        "        continue\n",
        "    fig_dist.add_trace(go.Box(\n",
        "        y=accs, name=tool, boxpoints='outliers',\n",
        "        hovertemplate=f\"{tool}<br>Q1–Q3 / median shown<extra></extra>\"\n",
        "    ))\n",
        "fig_dist.update_layout(\n",
        "    title=\"Distribution of Sentence-Level Accuracies (STRICT)\",\n",
        "    xaxis_title=\"Tool\",\n",
        "    yaxis_title=\"Sentence accuracy\",\n",
        "    yaxis=dict(range=[0, 1], tickformat='.0%'),\n",
        "    height=520\n",
        ")\n",
        "fig_dist.show()\n",
        "\n",
        "print(\"Enhanced visualizations complete!\")\n"
      ],
      "metadata": {
        "id": "YO2ne_dwK-cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# POWERFUL ADDITIONAL VISUALS: per-dataset CIs, McNemar composition, per-tag F1\n",
        "# ==============================================================================\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# -- Safety: Wilson CI helper (if not in scope)\n",
        "if 'wilson_confidence_interval' not in globals():\n",
        "    from math import sqrt\n",
        "    import scipy.stats as stats\n",
        "    def wilson_confidence_interval(successes, trials, confidence=0.95):\n",
        "        if trials == 0: return 0.0, 0.0, 0.0\n",
        "        z = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
        "        p = successes / trials\n",
        "        denom = 1 + z**2 / trials\n",
        "        centre = (p + z**2/(2*trials)) / denom\n",
        "        half = z * np.sqrt((p*(1-p) + z**2/(4*trials))/trials) / denom\n",
        "        return p, max(0, centre - half), min(1, centre + half)\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Per-dataset Wilson CIs\n",
        "# -------------------------------\n",
        "def compute_wilson_by_dataset(results):\n",
        "    tool_set = set()\n",
        "    for r in results:\n",
        "        tool_set.update(r.get('tool_results', {}).keys())\n",
        "    datasets = sorted({r['dataset'] for r in results})\n",
        "\n",
        "    out = {ds:{} for ds in datasets}\n",
        "    for ds in datasets:\n",
        "        for tool in sorted(tool_set):\n",
        "            total_tokens = correct = sentences = 0\n",
        "            for r in results:\n",
        "                if r['dataset'] != ds:\n",
        "                    continue\n",
        "                tri = r['tool_results'].get(tool, {})\n",
        "                if not isinstance(tri, dict) or 'error' in tri:\n",
        "                    continue\n",
        "                gt = r['ground_truth']\n",
        "                pred = tri.get('tags', [])\n",
        "                toks = tri.get('tokens', [])\n",
        "                m = min(len(gt), len(pred), len(toks))\n",
        "                if m == 0:\n",
        "                    continue\n",
        "                gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "                correct += sum(1 for i in range(m) if gt_penn[i] == pred[i])\n",
        "                total_tokens += m\n",
        "                sentences += 1\n",
        "            if total_tokens > 0:\n",
        "                p, lo, hi = wilson_confidence_interval(correct, total_tokens)\n",
        "                out[ds][tool] = dict(acc=p, lo=lo, hi=hi, tokens=total_tokens, sents=sentences)\n",
        "    return out\n",
        "\n",
        "ci_by_ds = compute_wilson_by_dataset(expanded_batch_results)\n",
        "\n",
        "# Plot: one combined horizontal CI chart (labels \"Dataset • Tool\")\n",
        "fig_ci_ds = go.Figure()\n",
        "y_labels, x_pts, x_los, x_his = [], [], [], []\n",
        "for ds in ci_by_ds:\n",
        "    for tool, d in sorted(ci_by_ds[ds].items(), key=lambda kv: kv[1]['acc'], reverse=True):\n",
        "        y_labels.append(f\"{ds} • {tool}\")\n",
        "        x_pts.append(d['acc']); x_los.append(d['lo']); x_his.append(d['hi'])\n",
        "\n",
        "for i, y in enumerate(y_labels):\n",
        "    fig_ci_ds.add_trace(go.Scatter(x=[x_pts[i]], y=[y], mode='markers',\n",
        "                                   marker=dict(size=10), name=y, showlegend=False))\n",
        "    fig_ci_ds.add_trace(go.Scatter(x=[x_los[i], x_his[i]], y=[y, y], mode='lines',\n",
        "                                   line=dict(width=3), showlegend=False))\n",
        "\n",
        "fig_ci_ds.update_layout(\n",
        "    title=\"Token Accuracy with 95% Wilson CIs by Dataset (STRICT)\",\n",
        "    xaxis_title=\"Token Accuracy\",\n",
        "    yaxis_title=\"Dataset • Tool\",\n",
        "    xaxis=dict(range=[0.45, 0.65]),\n",
        "    height=400 + 20*len(y_labels)\n",
        ")\n",
        "fig_ci_ds.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2) McNemar composition bars (interpretable disagreements)\n",
        "# ---------------------------------------------------------\n",
        "def build_mcnemar_table(tool_a, tool_b, results):\n",
        "    bc = bw = a_cbw = b_caw = 0\n",
        "    for r in results:\n",
        "        gt = r['ground_truth']\n",
        "        A = r['tool_results'].get(tool_a, {})\n",
        "        B = r['tool_results'].get(tool_b, {})\n",
        "        if 'error' in A or 'error' in B:\n",
        "            continue\n",
        "        pa, pb = A.get('tags', []), B.get('tags', [])\n",
        "        ta, tb = A.get('tokens', []), B.get('tokens', [])\n",
        "        m = min(len(gt), len(pa), len(pb), len(ta), len(tb))\n",
        "        if m == 0:\n",
        "            continue\n",
        "        gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "        for i in range(m):\n",
        "            a_ok = (gt_penn[i] == pa[i]); b_ok = (gt_penn[i] == pb[i])\n",
        "            if a_ok and b_ok: bc += 1\n",
        "            elif (not a_ok) and (not b_ok): bw += 1\n",
        "            elif a_ok and (not b_ok): a_cbw += 1\n",
        "            else: b_caw += 1\n",
        "    return [[bc, a_cbw],[b_caw, bw]]\n",
        "\n",
        "# Build a stacked horizontal bar per pair\n",
        "tools_for_pairs = sorted({t for r in expanded_batch_results for t in r.get('tool_results', {}).keys()})\n",
        "pair_labels = []\n",
        "BC = []; A_C_B_W = []; B_C_A_W = []; BW = []\n",
        "\n",
        "for a, b in itertools.combinations(tools_for_pairs, 2):\n",
        "    t = build_mcnemar_table(a, b, expanded_batch_results)\n",
        "    pair_labels.append(f\"{a} vs {b}\")\n",
        "    BC.append(t[0][0]); A_C_B_W.append(t[0][1]); B_C_A_W.append(t[1][0]); BW.append(t[1][1])\n",
        "\n",
        "fig_mcnemar = go.Figure()\n",
        "fig_mcnemar.add_trace(go.Bar(y=pair_labels, x=BC, name=\"Both correct\", orientation='h'))\n",
        "fig_mcnemar.add_trace(go.Bar(y=pair_labels, x=A_C_B_W, name=\"Only A correct\", orientation='h'))\n",
        "fig_mcnemar.add_trace(go.Bar(y=pair_labels, x=B_C_A_W, name=\"Only B correct\", orientation='h'))\n",
        "fig_mcnemar.add_trace(go.Bar(y=pair_labels, x=BW, name=\"Both wrong\", orientation='h'))\n",
        "\n",
        "fig_mcnemar.update_layout(\n",
        "    barmode='stack',\n",
        "    title=\"Paired Outcomes per Token (McNemar table visualisation)\",\n",
        "    xaxis_title=\"Token count\",\n",
        "    yaxis_title=\"Tool pair\",\n",
        "    height=300 + 30*len(pair_labels)\n",
        ")\n",
        "fig_mcnemar.show()\n",
        "\n",
        "# --------------------------------------------\n",
        "# 3) Per-tag F1 (top support, excluding 'UNK')\n",
        "# --------------------------------------------\n",
        "def per_tool_ytrue_ypred(results):\n",
        "    per_tool = defaultdict(lambda: {'y_true': [], 'y_pred': []})\n",
        "    for r in results:\n",
        "        gt = r['ground_truth']\n",
        "        for tool, tri in r.get('tool_results', {}).items():\n",
        "            if 'error' in tri:\n",
        "                continue\n",
        "            pred = tri.get('tags', [])\n",
        "            toks = tri.get('tokens', [])\n",
        "            m = min(len(gt), len(pred), len(toks))\n",
        "            if m == 0:\n",
        "                continue\n",
        "            gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "            per_tool[tool]['y_true'].extend(gt_penn)\n",
        "            per_tool[tool]['y_pred'].extend(pred[:m])\n",
        "    return per_tool\n",
        "\n",
        "per_tool_arrays = per_tool_ytrue_ypred(expanded_batch_results)\n",
        "\n",
        "# Define label set and supports (pooled gold)\n",
        "label_support = Counter()\n",
        "for d in per_tool_arrays.values():\n",
        "    label_support.update(d['y_true'])\n",
        "# exclude UNK for readability\n",
        "labels_ranked = [lbl for lbl, _ in label_support.most_common() if lbl != 'UNK']\n",
        "top_labels = labels_ranked[:10] if len(labels_ranked) >= 10 else labels_ranked\n",
        "\n",
        "fig_f1 = go.Figure()\n",
        "for tool, d in sorted(per_tool_arrays.items()):\n",
        "    y_true = np.array(d['y_true'])\n",
        "    y_pred = np.array(d['y_pred'])\n",
        "    if y_true.size == 0:\n",
        "        continue\n",
        "    # compute per-class metrics restricted to top_labels\n",
        "    p, r, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=top_labels, average=None, zero_division=0\n",
        "    )\n",
        "    fig_f1.add_trace(go.Bar(x=top_labels, y=f1, name=tool))\n",
        "\n",
        "fig_f1.update_layout(\n",
        "    title=\"Per-tag F1 by Tool (Top 10 Gold Labels, STRICT; UNK excluded)\",\n",
        "    xaxis_title=\"Penn tag\",\n",
        "    yaxis_title=\"F1\",\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    barmode='group',\n",
        "    height=450\n",
        ")\n",
        "fig_f1.show()\n",
        "\n",
        "print(\"Additional visuals generated: per-dataset CIs (fig_ci_ds), McNemar composition (fig_mcnemar), per-tag F1 (fig_f1).\")\n"
      ],
      "metadata": {
        "id": "7Um4khrNfOME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SAVE VISUALIZATIONS + RICH DASHBOARD (STRICT, pooled Dubliners + BNC)\n",
        "# Includes: more results sections + expanded methods + CLAWS C7 note\n",
        "# ==============================================================================\n",
        "\n",
        "import os, zipfile\n",
        "from datetime import datetime\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def _exists(name):  # object exists and is not None\n",
        "    return name in globals() and globals()[name] is not None\n",
        "\n",
        "assert 'wilson_results' in globals() and wilson_results, \"wilson_results not found. Run the CI cell first.\"\n",
        "\n",
        "# ---------------- Core stats for dashboard header ----------------\n",
        "_tool_stats = {k: v for k, v in wilson_results.items() if v.get('total_tokens', 0) > 0}\n",
        "tools_sorted = sorted(_tool_stats.keys(), key=lambda t: _tool_stats[t]['token_accuracy'], reverse=True)\n",
        "best_tool = tools_sorted[0]\n",
        "best = _tool_stats[best_tool]\n",
        "sentences_evaluated = max(s['total_sentences'] for s in _tool_stats.values())\n",
        "\n",
        "def _proportion_z_test(x1, n1, x2, n2):\n",
        "    p_pool = (x1 + x2) / (n1 + n2)\n",
        "    se = sqrt(p_pool * (1 - p_pool) * (1/n1 + 1/n2))\n",
        "    z = (x1/n1 - x2/n2) / se\n",
        "    p = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "    return z, p\n",
        "\n",
        "def _cohens_h(p1, p2):\n",
        "    return 2 * (np.arcsin(sqrt(p1)) - np.arcsin(sqrt(p2)))\n",
        "\n",
        "worst_tool = tools_sorted[-1]\n",
        "worst = _tool_stats[worst_tool]\n",
        "z_stat, p_val = _proportion_z_test(best['correct_tokens'], best['total_tokens'],\n",
        "                                   worst['correct_tokens'], worst['total_tokens'])\n",
        "h = _cohens_h(best['token_accuracy'], worst['token_accuracy'])\n",
        "h_mag = (\"negligible\" if abs(h) < 0.2 else\n",
        "         \"small\" if abs(h) < 0.5 else\n",
        "         \"medium\" if abs(h) < 0.8 else \"large\")\n",
        "\n",
        "best_acc = best['token_accuracy']\n",
        "best_lo  = best['token_ci_lower']\n",
        "best_hi  = best['token_ci_upper']\n",
        "best_perfect = best['perfect_rate']\n",
        "best_perfect_hi = best['perfect_ci_upper']\n",
        "\n",
        "# ---------------- Pairwise Newcombe–Wilson diffs -----------------\n",
        "if 'newcombe_wilson_diff' not in globals():\n",
        "    def wilson_interval(successes, n, confidence=0.95):\n",
        "        if n == 0: return (0.0, 0.0, 0.0)\n",
        "        z = stats.norm.ppf(1 - (1 - confidence)/2)\n",
        "        p = successes / n\n",
        "        denom = 1 + z*z/n\n",
        "        centre = (p + z*z/(2*n)) / denom\n",
        "        half = z * np.sqrt((p*(1-p) + z*z/(4*n))/n) / denom\n",
        "        return (p, max(0.0, centre - half), min(1.0, centre + half))\n",
        "\n",
        "    def newcombe_wilson_diff(x1, n1, x2, n2, confidence=0.95):\n",
        "        p1, L1, U1 = wilson_interval(x1, n1, confidence)\n",
        "        p2, L2, U2 = wilson_interval(x2, n2, confidence)\n",
        "        diff = p1 - p2\n",
        "        lo = L1 - U2\n",
        "        hi = U1 - L2\n",
        "        return diff, lo, hi, (p1, L1, U1), (p2, L2, U2)\n",
        "\n",
        "pair_rows = []\n",
        "for i in range(len(tools_sorted)):\n",
        "    for j in range(i+1, len(tools_sorted)):\n",
        "        a, b = tools_sorted[i], tools_sorted[j]\n",
        "        A, B = _tool_stats[a], _tool_stats[b]\n",
        "        d, lo, hi, p1info, p2info = newcombe_wilson_diff(\n",
        "            A['correct_tokens'], A['total_tokens'], B['correct_tokens'], B['total_tokens']\n",
        "        )\n",
        "        pair_rows.append((a, b, d, lo, hi))\n",
        "\n",
        "# -------------- Optional: per-dataset CI table data ---------------\n",
        "have_ci_by_ds = _exists('ci_by_ds') and bool(ci_by_ds)\n",
        "if have_ci_by_ds:\n",
        "    # flatten for a small table\n",
        "    per_ds_rows = []\n",
        "    for ds, d in ci_by_ds.items():\n",
        "        for tool, vals in d.items():\n",
        "            per_ds_rows.append((ds, tool, vals['acc'], vals['lo'], vals['hi'], vals['tokens'], vals['sents']))\n",
        "    # sort by dataset then descending acc\n",
        "    per_ds_rows.sort(key=lambda x: (x[0], -x[2]))\n",
        "\n",
        "# -------------- Optional: McNemar outcomes table data -------------\n",
        "have_mcnemar = _exists('mcnemar_outcomes') and _exists('adj_pvals') and len(mcnemar_outcomes) == len(adj_pvals)\n",
        "if have_mcnemar:\n",
        "    # prepare rows: a, b, table, discordant, test, chi2, p, adj\n",
        "    mcn_rows = []\n",
        "    for o, adjp in zip(mcnemar_outcomes, adj_pvals):\n",
        "        a = o['tool_a']; b = o['tool_b']; t = o['table']\n",
        "        test_name = 'exact' if o.get('exact') else 'chi²'\n",
        "        mcn_rows.append((a, b, t, o['discordant'], test_name, o['chi2'], o['p_value'], float(adjp)))\n",
        "\n",
        "# -------------- Optional: CLAWS C7 UNK share & top unmappables ----\n",
        "have_strict_report = _exists('strict_report') and isinstance(strict_report, dict) and 'unk_rate' in strict_report\n",
        "unk_share_pct = f\"{strict_report['unk_rate']*100:.1f}%\" if have_strict_report else None\n",
        "top_unks = None\n",
        "if have_strict_report:\n",
        "    # top 10 unmappable CLAWS sources\n",
        "    from collections import Counter\n",
        "    cnts: Counter = strict_report.get('counts', Counter())\n",
        "    top_unks = cnts.most_common(10)\n",
        "\n",
        "# ---------------- Save figures to disk --------------------------------\n",
        "results_dir = \"nlp_validation_results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "print(f\"Saving visualizations to {results_dir}/ directory...\")\n",
        "\n",
        "saved = []\n",
        "def _save(fig_var, filename):\n",
        "    if _exists(fig_var):\n",
        "        globals()[fig_var].write_html(\n",
        "            f\"{results_dir}/{filename}\",\n",
        "            config={'displayModeBar': True, 'displaylogo': False}\n",
        "        )\n",
        "        saved.append(filename)\n",
        "\n",
        "# Existing\n",
        "_save('fig_ci',        \"confidence_intervals.html\")\n",
        "_save('fig_heatmap',   \"error_heatmap.html\")\n",
        "_save('fig_scatter',   \"sentence_length_analysis.html\")\n",
        "_save('fig_dist',      \"accuracy_distributions.html\")\n",
        "_save('fig',           \"accuracy_comparison.html\")\n",
        "\n",
        "# New visuals included earlier\n",
        "_save('fig_ci_ds',     \"per_dataset_confidence_intervals.html\")\n",
        "_save('fig_mcnemar',   \"mcnemar_composition.html\")\n",
        "_save('fig_f1',        \"per_tag_f1.html\")\n",
        "\n",
        "# ---------------- Build dashboard HTML ----------------------------\n",
        "now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "def _link(fn, label):\n",
        "    return f'<a href=\"{fn}\" class=\"chart-link\">{label}</a>\\n' if fn in saved else ''\n",
        "\n",
        "dashboard_html = f\"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <meta charset=\"utf-8\" />\n",
        "    <title>NLP POS Tagging Validation — Dubliners + BNC (STRICT)</title>\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
        "        .header {{ background-color: #2c3e50; color: white; padding: 20px; margin-bottom: 20px; border-radius: 6px; }}\n",
        "        .summary, .chart-container {{ background-color: white; padding: 20px; margin-bottom: 20px; border-radius: 6px; }}\n",
        "        .chart-link {{ display: inline-block; background-color: #3498db; color: white; padding: 10px 16px;\n",
        "                       text-decoration: none; border-radius: 5px; margin: 5px 8px 0 0; }}\n",
        "        .chart-link:hover {{ background-color: #2980b9; }}\n",
        "        .stats-table {{ width: 100%; border-collapse: collapse; margin-top: 10px; }}\n",
        "        .stats-table th, .stats-table td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "        .stats-table th {{ background-color: #f2f2f2; }}\n",
        "        .muted {{ color: #666; }}\n",
        "        .small {{ font-size: 0.95em; }}\n",
        "        code {{ background: #f0f0f0; padding: 1px 4px; border-radius: 3px; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\">\n",
        "        <h1>NLP POS Tagging Validation — Dubliners + BNC (STRICT)</h1>\n",
        "        <p>spaCy (sm, lg), Flair, and NLTK vs. CLAWS7 gold after strict CLAWS→Penn projection</p>\n",
        "        <p class=\"muted\">Analysis Date: {now_str}</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Executive Summary</h2>\n",
        "        <p><strong>Design:</strong> Token-level evaluation across {sentences_evaluated} sentences, pooled over Dubliners + BNC.</p>\n",
        "        <ul>\n",
        "            <li><strong>Best observed accuracy:</strong> {best_tool} = {best_acc:.3f} (95% CI {best_lo:.3f}–{best_hi:.3f}).</li>\n",
        "            <li><strong>Perfect-sentence rate (illustrative):</strong> ≈ {best_perfect:.1%} (upper 95% CI ≈ {best_perfect_hi:.1%}).</li>\n",
        "            <li><strong>Best vs worst:</strong> z = {z_stat:.3f}, p = {p_val:.3g}; Cohen’s h = {h:.3f} ({h_mag}).</li>\n",
        "        </ul>\n",
        "        <p class=\"muted small\">Strict projection maps CLAWS distinctions lacking Penn equivalents to <code>UNK</code>, capping achievable accuracy.</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Token Accuracy (STRICT)</h2>\n",
        "        <table class=\"stats-table\">\n",
        "            <tr>\n",
        "                <th>Tool</th>\n",
        "                <th>Token Accuracy</th>\n",
        "                <th>95% CI Lower</th>\n",
        "                <th>95% CI Upper</th>\n",
        "                <th>Tokens</th>\n",
        "                <th>Sentences</th>\n",
        "                <th>Perfect Sentence Rate</th>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "\n",
        "for tool_name in tools_sorted:\n",
        "    s = _tool_stats[tool_name]\n",
        "    dashboard_html += f\"\"\"\n",
        "            <tr>\n",
        "                <td><strong>{tool_name}</strong></td>\n",
        "                <td>{s['token_accuracy']:.3f}</td>\n",
        "                <td>{s['token_ci_lower']:.3f}</td>\n",
        "                <td>{s['token_ci_upper']:.3f}</td>\n",
        "                <td>{s['total_tokens']:,}</td>\n",
        "                <td>{s['total_sentences']}</td>\n",
        "                <td>{s['perfect_rate']:.1%}</td>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "\n",
        "dashboard_html += \"\"\"\n",
        "        </table>\n",
        "    </div>\n",
        "\"\"\"\n",
        "\n",
        "# --------- Per-dataset CI table section (if available) ----------\n",
        "if have_ci_by_ds and per_ds_rows:\n",
        "    dashboard_html += \"\"\"\n",
        "    <div class=\"summary\">\n",
        "        <h2>Per-Dataset Token Accuracy (STRICT)</h2>\n",
        "        <table class=\"stats-table\">\n",
        "            <tr><th>Dataset</th><th>Tool</th><th>Accuracy</th><th>95% CI Lower</th><th>95% CI Upper</th><th>Tokens</th><th>Sentences</th></tr>\n",
        "    \"\"\"\n",
        "    for ds, tool, acc, lo, hi, toks, sents in per_ds_rows:\n",
        "        dashboard_html += f\"<tr><td>{ds}</td><td>{tool}</td><td>{acc:.3f}</td><td>{lo:.3f}</td><td>{hi:.3f}</td><td>{toks:,}</td><td>{sents}</td></tr>\\n\"\n",
        "    dashboard_html += \"\"\"\n",
        "        </table>\n",
        "        <p class=\"muted small\">Wilson score intervals per dataset; useful for checking consistency across Dubliners and BNC.</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# --------- Pairwise Newcombe–Wilson diffs table -----------------\n",
        "if pair_rows:\n",
        "    dashboard_html += \"\"\"\n",
        "    <div class=\"summary\">\n",
        "        <h2>Pairwise Accuracy Differences (Newcombe–Wilson 95% CIs)</h2>\n",
        "        <table class=\"stats-table\">\n",
        "            <tr><th>Comparison</th><th>Δ (A−B)</th><th>95% CI Lower</th><th>95% CI Upper</th><th>Interpretation</th></tr>\n",
        "    \"\"\"\n",
        "    for a, b, d, lo, hi in pair_rows:\n",
        "        interp = \"A>B (CI excludes 0)\" if lo > 0 else (\"B>A (CI excludes 0)\" if hi < 0 else \"No clear difference\")\n",
        "        dashboard_html += f\"<tr><td>{a} − {b}</td><td>{d:.3f}</td><td>{lo:.3f}</td><td>{hi:.3f}</td><td>{interp}</td></tr>\\n\"\n",
        "    dashboard_html += \"\"\"\n",
        "        </table>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# --------- McNemar outcomes summary (if available) --------------\n",
        "if have_mcnemar and mcn_rows:\n",
        "    dashboard_html += \"\"\"\n",
        "    <div class=\"summary\">\n",
        "        <h2>Paired Token Outcomes (McNemar)</h2>\n",
        "        <table class=\"stats-table\">\n",
        "            <tr>\n",
        "                <th>Pair</th>\n",
        "                <th>[both correct, A correct & B wrong]</th>\n",
        "                <th>[B correct & A wrong, both wrong]</th>\n",
        "                <th>Discordant</th>\n",
        "                <th>Test</th>\n",
        "                <th>χ²</th>\n",
        "                <th>p</th>\n",
        "                <th>Holm-adj p</th>\n",
        "            </tr>\n",
        "    \"\"\"\n",
        "    for a, b, t, disc, test_name, chi2, p, adjp in mcn_rows:\n",
        "        t1 = f\"[{t[0][0]}, {t[0][1]}]\"\n",
        "        t2 = f\"[{t[1][0]}, {t[1][1]}]\"\n",
        "        dashboard_html += f\"<tr><td>{a} vs {b}</td><td>{t1}</td><td>{t2}</td><td>{disc}</td><td>{test_name}</td><td>{chi2:.3f}</td><td>{p:.3g}</td><td>{adjp:.3g}</td></tr>\\n\"\n",
        "    dashboard_html += \"\"\"\n",
        "        </table>\n",
        "        <p class=\"muted small\">“Only A correct” and “Only B correct” are the discordant cells that drive McNemar’s test.</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# --------- CLAWS C7 note + live UNK share -----------------------\n",
        "dashboard_html += \"\"\"\n",
        "    <div class=\"summary\">\n",
        "        <h2>About the CLAWS C7 Tagset</h2>\n",
        "        <p>CLAWS C7 encodes fine-grained morphosyntactic distinctions not present in Penn Treebank tags, including article subtypes (<code>AT</code>, <code>AT1</code>), rich pronoun categories by person/number/case (e.g., <code>PPHS1</code> vs <code>PPIS1</code>), auxiliary identity (<code>VB</code>/<code>VH</code>/<code>VD</code> families), preposition subtypes (<code>IO</code>, <code>IW</code>), and semantic noun subclasses (e.g., months/days as <code>NPM*</code>/<code>NPD*</code>). Under <em>strict</em> projection, distinctions that lack Penn equivalents are mapped to <code>UNK</code> to ensure label comparability with modern taggers.</p>\n",
        "\"\"\"\n",
        "\n",
        "if have_strict_report and unk_share_pct:\n",
        "    dashboard_html += f\"\"\"\n",
        "        <p><strong>UNK share under strict projection:</strong> {unk_share_pct} of gold tokens.</p>\n",
        "    \"\"\"\n",
        "    if top_unks:\n",
        "        dashboard_html += \"\"\"\n",
        "        <table class=\"stats-table\">\n",
        "            <tr><th>Top unmappable CLAWS tags</th><th>Count</th></tr>\n",
        "        \"\"\"\n",
        "        for tag, c in top_unks:\n",
        "            dashboard_html += f\"<tr><td>{tag}</td><td>{c}</td></tr>\\n\"\n",
        "        dashboard_html += \"</table>\\n\"\n",
        "\n",
        "dashboard_html += \"\"\"\n",
        "        <p class=\"muted small\">This preserves a fair evaluation against Penn-style outputs but imposes a ceiling on achievable accuracy.</p>\n",
        "    </div>\n",
        "\"\"\"\n",
        "\n",
        "# --------- Visual links (no extra autodiscovery) -----------------\n",
        "dashboard_html += \"\"\"\n",
        "    <div class=\"chart-container\">\n",
        "        <h2>Interactive Visualizations</h2>\n",
        "        <p>Click to open:</p>\n",
        "\"\"\"\n",
        "dashboard_html += _link(\"confidence_intervals.html\",            \"Overall CIs\")\n",
        "dashboard_html += _link(\"per_dataset_confidence_intervals.html\", \"Per-dataset CIs\")\n",
        "dashboard_html += _link(\"mcnemar_composition.html\",             \"McNemar Composition\")\n",
        "dashboard_html += _link(\"per_tag_f1.html\",                      \"Per-tag F1 (Top labels)\")\n",
        "dashboard_html += _link(\"error_heatmap.html\",                   \"Error Heatmap\")\n",
        "dashboard_html += _link(\"sentence_length_analysis.html\",        \"Length vs Accuracy\")\n",
        "dashboard_html += _link(\"accuracy_distributions.html\",          \"Accuracy Distributions\")\n",
        "dashboard_html += _link(\"accuracy_comparison.html\",             \"Tool Comparison\")\n",
        "dashboard_html += \"\"\"\n",
        "    </div>\n",
        "\"\"\"\n",
        "\n",
        "# --------- Expanded Statistical Methods -------------------------\n",
        "dashboard_html += \"\"\"\n",
        "    <div class=\"summary\">\n",
        "        <h2>Statistical Methods</h2>\n",
        "        <ul class=\"small\">\n",
        "            <li><strong>Strict CLAWS→Penn projection:</strong> Gold CLAWS7 tags are mapped to Penn tags; distinctions without Penn equivalents are assigned <code>UNK</code> to maintain label comparability with modern taggers.</li>\n",
        "            <li><strong>Accuracy CIs:</strong> Wilson score intervals for single proportions (token accuracy; perfect-sentence rate). Preferred over Wald due to better coverage, especially away from 0.5.</li>\n",
        "            <li><strong>Pairwise accuracy differences:</strong> Newcombe’s Method 10 (1998) using Wilson intervals for each proportion; CIs reported for Δ = p<sub>A</sub> − p<sub>B</sub>.</li>\n",
        "            <li><strong>Best vs worst significance:</strong> Pooled two-proportion z-test plus effect size Cohen’s h for magnitude (negligible/small/medium/large).</li>\n",
        "            <li><strong>Paired disagreements:</strong> McNemar’s test on discordant token outcomes (exact test when discordant &lt; 25, otherwise continuity-corrected χ²), with Holm–Bonferroni correction for multiple comparisons.</li>\n",
        "            <li><strong>Bootstrap by sentence:</strong> Cluster bootstrap (resampling sentences) for 95% CIs on accuracy differences, preserving within-sentence token dependence.</li>\n",
        "            <li><strong>F1 reporting:</strong> Micro, macro, and weighted F1 computed on Penn labels; per-tag F1 reported for the top gold labels (excluding <code>UNK</code> for readability).</li>\n",
        "            <li><strong>Alignment:</strong> Token comparisons use sentence-internal minimum alignment length across gold/tool tokens to avoid spurious misalignments from tokenizer differences.</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# ---------------- Write dashboard & zip --------------------------\n",
        "results_dir = \"nlp_validation_results\"\n",
        "with open(f\"{results_dir}/index.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(dashboard_html)\n",
        "\n",
        "zip_filename = f\"nlp_validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    for root, dirs, files in os.walk(results_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, results_dir)\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(\"Dashboard updated and all visualizations saved!\")\n",
        "print(f\"Files saved to: {results_dir}/\")\n",
        "print(f\"Zip archive created: {zip_filename}\")\n",
        "\n",
        "# If on Colab, prompt download\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "    colab_files.download(zip_filename)\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "K6pPxv9-feBX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}