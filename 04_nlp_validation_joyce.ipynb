{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ee37d9b2e704395b1d7ea1955ac8513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72dc26e2ff234e94a15b4581e578d365",
              "IPY_MODEL_17ea5532d81f4bee9497cc782415f14d",
              "IPY_MODEL_9bea7cd9855e494dbd4312de9fa7e4ee"
            ],
            "layout": "IPY_MODEL_92468551a36b40138936055d2365c26d"
          }
        },
        "72dc26e2ff234e94a15b4581e578d365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37e5fe0c01cb477fbece2c15ba3bb9b0",
            "placeholder": "​",
            "style": "IPY_MODEL_3c4d6fa87925433ebcc3485155984092",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "17ea5532d81f4bee9497cc782415f14d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c467df025ea74cf7a623a4bc6a6c7eb0",
            "max": 249072763,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b32bafcbb18d45aab41774a91dc430fa",
            "value": 249072763
          }
        },
        "9bea7cd9855e494dbd4312de9fa7e4ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec0245fe2a9044cc87e590bc123b877f",
            "placeholder": "​",
            "style": "IPY_MODEL_c77130e738164bb4be9a4a67c748d5fb",
            "value": " 249M/249M [00:05&lt;00:00, 43.0MB/s]"
          }
        },
        "92468551a36b40138936055d2365c26d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e5fe0c01cb477fbece2c15ba3bb9b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c4d6fa87925433ebcc3485155984092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c467df025ea74cf7a623a4bc6a6c7eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b32bafcbb18d45aab41774a91dc430fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec0245fe2a9044cc87e590bc123b877f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c77130e738164bb4be9a4a67c748d5fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/joyce-dubliners-similes-analysis/blob/main/04_nlp_validation_joyce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# POS Tagging Validation on Joyce’s *Dubliners* with a BNC Comparator\n",
        "\n",
        "This study evaluates contemporary POS taggers against expert CLAWS7 annotations for simile-bearing sentences in James Joyce’s *Dubliners*, and incorporates a 200-sentence sample from the British National Corpus (BNC) as a comparative reference. Gold labels are projected to Penn-style tags under a strict CLAWS→Penn mapping to ensure label comparability across tools.\n",
        "\n",
        "## Research Objectives\n",
        "- Quantify token-level performance of spaCy (sm, lg), Flair, and NLTK against CLAWS7 gold under **strict** CLAWS→Penn projection.\n",
        "- Characterize systematic error patterns (e.g., UNK-induced losses; function-word and verb morphology confusions).\n",
        "- Assess whether performance differences are **Joyce-specific** or reflect a broader ceiling induced by projection.\n",
        "\n",
        "## Corpora\n",
        "- **Dubliners (literary)**: 183 sentences with CLAWS7 gold.\n",
        "- **BNC (reference)**: 200 sentences reconstructed as `Left + Node + Right` (with `Left` possibly empty) to form a single sentence string; CLAWS7 gold provided.\n",
        "- **Total**: 383 sentences. All evaluations report pooled results, with per-dataset confidence intervals provided to disentangle corpus effects.\n",
        "\n",
        "## Preprocessing and Alignment\n",
        "- **Gold normalization**: CLAWS7 tags are projected to Penn under a **strict** policy; distinctions without Penn equivalents are mapped to `UNK`.  \n",
        "- **Per-tool alignment**: For each sentence and tool, comparisons are made over the **per-sentence minimum length** across (gold tokens, tool tokens, tool tags) to avoid spurious misalignments arising from tokenization differences.\n",
        "\n",
        "## Evaluation Protocol\n",
        "- **Primary metrics**: Token accuracy; micro/macro/weighted precision, recall, and F1.  \n",
        "- **Uncertainty & inference**: Wilson score intervals for single-proportion estimates; Newcombe (Wilson) intervals for differences in accuracy; McNemar’s tests with Holm–Bonferroni correction for paired token outcomes; sentence-cluster bootstrap CIs for accuracy differences.\n",
        "\n",
        "## Role of the BNC Comparator\n",
        "The BNC sample functions as a **non-Joycean reference**. If tools perform markedly better on BNC than on *Dubliners*, this supports a **Joyce-specific difficulty** beyond the information loss induced by strict projection. Conversely, comparable ceilings across both corpora indicate that the binding constraint is the **projection itself**, rather than literary idiosyncrasy alone.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# COMPLETE SETUP AND INSTALLATION (NLTK yes, TextBlob no)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "# -q to keep output tidy\n",
        "!pip install -q spacy nltk flair scikit-learn plotly seaborn\n",
        "\n",
        "print(\"\\nDownloading spaCy models...\")\n",
        "# Use python -m to ensure install into the current kernel env\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "print(\"\\nEnsuring NLTK POS tagger resource...\")\n",
        "import nltk\n",
        "\n",
        "def _ensure_nltk_resource(path, downloader_name, verbose=False):\n",
        "    try:\n",
        "        nltk.data.find(path)\n",
        "        return True\n",
        "    except LookupError:\n",
        "        try:\n",
        "            nltk.download(downloader_name, quiet=not verbose)\n",
        "            nltk.data.find(path)  # verify\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "# Ensure POS tagger (try *_eng then classic)\n",
        "tagger_ok = _ensure_nltk_resource('taggers/averaged_perceptron_tagger_eng', 'averaged_perceptron_tagger_eng') or \\\n",
        "            _ensure_nltk_resource('taggers/averaged_perceptron_tagger',     'averaged_perceptron_tagger')\n",
        "\n",
        "print(f\"NLTK averaged_perceptron_tagger available: {'✓' if tagger_ok else '✗'}\")\n",
        "print(\"\\nAll installations attempted. Proceeding to imports...\")\n",
        "\n",
        "# ==============================================================================\n",
        "# IMPORTS AND SETUP\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP libraries\n",
        "import spacy\n",
        "from nltk.tokenize import TreebankWordTokenizer  # avoids punkt/punkt_tab\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Optional: Flair (enabled if import + model load succeed)\n",
        "FLAIR_AVAILABLE = False\n",
        "try:\n",
        "    from flair.data import Sentence\n",
        "    from flair.models import SequenceTagger\n",
        "    FLAIR_AVAILABLE = True\n",
        "    print(\"✓ Flair imported successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Flair not available: {e}\")\n",
        "\n",
        "# Analysis libraries\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from collections import defaultdict, Counter\n",
        "import scipy.stats as stats\n",
        "from math import sqrt\n",
        "\n",
        "# Visualization libraries\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# ==============================================================================\n",
        "# Load NLP models\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\nLoading spaCy models...\")\n",
        "nlp_sm = None\n",
        "nlp_lg = None\n",
        "try:\n",
        "    nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"✓ en_core_web_sm loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to load en_core_web_sm: {e}\")\n",
        "\n",
        "try:\n",
        "    nlp_lg = spacy.load(\"en_core_web_lg\")\n",
        "    print(\"✓ en_core_web_lg loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to load en_core_web_lg: {e}\")\n",
        "\n",
        "# Load Flair model if available\n",
        "flair_tagger = None\n",
        "if FLAIR_AVAILABLE:\n",
        "    try:\n",
        "        print(\"Loading Flair POS tagger (this may take a moment)...\")\n",
        "        flair_tagger = SequenceTagger.load('pos')\n",
        "        print(\"✓ Flair model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to load Flair model: {e}\")\n",
        "        flair_tagger = None\n",
        "        FLAIR_AVAILABLE = False\n",
        "\n",
        "# Final setup summary\n",
        "print(\"\\nSetup Summary:\")\n",
        "print(f\"spaCy sm: {'✓ Available' if nlp_sm is not None else '✗ Not available'}\")\n",
        "print(f\"spaCy lg: {'✓ Available' if nlp_lg is not None else '✗ Not available'}\")\n",
        "print(f\"NLTK averaged_perceptron_tagger: {'✓' if tagger_ok else '✗'} (tokenizer: TreebankWordTokenizer)\")\n",
        "print(f\"Flair: {'✓ Available' if FLAIR_AVAILABLE and flair_tagger is not None else '✗ Not available'}\")\n",
        "print(\"\\nReady to proceed with analysis!\")\n"
      ],
      "metadata": {
        "id": "ftuyS9S31zCk",
        "outputId": "55c8f4e4-4abd-4b10-ae5f-047245e2d6b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4ee37d9b2e704395b1d7ea1955ac8513",
            "72dc26e2ff234e94a15b4581e578d365",
            "17ea5532d81f4bee9497cc782415f14d",
            "9bea7cd9855e494dbd4312de9fa7e4ee",
            "92468551a36b40138936055d2365c26d",
            "37e5fe0c01cb477fbece2c15ba3bb9b0",
            "3c4d6fa87925433ebcc3485155984092",
            "c467df025ea74cf7a623a4bc6a6c7eb0",
            "b32bafcbb18d45aab41774a91dc430fa",
            "ec0245fe2a9044cc87e590bc123b877f",
            "c77130e738164bb4be9a4a67c748d5fb"
          ]
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "Downloading spaCy models...\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "Ensuring NLTK POS tagger resource...\n",
            "NLTK averaged_perceptron_tagger available: ✓\n",
            "\n",
            "All installations attempted. Proceeding to imports...\n",
            "✓ Flair imported successfully\n",
            "\n",
            "Loading spaCy models...\n",
            "✓ en_core_web_sm loaded\n",
            "✓ en_core_web_lg loaded\n",
            "Loading Flair POS tagger (this may take a moment)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/249M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ee37d9b2e704395b1d7ea1955ac8513"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-28 10:16:32,077 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n",
            "✓ Flair model loaded successfully\n",
            "\n",
            "Setup Summary:\n",
            "spaCy sm: ✓ Available\n",
            "spaCy lg: ✓ Available\n",
            "NLTK averaged_perceptron_tagger: ✓ (tokenizer: TreebankWordTokenizer)\n",
            "Flair: ✓ Available\n",
            "\n",
            "Ready to proceed with analysis!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CLEAN SETUP: CLAWS7 → Penn (STRICT), AUDIT + EVAL\n",
        "# ==============================================================================\n",
        "\n",
        "import sys, re, time, warnings, json\n",
        "warnings.filterwarnings('ignore')\n",
        "from collections import defaultdict, Counter\n",
        "from math import sqrt\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "print(\"Imports loaded.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# NLP libraries (optional ones guarded)\n",
        "# ------------------------------------------------------------------------------\n",
        "import spacy\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Flair is optional\n",
        "try:\n",
        "    from flair.data import Sentence\n",
        "    from flair.models import SequenceTagger\n",
        "    FLAIR_AVAILABLE = True\n",
        "except Exception:\n",
        "    FLAIR_AVAILABLE = False\n",
        "\n",
        "print(\"Loading spaCy models...\")\n",
        "try:\n",
        "    nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "except Exception as e:\n",
        "    print(\"spaCy sm load failed:\", e)\n",
        "    nlp_sm = None\n",
        "\n",
        "try:\n",
        "    nlp_lg = spacy.load(\"en_core_web_lg\")\n",
        "except Exception as e:\n",
        "    print(\"spaCy lg load failed:\", e)\n",
        "    nlp_lg = None\n",
        "\n",
        "flair_tagger = None\n",
        "if FLAIR_AVAILABLE:\n",
        "    try:\n",
        "        print(\"Loading Flair POS tagger...\")\n",
        "        flair_tagger = SequenceTagger.load('pos')\n",
        "    except Exception as e:\n",
        "        print(\"Flair load failed:\", e)\n",
        "        FLAIR_AVAILABLE = False\n",
        "        flair_tagger = None\n",
        "\n",
        "print(\"\\nSetup Summary:\")\n",
        "print(f\"  spaCy sm: {'✓' if nlp_sm else '✗'} | spaCy lg: {'✓' if nlp_lg else '✗'}\")\n",
        "print(f\"  NLTK: ✓ (TreebankWordTokenizer + averaged_perceptron_tagger)\")\n",
        "print(f\"  Flair: {'✓' if FLAIR_AVAILABLE and flair_tagger else '✗'}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# CLAWS7 parsing (from 'word_TAG' sequences)\n",
        "# ------------------------------------------------------------------------------\n",
        "def parse_claws_tags(claws_string):\n",
        "    \"\"\"Parse CLAWS7 format: 'word_TAG word_TAG ...' -> (tokens, tags)\"\"\"\n",
        "    if pd.isna(claws_string) or not str(claws_string).strip():\n",
        "        return [], []\n",
        "    tokens, tags = [], []\n",
        "    for item in str(claws_string).strip().split():\n",
        "        if '_' in item:\n",
        "            word, tag = item.rsplit('_', 1)\n",
        "            tokens.append(word)\n",
        "            tags.append(tag)\n",
        "    return tokens, tags\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# CLAWS7 → Penn mapping (closest base mapping) + STRICT UNTRANSLATABLE\n",
        "# ------------------------------------------------------------------------------\n",
        "_CLAWS_TO_PENN_BASE = {\n",
        "    # Determiners / wh-dets\n",
        "    'AT':'DT','DD':'DT','DDQ':'WDT','DDQGE':'WP$','DDQV':'WDT',\n",
        "    'DA':'DT','DA1':'DT','DA2':'DT','DAR':'JJR','DAT':'JJS',\n",
        "    'DB':'DT','DB2':'DT','DD1':'DT','DD2':'DT',\n",
        "\n",
        "    # Coords / subords / clause markers\n",
        "    'CC':'CC','CCB':'CC','CS':'IN','CSA':'IN','CSN':'IN','CST':'IN','CSW':'IN','BCL':'IN',\n",
        "\n",
        "    # Prepositions (base)\n",
        "    'II':'IN','IF':'IN','IO':'IN','IW':'IN',\n",
        "\n",
        "    # Adjectives\n",
        "    'JJ':'JJ','JJR':'JJR','JJT':'JJS','JK':'JJ',\n",
        "\n",
        "    # Adverbs\n",
        "    'RR':'RB','RRQ':'WRB','RRQV':'WRB','RGR':'RBR','RRT':'RBS','RG':'RB','RGQ':'WRB','RGQV':'WRB',\n",
        "    'REX':'RB','RL':'RB','RP':'RP','RPK':'RP','RA':'RB','RT':'RB',\n",
        "\n",
        "    # Nouns (basic)\n",
        "    'NN':'NN','NN1':'NN','NN2':'NNS','NNO':'NN','NNO2':'NNS',\n",
        "    'NP':'NNP','NP1':'NNP','NP2':'NNPS',\n",
        "\n",
        "    # Proper noun semantic subclasses (months/weekdays)\n",
        "    'NPM1':'NNP','NPM2':'NNPS','NPD1':'NNP','NPD2':'NNPS',\n",
        "\n",
        "    # Semantic noun subclasses (mapped loosely; strict will UNK them)\n",
        "    'ND1':'NN','NNL1':'NN','NNL2':'NNS','NNT1':'NN','NNT2':'NNS',\n",
        "    'NNU':'NN','NNU1':'NN','NNU2':'NNS','NNA':'NN','NNB':'NN',\n",
        "\n",
        "    # Numerals\n",
        "    'MC':'CD','MC1':'CD','MC2':'CD','MCGE':'CD','MCMC':'CD','MF':'CD','MD':'JJ',\n",
        "\n",
        "    # Pronouns (closest Penn)\n",
        "    'PPGE':'PRP$','PPH1':'PRP','PPHO1':'PRP','PPHO2':'PRP',\n",
        "    'PPHS1':'PRP','PPHS2':'PRP','PPIO1':'PRP','PPIO2':'PRP',\n",
        "    'PPIS1':'PRP','PPIS2':'PRP','PPX1':'PRP','PPX2':'PRP','PPY':'PRP',\n",
        "    'PN':'PRP','PN1':'PRP','PNQO':'WP','PNQS':'WP','PNQV':'WP',\n",
        "\n",
        "    # Verbs: lexical (good matches)\n",
        "    'VV0':'VB','VVD':'VBD','VVG':'VBG','VVI':'VB','VVN':'VBN','VVZ':'VBZ','VVGK':'VBG','VVNK':'VBN',\n",
        "\n",
        "    # Verbs: do/have/be (Penn loses aux identity; strict will UNK these)\n",
        "    'VD0':'VB','VDD':'VBD','VDG':'VBG','VDI':'VB','VDN':'VBN','VDZ':'VBZ',\n",
        "    'VH0':'VB','VHD':'VBD','VHG':'VBG','VHI':'VB','VHN':'VBN','VHZ':'VBZ',\n",
        "    'VB0':'VB','VBDR':'VBD','VBDZ':'VBD','VBG':'VBG','VBI':'VB','VBM':'VBP','VBN':'VBN','VBR':'VBP','VBZ':'VBZ',\n",
        "\n",
        "    # Modals\n",
        "    'VM':'MD','VMK':'MD',\n",
        "\n",
        "    # Other function words\n",
        "    'TO':'TO','UH':'UH','EX':'EX','GE':'POS','XX':'RB',\n",
        "\n",
        "    # Foreign/formula/unclassified/letters\n",
        "    'FW':'FW','FO':'FW','FU':'FW','ZZ1':'NN','ZZ2':'NNS',\n",
        "\n",
        "    # --- Punctuation / brackets / quotes (reduce UNK→. / , noise) ---\n",
        "    '.':'.', ',':',', ':':':', ';':';', '!':'.', '?':'.',\n",
        "    '(': '-LRB-', ')':'-RRB-',\n",
        "    '[':'-LSB-', ']':'-RSB-',\n",
        "    '{':'-LCB-', '}':'-RCB-',\n",
        "    '``':'``', \"''\":\"''\", '\"':\"''\", \"'\":\"''\",\n",
        "}\n",
        "\n",
        "# Ditto tags like II31 → II\n",
        "_DITTO_RE = re.compile(r'^(.*?)(\\d{2,3})$')\n",
        "def _strip_ditto(tag: str) -> str:\n",
        "    m = _DITTO_RE.match(tag or \"\")\n",
        "    return m.group(1) if m else (tag or \"\")\n",
        "\n",
        "# Tags whose CLAWS distinctions Penn CANNOT encode → mark UNK in STRICT mode\n",
        "# (Explicitly include APPGE to document possessive pronoun pre-nominal collapse.)\n",
        "_STRICT_UNTRANSLATABLE = set([\n",
        "    # Articles / number-specific determiners\n",
        "    'AT','AT1','DD1','DD2','DA','DA1','DA2','DAR','DAT','DB','DB2','DDQGE',\n",
        "    # Preposition subtypes\n",
        "    'IF','IO','IW',\n",
        "    # Semantic noun subclasses (temporal/locative/unit/direction/title/weekday/month)\n",
        "    'ND1','NNL1','NNL2','NNT1','NNT2','NNU','NNU1','NNU2','NNA','NNB',\n",
        "    'NPM1','NPM2','NPD1','NPD2',\n",
        "    # Person/number/case-marked pronouns (Penn POS lacks these features)\n",
        "    'APPGE','PPH1','PPHO1','PPHO2','PPHS1','PPHS2','PPIO1','PPIO2','PPIS1','PPIS2','PPX1','PPX2','PPY','PN1','PN',\n",
        "    # Auxiliary identity / catenatives (Penn POS doesn't encode identity/catenative)\n",
        "    'VBM','VBR','VBZ','VBDR','VBDZ','VBG','VBN','VBI','VB0',\n",
        "    'VD0','VDD','VDG','VDI','VDN','VDZ',\n",
        "    'VH0','VHD','VHG','VHI','VHN','VHZ',\n",
        "    'VVGK','VVNK','VMK','RPK','JK',\n",
        "    # Appositional adv marker / formula / unclassified\n",
        "    'REX','FO','FU',\n",
        "])\n",
        "\n",
        "def convert_claws_to_penn(tag: str, strict: bool = True) -> str:\n",
        "    \"\"\"CLAWS7 → Penn. STRICT=True marks Penn-uncapturable distinctions as UNK.\"\"\"\n",
        "    if not tag or not isinstance(tag, str):\n",
        "        return 'UNK'\n",
        "    base = _strip_ditto(tag)\n",
        "    if strict and base in _STRICT_UNTRANSLATABLE:\n",
        "        return 'UNK'\n",
        "    if base in _CLAWS_TO_PENN_BASE:\n",
        "        return _CLAWS_TO_PENN_BASE[base]\n",
        "\n",
        "    # Generic fallbacks (rare)\n",
        "    if base.startswith('NN'):\n",
        "        return 'UNK' if strict else ('NNS' if tag.endswith('2') else 'NN')\n",
        "    if base.startswith('NP'):\n",
        "        return 'UNK' if strict else ('NNPS' if tag.endswith('2') else 'NNP')\n",
        "    if base.startswith('VV'):\n",
        "        if strict:\n",
        "            return 'UNK'\n",
        "        if base in ('VV0','VVI'): return 'VB'\n",
        "        if base == 'VVD': return 'VBD'\n",
        "        if base == 'VVG': return 'VBG'\n",
        "        if base == 'VVN': return 'VBN'\n",
        "        if base == 'VVZ': return 'VBZ'\n",
        "    if base in {'CS','CSA','CSN','CST','CSW','BCL'}:\n",
        "        return 'IN' if not strict else ('IN' if base == 'CS' else 'UNK')\n",
        "    if base in {'CC','CCB'}:\n",
        "        return 'CC'\n",
        "    return 'UNK'\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Diagnostics for unmappable tags\n",
        "# ------------------------------------------------------------------------------\n",
        "def analyze_unmappable_tags(processed_items, strict: bool = True, top_n: int = 30):\n",
        "    counts = Counter(); examples = {}\n",
        "    total = 0; unk_total = 0\n",
        "    for row in processed_items:\n",
        "        claws_tags = row.get('claws_tags', [])\n",
        "        sent = row.get('sentence', '')\n",
        "        for t in claws_tags:\n",
        "            total += 1\n",
        "            base = _strip_ditto(t)\n",
        "            penn = convert_claws_to_penn(base, strict=strict)\n",
        "            if penn == 'UNK':\n",
        "                unk_total += 1\n",
        "                counts[base] += 1\n",
        "                if base not in examples:\n",
        "                    examples[base] = sent[:120] + ('...' if len(sent) > 120 else '')\n",
        "    print(\"=\"*70)\n",
        "    print(f\"CLAWS7 → Penn STRICT={strict}  | Total tags: {total:,} | UNK: {unk_total:,} ({(unk_total/total*100 if total else 0):.1f}%)\")\n",
        "    print(\"- Top UNTRANSLATABLE CLAWS tags -\")\n",
        "    for tag, c in counts.most_common(top_n):\n",
        "        print(f\"{tag:8s} : {c:6d}   e.g. {examples[tag]}\")\n",
        "    print(\"=\"*70)\n",
        "    return {\n",
        "        'total_tags': total,\n",
        "        'unk_tags': unk_total,\n",
        "        'unk_rate': (unk_total/total if total else 0.0),\n",
        "        'counts': counts,\n",
        "        'examples': examples\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Taggers returning Penn tags directly (for fair comparison against STRICT-projected GT)\n",
        "# ------------------------------------------------------------------------------\n",
        "def tag_with_spacy(sentence, model='sm'):\n",
        "    nlp_model = nlp_sm if model == 'sm' else nlp_lg\n",
        "    if nlp_model is None:\n",
        "        return []\n",
        "    doc = nlp_model(sentence)\n",
        "    return [(t.text, t.tag_) for t in doc]  # spaCy Penn-style tags\n",
        "\n",
        "def tag_with_flair(sentence):\n",
        "    if not (FLAIR_AVAILABLE and flair_tagger):\n",
        "        return []\n",
        "    s = Sentence(sentence); flair_tagger.predict(s)\n",
        "    return [(t.text, t.tag) for t in s]  # Penn-style tags\n",
        "\n",
        "_tb_tok = TreebankWordTokenizer()\n",
        "def tag_with_nltk(sentence):\n",
        "    try:\n",
        "        tokens = _tb_tok.tokenize(sentence)\n",
        "    except Exception:\n",
        "        tokens = sentence.split()\n",
        "    pos_tags = pos_tag(tokens)  # averaged_perceptron_tagger(_eng) ensured in setup cell\n",
        "    return [(w, tag) for w, tag in pos_tags]  # Penn-style tags\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Accuracy + stats (STRICT projection on GT)\n",
        "# ------------------------------------------------------------------------------\n",
        "def calculate_accuracy_fair(ground_truth_claws, predicted_penn, tokens, strict=True):\n",
        "    \"\"\"Compute token-level accuracy after STRICT projecting CLAWS gold to Penn.\"\"\"\n",
        "    if not ground_truth_claws or not predicted_penn or not tokens:\n",
        "        return 0.0\n",
        "    m = min(len(ground_truth_claws), len(predicted_penn), len(tokens))\n",
        "    if m == 0:\n",
        "        return 0.0\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=strict) for t in ground_truth_claws[:m]]\n",
        "    pred = predicted_penn[:m]\n",
        "    return sum(1 for i in range(m) if gt_penn[i] == pred[i]) / m\n",
        "\n",
        "def wilson_confidence_interval(successes, trials, confidence=0.95):\n",
        "    if trials == 0:\n",
        "        return 0, 0, 0\n",
        "    p = successes / trials\n",
        "    z = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
        "    denom = 1 + z**2 / trials\n",
        "    centre = (p + z**2 / (2 * trials)) / denom\n",
        "    half = z * sqrt((p * (1 - p) + z**2 / (4 * trials)) / trials) / denom\n",
        "    return p, max(0, centre - half), min(1, centre + half)\n",
        "\n",
        "def proportion_z_test(x1, n1, x2, n2):\n",
        "    p1, p2 = x1/n1, x2/n2\n",
        "    p_pool = (x1+x2)/(n1+n2)\n",
        "    se = sqrt(p_pool*(1-p_pool)*(1/n1+1/n2))\n",
        "    z = (p1-p2)/se\n",
        "    p_value = 2*(1 - stats.norm.cdf(abs(z)))\n",
        "    return z, p_value\n",
        "\n",
        "def cohens_h(p1, p2):\n",
        "    return 2*(np.arcsin(sqrt(p1)) - np.arcsin(sqrt(p2)))\n",
        "\n",
        "print(\"Setup cell ready: mapping, taggers, and metrics defined.\")\n"
      ],
      "metadata": {
        "id": "KX5yohRk3gbt",
        "outputId": "b8905b1c-8cb9-411f-d4e6-92440f3f0cd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports loaded.\n",
            "Loading spaCy models...\n",
            "Loading Flair POS tagger...\n",
            "2025-08-28 10:24:08,405 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n",
            "\n",
            "Setup Summary:\n",
            "  spaCy sm: ✓ | spaCy lg: ✓\n",
            "  NLTK: ✓ (TreebankWordTokenizer + averaged_perceptron_tagger)\n",
            "  Flair: ✓\n",
            "Setup cell ready: mapping, taggers, and metrics defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Data upload + processing (handles BNC KWIC: Left/Node/Right + CLAWS)\n",
        "# ------------------------------------------------------------------------------\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\nUpload your Dubliners CSV file:\")\n",
        "dubliners_uploaded = files.upload()\n",
        "print(\"\\nUpload your BNC CSV file:\")\n",
        "bnc_uploaded = files.upload()\n",
        "\n",
        "def _read_csv_any_encoding(filename):\n",
        "    df = None\n",
        "    for enc in ('cp1252','latin1','utf-8'):\n",
        "        try:\n",
        "            df = pd.read_csv(filename, encoding=enc, dtype=str)\n",
        "            break\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "    if df is None:\n",
        "        raise ValueError(f\"Failed to read {filename} with tried encodings.\")\n",
        "    return df\n",
        "\n",
        "def _find_col(df, candidates):\n",
        "    \"\"\"Return first matching column (case-insensitive) or None.\"\"\"\n",
        "    lower_map = {c.lower(): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        if cand in lower_map:\n",
        "            return lower_map[cand]\n",
        "    # also allow substring match like 'left context'\n",
        "    for c in df.columns:\n",
        "        lc = c.lower()\n",
        "        if any(cand in lc for cand in candidates):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def load_dataset_with_kwic(filename, dataset_name, preview=3):\n",
        "    df = _read_csv_any_encoding(filename)\n",
        "    print(f\"{dataset_name}: Loaded {len(df)} rows with columns: {list(df.columns)}\")\n",
        "    try:\n",
        "        print(df.head(preview).to_string(index=False))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Detects CLAWS tag string column (any 'claws' match)\n",
        "    claws_col = None\n",
        "    for c in df.columns:\n",
        "        if 'claws' in c.lower():\n",
        "            claws_col = c\n",
        "            break\n",
        "\n",
        "    # Detects KWIC columns\n",
        "    left_col = _find_col(df, ['left'])\n",
        "    node_col = _find_col(df, ['node'])\n",
        "    right_col = _find_col(df, ['right'])\n",
        "\n",
        "    # Detects sentence/context text columns (for non-KWIC datasets)\n",
        "    sentence_col = _find_col(df, ['sentence','context','text','sent','snippet','content','line','string'])\n",
        "\n",
        "    processed = []\n",
        "\n",
        "    if claws_col and left_col and node_col and right_col:\n",
        "        print(f\"✓ Detected KWIC columns: {left_col} | {node_col} | {right_col} and CLAWS column: {claws_col}\")\n",
        "        subset = df[[left_col, node_col, right_col, claws_col]].dropna(subset=[claws_col])\n",
        "        for _, row in subset.iterrows():\n",
        "            # Reconstructs sentence text from KWIC\n",
        "            L = str(row[left_col]).strip() if pd.notna(row[left_col]) else \"\"\n",
        "            N = str(row[node_col]).strip() if pd.notna(row[node_col]) else \"\"\n",
        "            R = str(row[right_col]).strip() if pd.notna(row[right_col]) else \"\"\n",
        "            sent_text = \" \".join([s for s in [L, N, R] if s]).strip()\n",
        "            # Parse CLAWS 'word_TAG' stream\n",
        "            tokens, tags = parse_claws_tags(row[claws_col])\n",
        "            if tokens and tags and len(tokens) == len(tags):\n",
        "                processed.append({\n",
        "                    'sentence': sent_text if sent_text else \" \".join(tokens),\n",
        "                    'tokens': tokens,\n",
        "                    'claws_tags': tags,\n",
        "                    'dataset': dataset_name\n",
        "                })\n",
        "        print(f\"{dataset_name}: Processed {len(processed)} valid KWIC rows.\")\n",
        "        return processed\n",
        "\n",
        "    # Fallback: sentence + CLAWS on the same row (non-KWIC)\n",
        "    if claws_col and sentence_col:\n",
        "        print(f\"✓ Using sentence column: {sentence_col} and CLAWS column: {claws_col}\")\n",
        "        subset = df[[sentence_col, claws_col]].dropna(subset=[claws_col])\n",
        "        for _, row in subset.iterrows():\n",
        "            tokens, tags = parse_claws_tags(row[claws_col])\n",
        "            if tokens and tags and len(tokens) == len(tags):\n",
        "                processed.append({\n",
        "                    'sentence': row[sentence_col],\n",
        "                    'tokens': tokens,\n",
        "                    'claws_tags': tags,\n",
        "                    'dataset': dataset_name\n",
        "                })\n",
        "        print(f\"{dataset_name}: Processed {len(processed)} valid sentences.\")\n",
        "        return processed\n",
        "\n",
        "    # Last fallback: CLAWS only → reconstruct sentence from tokens\n",
        "    if claws_col:\n",
        "        print(f\"⚠ No text column found; reconstructing sentence from CLAWS tokens. Using: {claws_col}\")\n",
        "        subset = df[[claws_col]].dropna(subset=[claws_col])\n",
        "        for _, row in subset.iterrows():\n",
        "            tokens, tags = parse_claws_tags(row[claws_col])\n",
        "            if tokens and tags and len(tokens) == len(tags):\n",
        "                processed.append({\n",
        "                    'sentence': \" \".join(tokens),\n",
        "                    'tokens': tokens,\n",
        "                    'claws_tags': tags,\n",
        "                    'dataset': dataset_name\n",
        "                })\n",
        "        print(f\"{dataset_name}: Processed {len(processed)} reconstructed sentences.\")\n",
        "        return processed\n",
        "\n",
        "    print(f\"✗ Could not locate CLAWS or usable sentence/KWIC columns in {dataset_name}.\")\n",
        "    return []\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Load both datasets now\n",
        "# ------------------------------------------------------------------------------\n",
        "dubliners_filename = list(dubliners_uploaded.keys())[0]\n",
        "bnc_filename       = list(bnc_uploaded.keys())[0]\n",
        "\n",
        "dubliners_data = load_dataset_with_kwic(dubliners_filename, \"Dubliners\")\n",
        "bnc_data       = load_dataset_with_kwic(bnc_filename, \"BNC\")\n",
        "\n",
        "all_processed_data = dubliners_data + bnc_data\n",
        "\n",
        "print(f\"\\nTotal processed sentences: {len(all_processed_data)}\")\n",
        "print(f\"Dubliners: {len(dubliners_data)} | BNC: {len(bnc_data)}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# STRICT unmappable audit\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\nRunning STRICT unmappable audit (CLAWS distinctions Penn can't encode)...\")\n",
        "strict_report = analyze_unmappable_tags(all_processed_data, strict=True, top_n=40)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Batch POS tagging across tools (Penn output) + STRICT accuracy against GT\n",
        "# ------------------------------------------------------------------------------\n",
        "def process_sentence_with_all_tools(sentence):\n",
        "    tools = {\n",
        "        'spacy_sm': (lambda s: tag_with_spacy(s, 'sm')),\n",
        "        'spacy_lg': (lambda s: tag_with_spacy(s, 'lg')),\n",
        "        'nltk':      tag_with_nltk,\n",
        "    }\n",
        "    if FLAIR_AVAILABLE and flair_tagger:\n",
        "        tools['flair'] = tag_with_flair\n",
        "\n",
        "    results = {}\n",
        "    for name, fn in tools.items():\n",
        "        try:\n",
        "            t0 = time.time()\n",
        "            tagged = fn(sentence)  # list of (token, PennTag)\n",
        "            dt = time.time() - t0\n",
        "            results[name] = {\n",
        "                'tags':   [tag for _, tag in tagged],   # Penn\n",
        "                'tokens': [tok for tok, _ in tagged],\n",
        "                'processing_time': dt\n",
        "            }\n",
        "        except Exception as e:\n",
        "            results[name] = {'error': str(e)}\n",
        "    return results\n",
        "\n",
        "print(\"\\nBatch tagging sentences with available tools...\")\n",
        "expanded_batch_results = []\n",
        "for i, data in enumerate(all_processed_data):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"  Progress: {i}/{len(all_processed_data)}\")\n",
        "        sys.stdout.flush()\n",
        "    tool_results = process_sentence_with_all_tools(data['sentence'])\n",
        "    expanded_batch_results.append({\n",
        "        'sentence': data['sentence'],\n",
        "        'ground_truth': data['claws_tags'],  # CLAWS7 tags\n",
        "        'dataset': data['dataset'],\n",
        "        'tool_results': tool_results\n",
        "    })\n",
        "print(\"Batch tagging complete.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Post-batch sanity summary (coverage + alignment health)\n",
        "# ------------------------------------------------------------------------------\n",
        "tool_names = set()\n",
        "for r in expanded_batch_results:\n",
        "    tool_names.update(r['tool_results'].keys())\n",
        "tool_names = sorted(tool_names)\n",
        "\n",
        "coverage = {t: 0 for t in tool_names}\n",
        "mean_len = {t: [] for t in tool_names}\n",
        "zero_min = {t: 0 for t in tool_names}  # count of cases where min alignment length was 0\n",
        "\n",
        "for r in expanded_batch_results:\n",
        "    gt = r['ground_truth']\n",
        "    for t in tool_names:\n",
        "        tri = r['tool_results'].get(t, {})\n",
        "        if tri and 'error' not in tri:\n",
        "            coverage[t] += 1\n",
        "            pred = tri['tags']\n",
        "            toks = tri['tokens']\n",
        "            mean_len[t].append(len(pred))\n",
        "            m = min(len(gt), len(pred), len(toks))\n",
        "            if m == 0:\n",
        "                zero_min[t] += 1\n",
        "\n",
        "print(\"\\nTagger coverage summary:\")\n",
        "for t in tool_names:\n",
        "    n = coverage[t]\n",
        "    ml = (np.mean(mean_len[t]) if mean_len[t] else 0)\n",
        "    zm = zero_min[t]\n",
        "    print(f\"  {t:10s} : sentences={n:4d} | mean_pred_len={ml:5.1f} | zero_min_align={zm}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b6GfGp8xgcWI",
        "outputId": "d62bbdbe-1da0-4c34-f36a-e85a8fe81ce1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Upload your Dubliners CSV file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-272922c4-6c21-4a34-bfd8-db8bf4868fa2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-272922c4-6c21-4a34-bfd8-db8bf4868fa2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving All Similes - Dubliners cont.csv to All Similes - Dubliners cont (1).csv\n",
            "\n",
            "Upload your BNC CSV file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b0bd96a3-c895-453e-a20a-aa50e19dd2ad\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b0bd96a3-c895-453e-a20a-aa50e19dd2ad\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving concordance from BNC.csv to concordance from BNC (1).csv\n",
            "Dubliners: Loaded 184 rows with columns: ['ID', 'Story', 'Page No.', 'Sentence Context', 'Comparator Type ', 'Category (Framwrok)', 'Additional Notes', 'CLAWS']\n",
            "   ID       Story Page No.                                                                                                                                                                                                                                                                                                                                                                   Sentence Context Comparator Type  Category (Framwrok)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Additional Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   CLAWS\n",
            "J-S01 The Sisters        7                                                                                                                                                                                                                                                                                                                      There was no hope for him this time: it was the third stroke.            colon      Joycean_Silent                                                                                                                                                                                                                                                                                   Silent simile: main declarative (“There was no hope…”) followed by a second declarative (“it was the third stroke”), the colon replacing “like/as if” to align them in an implicit comparison. The first clause states the condition, the second supplies the figurative analogue.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                There_EX was_VBDZ no_AT hope_NN1 for_IF him_PPHO1 this_DD1 time_NNT1 :_: it_PPH1 was_VBDZ the_AT third_MD stroke_NN1 ._.\n",
            "J-S02 The Sisters        7 Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in my ears, like the word gnomon in the Euclid and the word simony in the Catechism. But now it sounded to me like the name of some maleficent and sinful being. It filled me with fear, and yet I longed to be nearer to it and to look upon its deadly work.      like + like      Joycean_Framed Although these clauses contain two standard simile structures through the use of \"like,\" they are still framed. The it clause introducing the first simile refers back to the sentence containing the word \"paralysis\" which is the object of comparison. The sentence after the similes echos tone, semantic field, and refers back to the \"It'\" the word \"paralysis.\" Additionally, the second simile is important and extend the frame, as the quasi-simile on page 9, also contructed of the words \"papralysis\" and \"sin\" but with different vehicle and tenor.  Every_AT1 night_NNT1 as_CSA I_PPIS1 gazed_VVD up_RP at_II the_AT window_NN1 I_PPIS1 said_VVD softly_RR to_II myself_PPX1 the_AT word_NN1 paralysis_NN1 ._. It_PPH1 had_VHD always_RR sounded_VVN strangely_RR in_II my_APPGE ears_NN2 ,_, like_II the_AT word_NN1 gnomon_NN1 in_II the_AT Euclid_NP1 and_CC the_AT word_NN1 simony_NN1 in_II the_AT Catechism_NN1 ._. But_CCB now_RT it_PPH1 sounded_VVD to_II me_PPIO1 like_II the_AT name_NN1 of_IO some_DD maleficent_JJ and_CC sinful_JJ being_NN1 ._. \\nIt_PPH1 filled_VVD me_PPIO1 with_IW fear_NN1 ,_, and_CC yet_RR I_PPIS1 longed_VVD to_TO be_VBI nearer_II21 to_II22 it_PPH1 and_CC to_TO look_VVI upon_II its_APPGE deadly_JJ work_NN1 ._. \n",
            "J-S03 The Sisters        7                                                                                                                                                                                                                                                                                  While my aunt was ladling out my stirabout he said, as if returning to some former remark of his:            as if       Joycean_Quasi                                                                                                                                                                                                                                                                                      Leech and Short, using Conrad as an example, explain that quasi similes are \"constructions which express or imply similitude,\" which can be achieved by words such as \"resembling,\" \"as if,\" \"looked,\" or \"suggesting.\" In Joyce, there are quite a few \"as if\" quasi similes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 While_CS my_APPGE aunt_NN1 was_VBDZ ladling_VVG out_RP my_APPGE stirabout_NN1 he_PPHS1 said_VVD ,_, as_CS21 if_CS22 returning_VVG to_II some_DD former_DA remark_NN1 of_IO his_PPGE :_: \n",
            "✓ Using sentence column: Sentence Context and CLAWS column: CLAWS\n",
            "Dubliners: Processed 183 valid sentences.\n",
            "BNC: Loaded 200 rows with columns: ['Index', 'Left', 'Node', 'Right', 'Genre', 'Comparator Type', 'Category (Framework)', 'CLAWS']\n",
            "  Index                 Left Node                                                                          Right   Genre Comparator Type Category (Framework)                                                                                                                                                                       CLAWS\n",
            "BNClab1 It seemed very much  like                                            she'd given up even trying to live. fiction            like         Quasi_Simile                                                         It_PPH1 seemed_VVD very_RG much_DA1 like_CS she_PPHS1 'd_VHD given_VVN up_RP even_RR trying_VVG to_TO live_VVI ._. \n",
            "BNClab2            Memories  like this seem to pour out of her and she finds herself crying for those lost days. fiction            like             Standard Memories_NN2 like_II this_DD1 seem_VV0 to_TO pour_VVI out_II21 of_II22 her_PPHO1 and_CC she_PPHS1 finds_VVZ herself_PPX1 crying_VVG for_IF those_DD2 lost_JJ days_NNT2 ._. \n",
            "BNClab3            You sound like                                                                            me. fiction            like             Standard                                                                                                                                     You_PPY sound_VV0 like_II me_PPIO1 ._. \n",
            "✓ Detected KWIC columns: Left | Node | Right and CLAWS column: CLAWS\n",
            "BNC: Processed 200 valid KWIC rows.\n",
            "\n",
            "Total processed sentences: 383\n",
            "Dubliners: 183 | BNC: 200\n",
            "\n",
            "Running STRICT unmappable audit (CLAWS distinctions Penn can't encode)...\n",
            "======================================================================\n",
            "CLAWS7 → Penn STRICT=True  | Total tags: 10,006 | UNK: 3,148 (31.5%)\n",
            "- Top UNTRANSLATABLE CLAWS tags -\n",
            "AT       :    472   e.g. There was no hope for him this time: it was the third stroke.\n",
            "AT1      :    301   e.g. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in m...\n",
            "PPHS1    :    297   e.g. While my aunt was ladling out my stirabout he said, as if returning to some former remark of his:\n",
            "APPGE    :    290   e.g. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in m...\n",
            "IO       :    213   e.g. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in m...\n",
            "VBDZ     :    147   e.g. There was no hope for him this time: it was the third stroke.\n",
            "PPH1     :    121   e.g. There was no hope for him this time: it was the third stroke.\n",
            "VHD      :    107   e.g. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in m...\n",
            "PPIS1    :    105   e.g. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in m...\n",
            "PPHO1    :    102   e.g. There was no hope for him this time: it was the third stroke.\n",
            "DD1      :     82   e.g. There was no hope for him this time: it was the third stroke.\n",
            "IW       :     60   e.g. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in m...\n",
            "PN1      :     59   e.g. I found it strange that neither I nor the day seemed in a mourning mood and I felt even annoyed at discovering in myself...\n",
            "NNT1     :     45   e.g. There was no hope for him this time: it was the third stroke.\n",
            "IF       :     44   e.g. There was no hope for him this time: it was the third stroke.\n",
            "PPY      :     42   e.g. When children see things like that, you know, it has an effect….\n",
            "VBDR     :     42   e.g. I felt that I had been very far away, in some land where the customs were strange - in Persia, I thought….But I could no...\n",
            "VBZ      :     42   e.g. He knew then? He was quite resigned. He looks quite resigned, said my aunt. That's what the woman we had in to wash him ...\n",
            "PPHS2    :     42   e.g. But when the restraining influence of the school was at a distance I began to hunger again for wild sensations, for the ...\n",
            "PPIO1    :     38   e.g. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in m...\n",
            "VBI      :     35   e.g. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in m...\n",
            "PPHO2    :     33   e.g. The duties of the priest towards the Eucharist and towards the secrecy of the confessional seemed so grace to me that I ...\n",
            "NNB      :     31   e.g. Mahony used slang freely, and spoke of Father Butler as Bunsen Burner. \n",
            "VBN      :     28   e.g. I found it strange that neither I nor the day seemed in a mourning mood and I felt even annoyed at discovering in myself...\n",
            "DB       :     28   e.g. The duties of the priest towards the Eucharist and towards the secrecy of the confessional seemed so grace to me that I ...\n",
            "PPX1     :     25   e.g. Every night as I gazed up at the window I said softly to myself the word paralysis. It had always sounded strangely in m...\n",
            "DA       :     23   e.g. While my aunt was ladling out my stirabout he said, as if returning to some former remark of his:\n",
            "DD2      :     23   e.g. The duties of the priest towards the Eucharist and towards the secrecy of the confessional seemed so grace to me that I ...\n",
            "VHI      :     20   e.g. I wouldn't want children of mine, he said, to have too much to say to a man like that. \n",
            "NNT2     :     19   e.g. He gave me the impression that he was repeating something which he had learned by heart or that, magnetised by some word...\n",
            "VDD      :     19   e.g. He gave me the impression that he was repeating something which he had learned by heart or that, magnetised by some word...\n",
            "PPIS2    :     18   e.g. He knew then? He was quite resigned. He looks quite resigned, said my aunt. That's what the woman we had in to wash him ...\n",
            "VD0      :     16   e.g. And what do you think but there he was, sitting up by himself in the dark of his confession-box, wide-awake and laughing...\n",
            "VBR      :     14   e.g. God knows we done all we could, as poor as we are - we wouldn't see him want anything while he was in it. \n",
            "DA1      :     13   e.g. I wouldn't want children of mine, he said, to have too much to say to a man like that. \n",
            "VHZ      :     13   e.g. When children see things like that, you know, it has an effect….\n",
            "-        :     11   e.g. I felt that I had been very far away, in some land where the customs were strange - in Persia, I thought….But I could no...\n",
            "PN       :      9   e.g. He knew then? He was quite resigned. He looks quite resigned, said my aunt. That's what the woman we had in to wash him ...\n",
            "PPIO2    :      9   e.g. He gave me the impression that he was repeating something which he had learned by heart or that, magnetised by some word...\n",
            "VBM      :      8   e.g. I'm surprised at boys like you, educated, reading such stuff. \n",
            "======================================================================\n",
            "\n",
            "Batch tagging sentences with available tools...\n",
            "  Progress: 0/383\n",
            "  Progress: 10/383\n",
            "  Progress: 20/383\n",
            "  Progress: 30/383\n",
            "  Progress: 40/383\n",
            "  Progress: 50/383\n",
            "  Progress: 60/383\n",
            "  Progress: 70/383\n",
            "  Progress: 80/383\n",
            "  Progress: 90/383\n",
            "  Progress: 100/383\n",
            "  Progress: 110/383\n",
            "  Progress: 120/383\n",
            "  Progress: 130/383\n",
            "  Progress: 140/383\n",
            "  Progress: 150/383\n",
            "  Progress: 160/383\n",
            "  Progress: 170/383\n",
            "  Progress: 180/383\n",
            "  Progress: 190/383\n",
            "  Progress: 200/383\n",
            "  Progress: 210/383\n",
            "  Progress: 220/383\n",
            "  Progress: 230/383\n",
            "  Progress: 240/383\n",
            "  Progress: 250/383\n",
            "  Progress: 260/383\n",
            "  Progress: 270/383\n",
            "  Progress: 280/383\n",
            "  Progress: 290/383\n",
            "  Progress: 300/383\n",
            "  Progress: 310/383\n",
            "  Progress: 320/383\n",
            "  Progress: 330/383\n",
            "  Progress: 340/383\n",
            "  Progress: 350/383\n",
            "  Progress: 360/383\n",
            "  Progress: 370/383\n",
            "  Progress: 380/383\n",
            "Batch tagging complete.\n",
            "\n",
            "Tagger coverage summary:\n",
            "  flair      : sentences= 383 | mean_pred_len= 26.3 | zero_min_align=0\n",
            "  nltk       : sentences= 383 | mean_pred_len= 26.0 | zero_min_align=0\n",
            "  spacy_lg   : sentences= 383 | mean_pred_len= 26.6 | zero_min_align=0\n",
            "  spacy_sm   : sentences= 383 | mean_pred_len= 26.6 | zero_min_align=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Tool coverage + alignment audit (with optional include filter and per-dataset view)\n",
        "# ==============================================================================\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def audit_tool_coverage(results, include_tools=None):\n",
        "    tools_seen = set()\n",
        "    cov_sentences = Counter()      # overall sentences with non-error result\n",
        "    had_error = Counter()          # overall sentences with 'error'\n",
        "    zero_min = Counter()           # sentences where min alignment length == 0\n",
        "    pred_len_sum = Counter()\n",
        "    pred_len_cnt = Counter()\n",
        "    sample_errors = defaultdict(list)\n",
        "\n",
        "    # Per-dataset breakdown\n",
        "    cov_by_ds = defaultdict(Counter)   # dataset -> tool -> count\n",
        "    err_by_ds = defaultdict(Counter)   # dataset -> tool -> count\n",
        "\n",
        "    for res in results:\n",
        "        gt = res.get('ground_truth', [])\n",
        "        ds = res.get('dataset', 'UNKNOWN')\n",
        "        tri_map = res.get('tool_results', {})\n",
        "\n",
        "        tools_seen.update(tri_map.keys())\n",
        "        for tool, tri in tri_map.items():\n",
        "            if include_tools and tool not in include_tools:\n",
        "                continue\n",
        "            if not isinstance(tri, dict):\n",
        "                continue\n",
        "\n",
        "            if 'error' in tri:\n",
        "                had_error[tool] += 1\n",
        "                err_by_ds[ds][tool] += 1\n",
        "                if len(sample_errors[tool]) < 3:\n",
        "                    sample_errors[tool].append(tri.get('error'))\n",
        "                continue\n",
        "\n",
        "            preds = tri.get('tags') or []\n",
        "            toks  = tri.get('tokens') or []\n",
        "            if preds:\n",
        "                cov_sentences[tool] += 1\n",
        "                cov_by_ds[ds][tool] += 1\n",
        "                pred_len_sum[tool] += len(preds)\n",
        "                pred_len_cnt[tool] += 1\n",
        "\n",
        "            m = min(len(gt), len(preds), len(toks))\n",
        "            if m == 0:\n",
        "                zero_min[tool] += 1\n",
        "\n",
        "    print(\"=== Tool coverage & alignment audit ===\")\n",
        "    shown_tools = sorted(t for t in tools_seen if (not include_tools or t in include_tools))\n",
        "    for tool in shown_tools:\n",
        "        n_sent = cov_sentences[tool]\n",
        "        n_err  = had_error[tool]\n",
        "        zm     = zero_min[tool]\n",
        "        avg_len = (pred_len_sum[tool]/pred_len_cnt[tool]) if pred_len_cnt[tool] else 0.0\n",
        "        print(f\"{tool:10s} | sentences_ok={n_sent:4d} | errors={n_err:3d} | zero_min={zm:3d} | mean_pred_len={avg_len:5.1f}\")\n",
        "        if sample_errors[tool]:\n",
        "            for e in sample_errors[tool]:\n",
        "                print(f\"   ↪ error sample: {e}\")\n",
        "\n",
        "    # Optional per-dataset breakdown\n",
        "    print(\"\\n--- Per-dataset coverage ---\")\n",
        "    for ds in sorted(cov_by_ds.keys() | err_by_ds.keys()):\n",
        "        print(f\"[{ds}]\")\n",
        "        for tool in shown_tools:\n",
        "            ok = cov_by_ds[ds][tool]\n",
        "            er = err_by_ds[ds][tool]\n",
        "            print(f\"  {tool:10s} ok={ok:4d} | errors={er:3d}\")\n",
        "    return {\n",
        "        \"tools_seen\": shown_tools,\n",
        "        \"sentences_ok\": dict(cov_sentences),\n",
        "        \"errors\": dict(had_error),\n",
        "        \"zero_min\": dict(zero_min),\n",
        "        \"per_dataset_ok\": {ds: dict(cov_by_ds[ds]) for ds in cov_by_ds},\n",
        "        \"per_dataset_err\": {ds: dict(err_by_ds[ds]) for ds in err_by_ds},\n",
        "    }\n",
        "\n",
        "# Example: exclude textblob explicitly\n",
        "_coverage = audit_tool_coverage(expanded_batch_results, include_tools={'spacy_sm','spacy_lg','nltk','flair'})\n"
      ],
      "metadata": {
        "id": "nks3ItJc8d56",
        "outputId": "cccf8c1b-e3a1-44f6-e6a4-8fec039b28d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Tool coverage & alignment audit ===\n",
            "flair      | sentences_ok= 383 | errors=  0 | zero_min=  0 | mean_pred_len= 26.3\n",
            "nltk       | sentences_ok= 383 | errors=  0 | zero_min=  0 | mean_pred_len= 26.0\n",
            "spacy_lg   | sentences_ok= 383 | errors=  0 | zero_min=  0 | mean_pred_len= 26.6\n",
            "spacy_sm   | sentences_ok= 383 | errors=  0 | zero_min=  0 | mean_pred_len= 26.6\n",
            "\n",
            "--- Per-dataset coverage ---\n",
            "[BNC]\n",
            "  flair      ok= 200 | errors=  0\n",
            "  nltk       ok= 200 | errors=  0\n",
            "  spacy_lg   ok= 200 | errors=  0\n",
            "  spacy_sm   ok= 200 | errors=  0\n",
            "[Dubliners]\n",
            "  flair      ok= 183 | errors=  0\n",
            "  nltk       ok= 183 | errors=  0\n",
            "  spacy_lg   ok= 183 | errors=  0\n",
            "  spacy_sm   ok= 183 | errors=  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Minimal STRICT error analysis (TextBlob excluded)\n",
        "# ==============================================================================\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def run_min_error_analysis_all_tools(results, top_n=5):\n",
        "    # discover all tool names first\n",
        "    tools_seen = set()\n",
        "    for r in results:\n",
        "        tools_seen.update(r.get('tool_results', {}).keys())\n",
        "    # exclude TextBlob\n",
        "    tools_seen.discard('textblob')\n",
        "\n",
        "    error_counts = {t: Counter() for t in tools_seen}  # include empty counters\n",
        "    confusions_by_gold = {t: defaultdict(Counter) for t in tools_seen}\n",
        "    totals = Counter()\n",
        "    errors = Counter()\n",
        "\n",
        "    for res in results:\n",
        "        gt_claws = res.get('ground_truth', [])\n",
        "        gt_penn_full = [convert_claws_to_penn(t, strict=True) for t in gt_claws]\n",
        "\n",
        "        for tool in tools_seen:\n",
        "            tri = res.get('tool_results', {}).get(tool, {})\n",
        "            if not isinstance(tri, dict) or ('error' in tri):\n",
        "                continue\n",
        "            pred = tri.get('tags') or []\n",
        "            toks = tri.get('tokens') or []\n",
        "            m = min(len(gt_penn_full), len(pred), len(toks))\n",
        "            if m <= 0:\n",
        "                continue\n",
        "            gt_penn = gt_penn_full[:m]\n",
        "            for i in range(m):\n",
        "                g, p = gt_penn[i], pred[i]\n",
        "                totals[tool] += 1\n",
        "                if g != p:\n",
        "                    errors[tool] += 1\n",
        "                    error_counts[tool][f\"{g}->{p}\"] += 1\n",
        "                    confusions_by_gold[tool][g][p] += 1\n",
        "\n",
        "    # Report\n",
        "    print(\"Most common error patterns (STRICT):\")\n",
        "    print(\"=\"*60)\n",
        "    for tool in sorted(tools_seen):\n",
        "        err = errors.get(tool, 0)\n",
        "        tot = totals.get(tool, 0)\n",
        "        err_rate = (err / tot) if tot else 0.0\n",
        "        print(f\"\\n{tool} (errors={err:,} / {tot:,} | error rate={err_rate:.3f}):\")\n",
        "        for pat, c in error_counts[tool].most_common(top_n):\n",
        "            print(f\"  {pat:12s} : {c}\")\n",
        "        if tot == 0:\n",
        "            print(\"  (no aligned tokens; check coverage/zero_min above)\")\n",
        "\n",
        "    print(\"\\nTop confusions by GOLD (STRICT):\")\n",
        "    print(\"=\"*60)\n",
        "    default_order = ['UNK','NN','NNS','JJ','RB','IN','VB','VBD','VBG','VBN','VBZ','PRP','DT', ',', '.']\n",
        "    for tool in sorted(tools_seen):\n",
        "        cg = confusions_by_gold[tool]\n",
        "        extras = [g for g in cg.keys() if g not in default_order]\n",
        "        key_gold = default_order + sorted(extras)\n",
        "        print(f\"\\n{tool}:\")\n",
        "        any_line = False\n",
        "        for g in key_gold:\n",
        "            if g in cg and cg[g]:\n",
        "                top = \", \".join([f\"{p}×{c}\" for p, c in cg[g].most_common(3)])\n",
        "                print(f\"  {g:>4s} → {top}\")\n",
        "                any_line = True\n",
        "        if not any_line:\n",
        "            print(\"  (no confusions recorded)\")\n",
        "\n",
        "    return {\n",
        "        \"tools_seen\": sorted(tools_seen),\n",
        "        \"error_counts\": error_counts,\n",
        "        \"confusions_by_gold\": confusions_by_gold,\n",
        "        \"totals\": dict(totals),\n",
        "        \"errors\": dict(errors)\n",
        "    }\n",
        "\n",
        "_min_err = run_min_error_analysis_all_tools(expanded_batch_results, top_n=5)\n"
      ],
      "metadata": {
        "id": "TNCR8CAh-Jfg",
        "outputId": "7894857c-0264-4f90-f701-e650b58c9589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common error patterns (STRICT):\n",
            "============================================================\n",
            "\n",
            "flair (errors=4,407 / 9,994 | error rate=0.441):\n",
            "  UNK->DT      : 820\n",
            "  UNK->PRP     : 777\n",
            "  UNK->IN      : 355\n",
            "  UNK->VBD     : 306\n",
            "  UNK->PRP$    : 271\n",
            "\n",
            "nltk (errors=4,667 / 9,941 | error rate=0.469):\n",
            "  UNK->DT      : 818\n",
            "  UNK->PRP     : 698\n",
            "  UNK->IN      : 331\n",
            "  UNK->VBD     : 310\n",
            "  UNK->PRP$    : 288\n",
            "\n",
            "spacy_lg (errors=4,695 / 9,994 | error rate=0.470):\n",
            "  UNK->DT      : 785\n",
            "  UNK->PRP     : 754\n",
            "  UNK->IN      : 339\n",
            "  UNK->VBD     : 304\n",
            "  UNK->PRP$    : 264\n",
            "\n",
            "spacy_sm (errors=4,723 / 9,994 | error rate=0.473):\n",
            "  UNK->DT      : 783\n",
            "  UNK->PRP     : 754\n",
            "  UNK->IN      : 342\n",
            "  UNK->VBD     : 308\n",
            "  UNK->PRP$    : 265\n",
            "\n",
            "Top confusions by GOLD (STRICT):\n",
            "============================================================\n",
            "\n",
            "flair:\n",
            "   UNK → DT×820, PRP×777, IN×355\n",
            "    NN → NNP×38, JJ×38, DT×31\n",
            "   NNS → NN×12, JJ×5, IN×5\n",
            "    JJ → RB×22, VBG×20, NN×18\n",
            "    RB → JJ×16, IN×12, PRP×11\n",
            "    IN → RB×24, WRB×23, WDT×22\n",
            "    VB → VBP×47, MD×15, NN×11\n",
            "   VBD → PRP×11, VBN×6, NN×5\n",
            "   VBG → NN×5, ,×3, JJ×2\n",
            "   VBN → VBD×29, JJ×10, VB×4\n",
            "   VBZ → NNS×3, DT×1, PRP×1\n",
            "    DT → IN×2, RB×1, NN×1\n",
            "     , → NN×15, RB×6, PRP×5\n",
            "     . → ,×25, NN×15, RB×7\n",
            "     : → NN×2, NFP×2, VBG×1\n",
            "     ; → :×15, ,×1, NN×1\n",
            "    CC → NN×7, NNS×5, IN×3\n",
            "    CD → PRP×2, IN×2, DT×1\n",
            "    EX → ,×1, PRP×1, NN×1\n",
            "   JJR → RBR×1\n",
            "    MD → PRP×7, NN×3, CC×1\n",
            "   NNP → IN×4, VBD×4, ,×3\n",
            "  NNPS → NNP×3\n",
            "   POS → ''×3, NN×2, NNP×1\n",
            "  PRP$ → NN×2\n",
            "   RBR → JJR×1\n",
            "   RBS → RB×1\n",
            "    RP → RB×22, IN×5, VB×3\n",
            "    TO → VBD×4, VBN×3, PRP×2\n",
            "    UH → DT×1, :×1, WRB×1\n",
            "   WDT → WP×15, RB×4, JJ×1\n",
            "    WP → ,×1\n",
            "   WRB → CC×2, RB×1, ,×1\n",
            "\n",
            "nltk:\n",
            "   UNK → DT×818, PRP×698, IN×331\n",
            "    NN → NNP×36, IN×33, JJ×33\n",
            "   NNS → NN×19, IN×8, VBZ×8\n",
            "    JJ → NN×61, VBG×20, RB×18\n",
            "    RB → IN×27, JJ×17, DT×15\n",
            "    IN → TO×75, WRB×25, RB×22\n",
            "    VB → VBP×44, NN×28, PRP×15\n",
            "   VBD → IN×11, VBN×8, PRP×6\n",
            "   VBG → NN×8, IN×3, TO×2\n",
            "   VBN → VBD×30, JJ×5, NN×4\n",
            "   VBZ → NNS×7, RB×1, NNP×1\n",
            "    DT → JJ×2, CC×1, NN×1\n",
            "     , → IN×11, VBD×6, PRP×5\n",
            "     . → PRP×15, DT×8, NN×6\n",
            "     : → MD×1, JJ×1, IN×1\n",
            "     ; → :×16, PRP$×1, VBD×1\n",
            "    CC → PRP×7, IN×6, NN×4\n",
            "    CD → JJ×5, PRP×2, NN×2\n",
            "    EX → VBD×1, NNS×1\n",
            "   JJR → NN×1, VBP×1\n",
            "    MD → VB×8, IN×3, DT×3\n",
            "   NNP → PRP×3, NN×2, IN×2\n",
            "  NNPS → NNP×4\n",
            "   POS → NNS×1\n",
            "  PRP$ → NN×1, ,×1\n",
            "   RBR → JJ×1, JJR×1\n",
            "   RBS → NN×1\n",
            "    RP → IN×17, RB×13, NN×5\n",
            "    TO → VB×6, VBN×3, PRP×2\n",
            "    UH → NNP×8, DT×5, VBD×1\n",
            "   WDT → WP×13, NN×4, DT×3\n",
            "    WP → IN×1\n",
            "   WRB → NN×2, RB×2, PRP×1\n",
            "\n",
            "spacy_lg:\n",
            "   UNK → DT×785, PRP×754, IN×339\n",
            "    NN → NNP×47, IN×40, DT×31\n",
            "   NNS → IN×12, NN×10, JJ×7\n",
            "    JJ → NN×25, RB×25, IN×23\n",
            "    RB → JJ×15, NN×13, IN×11\n",
            "    IN → RB×30, WRB×23, NN×23\n",
            "    VB → VBP×44, MD×15, NN×15\n",
            "   VBD → PRP×11, VBN×6, DT×5\n",
            "   VBG → NN×9, PRP$×3, ,×2\n",
            "   VBN → VBD×29, PRP×7, JJ×6\n",
            "   VBZ → NNS×3, DT×1, IN×1\n",
            "    DT → IN×1, RB×1, NN×1\n",
            "     , → NN×13, IN×9, PRP×7\n",
            "     . → NN×18, DT×11, IN×8\n",
            "     : → NN×3, JJ×2, IN×2\n",
            "     ; → :×14, DT×1, NN×1\n",
            "    CC → NN×9, NNS×6, IN×6\n",
            "    CD → IN×3, PRP×2, DT×2\n",
            "    EX → ,×2, NNS×1\n",
            "   JJR → CC×1, IN×1, RBR×1\n",
            "    MD → PRP×7, NN×3, DT×3\n",
            "   NNP → VBD×5, IN×4, ,×4\n",
            "  NNPS → NNP×4\n",
            "   POS → NNS×1, NN×1\n",
            "  PRP$ → NN×2\n",
            "   RBR → IN×1, JJR×1\n",
            "   RBS → JJS×1\n",
            "    RP → RB×16, VB×5, IN×5\n",
            "    TO → VBN×4, VBD×2, PRP×2\n",
            "    UH → DT×2, IN×1, :×1\n",
            "   WDT → WP×14, RB×5, NNS×1\n",
            "    WP → NN×1, ,×1\n",
            "   WRB → CC×2, NN×2, RB×1\n",
            "\n",
            "spacy_sm:\n",
            "   UNK → DT×783, PRP×754, IN×342\n",
            "    NN → NNP×47, IN×41, DT×31\n",
            "   NNS → IN×12, NN×11, DT×7\n",
            "    JJ → NN×31, RB×28, IN×21\n",
            "    RB → JJ×15, NN×13, IN×13\n",
            "    IN → RB×24, NN×24, WRB×23\n",
            "    VB → VBP×43, NN×16, MD×15\n",
            "   VBD → PRP×11, VBN×8, DT×5\n",
            "   VBG → NN×9, PRP$×3, ,×2\n",
            "   VBN → VBD×28, PRP×7, VB×5\n",
            "   VBZ → NNS×3, DT×1, IN×1\n",
            "    DT → RB×2, IN×1, NN×1\n",
            "     , → NN×12, IN×9, NNP×7\n",
            "     . → NN×19, DT×11, IN×8\n",
            "     : → NN×3, VBN×3, RB×2\n",
            "     ; → :×14, DT×1, NN×1\n",
            "    CC → NN×9, NNS×6, IN×6\n",
            "    CD → IN×3, PRP×2, DT×2\n",
            "    EX → ,×2, NNS×1\n",
            "   JJR → CC×1, IN×1\n",
            "   JJS → JJ×1\n",
            "    MD → PRP×7, NN×3, DT×3\n",
            "   NNP → NN×7, IN×5, VBD×4\n",
            "  NNPS → NNP×3\n",
            "   POS → NNS×1, NN×1\n",
            "  PRP$ → NN×2\n",
            "   RBR → IN×1, JJR×1\n",
            "   RBS → JJS×1\n",
            "    RP → RB×16, IN×5, VB×5\n",
            "    TO → VBN×4, VBD×2, PRP×2\n",
            "    UH → DT×1, IN×1, :×1\n",
            "   WDT → WP×15, RB×5, NN×2\n",
            "    WP → NN×1, ,×1\n",
            "   WRB → CC×2, NN×2, RB×1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Drilldown: Which CLAWS tags become UNK (STRICT) and what do tools predict?\n",
        "# ==============================================================================\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def unk_source_analysis(results, tool_names=None, top_n=12, strip_ditto=True, return_data=True):\n",
        "    \"\"\"\n",
        "    For tokens where STRICT-projected gold == 'UNK', count:\n",
        "      - aggregate: CLAWS source -> predicted Penn\n",
        "      - per tool: tool -> CLAWS source -> predicted Penn\n",
        "    Parameters\n",
        "      results      : expanded_batch_results\n",
        "      tool_names   : iterable of tool names to include (None = all discovered)\n",
        "      top_n        : top CLAWS sources to print\n",
        "      strip_ditto  : collapse ditto tags (e.g., II31 -> II)\n",
        "      return_data  : return dicts with flat rows for export\n",
        "    \"\"\"\n",
        "    per_tool = defaultdict(lambda: defaultdict(Counter))   # tool -> CLAWS -> Counter(pred)\n",
        "    all_tools = defaultdict(Counter)                       # CLAWS -> Counter(pred)\n",
        "    totals_by_source = Counter()                           # CLAWS -> total UNK obs (all tools)\n",
        "    totals_by_tool = Counter()                             # tool -> total UNK obs\n",
        "    discovered_tools = set()\n",
        "\n",
        "    for res in results:\n",
        "        gt_claws_full = res.get('ground_truth', [])\n",
        "        gt_penn_full = [convert_claws_to_penn(t, strict=True) for t in gt_claws_full]\n",
        "        tri_map = res.get('tool_results', {})\n",
        "        discovered_tools.update(tri_map.keys())\n",
        "\n",
        "        for tool, tri in tri_map.items():\n",
        "            if tool_names and tool not in tool_names:\n",
        "                continue\n",
        "            if not isinstance(tri, dict) or ('error' in tri):\n",
        "                continue\n",
        "            pred = tri.get('tags') or []\n",
        "            toks = tri.get('tokens') or []\n",
        "            m = min(len(gt_claws_full), len(gt_penn_full), len(pred), len(toks))\n",
        "            if m <= 0:\n",
        "                continue\n",
        "\n",
        "            for i in range(m):\n",
        "                if gt_penn_full[i] == 'UNK':\n",
        "                    claws_src = gt_claws_full[i]\n",
        "                    if strip_ditto:\n",
        "                        claws_src = _strip_ditto(claws_src)\n",
        "                    per_tool[tool][claws_src][pred[i]] += 1\n",
        "                    all_tools[claws_src][pred[i]] += 1\n",
        "                    totals_by_source[claws_src] += 1\n",
        "                    totals_by_tool[tool] += 1\n",
        "\n",
        "    # Resolve default tool list\n",
        "    if tool_names is None:\n",
        "        tool_names = sorted(discovered_tools)\n",
        "\n",
        "    # ---- Aggregate printout ---------------------------------------------------\n",
        "    print(\"Top CLAWS sources of UNK (all tools combined):\")\n",
        "    print(\"=\"*70)\n",
        "    total_unk = sum(totals_by_source.values())\n",
        "    for claws_src, cnts in sorted(all_tools.items(),\n",
        "                                  key=lambda x: sum(x[1].values()),\n",
        "                                  reverse=True)[:top_n]:\n",
        "        src_total = sum(cnts.values())\n",
        "        share = (src_total / total_unk) if total_unk else 0.0\n",
        "        top_preds = \", \".join(f\"{p}×{c}\" for p, c in cnts.most_common(3))\n",
        "        print(f\"{claws_src:8s}  total={src_total:4d}  ({share:5.1%})  →  {top_preds}\")\n",
        "\n",
        "    # ---- Per-tool printout ----------------------------------------------------\n",
        "    for tool in tool_names:\n",
        "        print(f\"\\n{tool}: Top CLAWS sources of UNK\")\n",
        "        print(\"-\"*70)\n",
        "        tool_map = per_tool.get(tool, {})\n",
        "        tool_tot = sum(sum(c.values()) for c in tool_map.values())\n",
        "        if tool_tot == 0:\n",
        "            print(\"  (no UNK observations for this tool)\")\n",
        "            continue\n",
        "        items = sorted(tool_map.items(),\n",
        "                       key=lambda x: sum(x[1].values()),\n",
        "                       reverse=True)[:top_n]\n",
        "        for claws_src, cnts in items:\n",
        "            src_total = sum(cnts.values())\n",
        "            share = src_total / tool_tot if tool_tot else 0.0\n",
        "            top_preds = \", \".join(f\"{p}×{c}\" for p, c in cnts.most_common(3))\n",
        "            print(f\"{claws_src:8s}  total={src_total:4d}  ({share:5.1%})  →  {top_preds}\")\n",
        "\n",
        "    if not return_data:\n",
        "        return None\n",
        "\n",
        "    # ---- Build flat rows for downstream export --------------------------------\n",
        "    agg_rows = []\n",
        "    for claws_src, cnts in all_tools.items():\n",
        "        src_total = sum(cnts.values())\n",
        "        for pred, c in cnts.items():\n",
        "            agg_rows.append({\n",
        "                'CLAWS_source': claws_src,\n",
        "                'pred_penn': pred,\n",
        "                'count': int(c),\n",
        "                'source_total': int(src_total),\n",
        "                'source_share_all': (src_total / total_unk) if total_unk else 0.0\n",
        "            })\n",
        "\n",
        "    per_tool_rows = []\n",
        "    for tool, m in per_tool.items():\n",
        "        tool_tot = sum(sum(c.values()) for c in m.values())\n",
        "        for claws_src, cnts in m.items():\n",
        "            src_total = sum(cnts.values())\n",
        "            for pred, c in cnts.items():\n",
        "                per_tool_rows.append({\n",
        "                    'tool': tool,\n",
        "                    'CLAWS_source': claws_src,\n",
        "                    'pred_penn': pred,\n",
        "                    'count': int(c),\n",
        "                    'source_total_tool': int(src_total),\n",
        "                    'source_share_tool': (src_total / tool_tot) if tool_tot else 0.0\n",
        "                })\n",
        "\n",
        "    return {\n",
        "        'aggregate': agg_rows,\n",
        "        'per_tool': per_tool_rows,\n",
        "        'totals_by_source': dict(totals_by_source),\n",
        "        'totals_by_tool': dict(totals_by_tool),\n",
        "        'tools_seen': tool_names,\n",
        "        'total_unk': int(total_unk)\n",
        "    }\n",
        "\n",
        "# Run — include nltk explicitly (or leave tool_names=None to include all seen tools)\n",
        "_unk = unk_source_analysis(expanded_batch_results,\n",
        "                           tool_names=['spacy_sm','spacy_lg','flair','nltk'],\n",
        "                           top_n=12,\n",
        "                           strip_ditto=True,\n",
        "                           return_data=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMzRRByNnd7b",
        "outputId": "3b075937-c781-45c8-ef2a-76bf0884e1de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top CLAWS sources of UNK (all tools combined):\n",
            "======================================================================\n",
            "AT        total=1888  (15.0%)  →  DT×1658, IN×60, NN×52\n",
            "AT1       total=1201  ( 9.5%)  →  DT×1066, IN×21, NN×20\n",
            "PPHS1     total=1188  ( 9.4%)  →  PRP×1066, IN×18, ,×18\n",
            "APPGE     total=1159  ( 9.2%)  →  PRP$×1039, NN×25, PRP×21\n",
            "IO        total= 852  ( 6.8%)  →  IN×715, DT×48, NN×22\n",
            "VBDZ      total= 588  ( 4.7%)  →  VBD×547, PRP×9, ,×8\n",
            "PPH1      total= 484  ( 3.8%)  →  PRP×405, VB×16, IN×15\n",
            "VHD       total= 428  ( 3.4%)  →  VBD×355, IN×20, MD×19\n",
            "PPIS1     total= 420  ( 3.3%)  →  PRP×355, IN×10, .×9\n",
            "PPHO1     total= 407  ( 3.2%)  →  PRP×329, PRP$×32, NN×8\n",
            "DD1       total= 328  ( 2.6%)  →  DT×250, IN×32, PRP×8\n",
            "IW        total= 240  ( 1.9%)  →  IN×221, CC×4, DT×3\n",
            "\n",
            "spacy_sm: Top CLAWS sources of UNK\n",
            "----------------------------------------------------------------------\n",
            "AT        total= 472  (15.0%)  →  DT×404, IN×16, NN×15\n",
            "AT1       total= 301  ( 9.6%)  →  DT×259, IN×7, JJ×4\n",
            "PPHS1     total= 297  ( 9.4%)  →  PRP×269, ,×5, IN×5\n",
            "APPGE     total= 290  ( 9.2%)  →  PRP$×259, NN×7, PRP×5\n",
            "IO        total= 213  ( 6.8%)  →  IN×172, DT×16, JJ×6\n",
            "VBDZ      total= 147  ( 4.7%)  →  VBD×135, ,×3, PRP×2\n",
            "PPH1      total= 121  ( 3.8%)  →  PRP×100, IN×5, VB×5\n",
            "VHD       total= 107  ( 3.4%)  →  VBD×88, IN×6, MD×4\n",
            "PPIS1     total= 105  ( 3.3%)  →  PRP×88, .×3, IN×3\n",
            "PPHO1     total= 102  ( 3.2%)  →  PRP×86, PRP$×3, VB×3\n",
            "DD1       total=  82  ( 2.6%)  →  DT×61, IN×9, PRP×2\n",
            "IW        total=  60  ( 1.9%)  →  IN×55, DT×1, VBZ×1\n",
            "\n",
            "spacy_lg: Top CLAWS sources of UNK\n",
            "----------------------------------------------------------------------\n",
            "AT        total= 472  (15.0%)  →  DT×404, IN×16, NN×15\n",
            "AT1       total= 301  ( 9.6%)  →  DT×259, IN×7, NN×5\n",
            "PPHS1     total= 297  ( 9.4%)  →  PRP×269, ,×5, IN×5\n",
            "APPGE     total= 290  ( 9.2%)  →  PRP$×258, NN×7, PRP×6\n",
            "IO        total= 213  ( 6.8%)  →  IN×172, DT×16, JJ×7\n",
            "VBDZ      total= 147  ( 4.7%)  →  VBD×135, ,×3, PRP×2\n",
            "PPH1      total= 121  ( 3.8%)  →  PRP×100, VB×5, IN×4\n",
            "VHD       total= 107  ( 3.4%)  →  VBD×87, IN×6, MD×5\n",
            "PPIS1     total= 105  ( 3.3%)  →  PRP×88, .×3, IN×3\n",
            "PPHO1     total= 102  ( 3.2%)  →  PRP×86, PRP$×3, VB×3\n",
            "DD1       total=  82  ( 2.6%)  →  DT×63, IN×7, PRP×2\n",
            "IW        total=  60  ( 1.9%)  →  IN×55, DT×1, VBZ×1\n",
            "\n",
            "flair: Top CLAWS sources of UNK\n",
            "----------------------------------------------------------------------\n",
            "AT        total= 472  (15.0%)  →  DT×432, IN×19, RB×5\n",
            "AT1       total= 301  ( 9.6%)  →  DT×276, IN×5, PRP×4\n",
            "PPHS1     total= 297  ( 9.4%)  →  PRP×274, ,×5, RB×4\n",
            "APPGE     total= 290  ( 9.2%)  →  PRP$×266, PRP×5, IN×5\n",
            "IO        total= 213  ( 6.8%)  →  IN×188, DT×7, NN×6\n",
            "VBDZ      total= 147  ( 4.7%)  →  VBD×139, PRP×4, ,×1\n",
            "PPH1      total= 121  ( 3.8%)  →  PRP×104, IN×4, VB×4\n",
            "VHD       total= 107  ( 3.4%)  →  VBD×94, IN×3, PRP×2\n",
            "PPIS1     total= 105  ( 3.3%)  →  PRP×91, ,×3, .×3\n",
            "PPHO1     total= 102  ( 3.2%)  →  PRP×90, PRP$×3, VBN×2\n",
            "DD1       total=  82  ( 2.6%)  →  DT×66, IN×9, PRP×2\n",
            "IW        total=  60  ( 1.9%)  →  IN×58, VBZ×1, NN×1\n",
            "\n",
            "nltk: Top CLAWS sources of UNK\n",
            "----------------------------------------------------------------------\n",
            "AT        total= 472  (15.0%)  →  DT×418, NN×18, IN×9\n",
            "AT1       total= 298  ( 9.5%)  →  DT×272, NN×10, JJ×4\n",
            "PPHS1     total= 297  ( 9.5%)  →  PRP×254, VBD×10, RB×4\n",
            "APPGE     total= 289  ( 9.2%)  →  PRP$×256, NN×9, PRP×5\n",
            "IO        total= 213  ( 6.8%)  →  IN×183, DT×9, NN×4\n",
            "VBDZ      total= 147  ( 4.7%)  →  VBD×138, NN×2, PRP×1\n",
            "PPH1      total= 121  ( 3.9%)  →  PRP×101, VBD×3, IN×2\n",
            "VHD       total= 107  ( 3.4%)  →  VBD×86, MD×8, IN×5\n",
            "PPIS1     total= 105  ( 3.3%)  →  PRP×88, VBP×6, VB×3\n",
            "PPHO1     total= 101  ( 3.2%)  →  PRP×67, PRP$×23, IN×4\n",
            "DD1       total=  82  ( 2.6%)  →  DT×60, IN×7, NN×5\n",
            "IW        total=  60  ( 1.9%)  →  IN×53, CC×2, JJ×1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Build performance_summary directly from expanded_batch_results\n",
        "# ==============================================================================\n",
        "\n",
        "EXCLUDE_TOOLS = {'textblob'}  # ensure TextBlob is not considered\n",
        "\n",
        "performance_summary = {}\n",
        "\n",
        "for res in expanded_batch_results:\n",
        "    gt_claws = res.get('ground_truth', [])\n",
        "    # Strictly project CLAWS → Penn for gold\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt_claws]\n",
        "\n",
        "    tri_map = res.get('tool_results', {})\n",
        "    for tool, tri in tri_map.items():\n",
        "        # drop excluded tools (e.g., textblob)\n",
        "        if tool in EXCLUDE_TOOLS:\n",
        "            continue\n",
        "        # skip errors / malformed entries\n",
        "        if not isinstance(tri, dict) or ('error' in tri):\n",
        "            continue\n",
        "\n",
        "        pred = tri.get('tags') or []\n",
        "        toks = tri.get('tokens') or []\n",
        "        # align by minimum length across gold, predictions, and tokens\n",
        "        m = min(len(gt_penn), len(pred), len(toks))\n",
        "        if m == 0:\n",
        "            continue\n",
        "\n",
        "        acc = sum(1 for i in range(m) if gt_penn[i] == pred[i]) / m\n",
        "\n",
        "        # init bucket and record\n",
        "        if tool not in performance_summary:\n",
        "            performance_summary[tool] = {\n",
        "                'accuracies': [],\n",
        "                'total_sentences': 0,\n",
        "                'perfect_sentences': 0\n",
        "            }\n",
        "        performance_summary[tool]['accuracies'].append(acc)\n",
        "        performance_summary[tool]['total_sentences'] += 1\n",
        "        if acc == 1.0:\n",
        "            performance_summary[tool]['perfect_sentences'] += 1\n",
        "\n",
        "# Finalize stats\n",
        "for tool, d in list(performance_summary.items()):\n",
        "    accs = d['accuracies']\n",
        "    performance_summary[tool] = {\n",
        "        'mean_accuracy': float(np.mean(accs)) if accs else 0.0,\n",
        "        'std_accuracy': float(np.std(accs)) if accs else 0.0,\n",
        "        'total_sentences': int(d['total_sentences']),\n",
        "        'perfect_sentences': int(d['perfect_sentences'])\n",
        "    }\n",
        "\n",
        "print(\"Tools available for pairwise tests (excluding TextBlob):\", sorted(performance_summary.keys()))\n"
      ],
      "metadata": {
        "id": "EXx_Xn0X-6gn",
        "outputId": "a072d692-fdba-4af5-b400-533e6fa395bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tools available for pairwise tests (excluding TextBlob): ['flair', 'nltk', 'spacy_lg', 'spacy_sm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# WILSON CONFIDENCE INTERVALS (STRICT) + SIGNIFICANCE (OVERALL, no TextBlob)\n",
        "# ==============================================================================\n",
        "\n",
        "import scipy.stats as stats\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "EXCLUDE_TOOLS = {'textblob'}\n",
        "\n",
        "def wilson_confidence_interval(successes, trials, confidence=0.95):\n",
        "    if trials == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    p = successes / trials\n",
        "    z = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
        "    denom = 1 + z**2 / trials\n",
        "    centre = (p + z**2 / (2 * trials)) / denom\n",
        "    half = z * sqrt((p * (1 - p) + z**2 / (4 * trials)) / trials) / denom\n",
        "    lower = max(0, centre - half)\n",
        "    upper = min(1, centre + half)\n",
        "    return p, lower, upper\n",
        "\n",
        "def proportion_z_test(x1, n1, x2, n2):\n",
        "    p1, p2 = x1 / n1, x2 / n2\n",
        "    p_pool = (x1 + x2) / (n1 + n2)\n",
        "    se = sqrt(p_pool * (1 - p_pool) * (1/n1 + 1/n2))\n",
        "    z = (p1 - p2) / se\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "    return z, p_value\n",
        "\n",
        "def cohens_h(p1, p2):\n",
        "    return 2 * (np.arcsin(sqrt(p1)) - np.arcsin(sqrt(p2)))\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Ensure performance_summary exists (build from expanded_batch_results if not)\n",
        "# --------------------------------------------------------------------------\n",
        "if 'performance_summary' not in globals() or not performance_summary:\n",
        "    performance_summary = {}\n",
        "    for res in expanded_batch_results:\n",
        "        gt_claws = res.get('ground_truth', [])\n",
        "        gt_penn  = [convert_claws_to_penn(t, strict=True) for t in gt_claws]\n",
        "        tri_map  = res.get('tool_results', {})\n",
        "        for tool, tri in tri_map.items():\n",
        "            if tool in EXCLUDE_TOOLS:\n",
        "                continue\n",
        "            if not isinstance(tri, dict) or ('error' in tri):\n",
        "                continue\n",
        "            pred = tri.get('tags') or []\n",
        "            toks = tri.get('tokens') or []\n",
        "            m = min(len(gt_penn), len(pred), len(toks))\n",
        "            if m == 0:\n",
        "                continue\n",
        "            acc = sum(1 for i in range(m) if gt_penn[i] == pred[i]) / m\n",
        "            if tool not in performance_summary:\n",
        "                performance_summary[tool] = {\n",
        "                    'accuracies': [],\n",
        "                    'total_sentences': 0,\n",
        "                    'perfect_sentences': 0\n",
        "                }\n",
        "            performance_summary[tool]['accuracies'].append(acc)\n",
        "            performance_summary[tool]['total_sentences'] += 1\n",
        "            if acc == 1.0:\n",
        "                performance_summary[tool]['perfect_sentences'] += 1\n",
        "\n",
        "    for tool, d in list(performance_summary.items()):\n",
        "        accs = d['accuracies']\n",
        "        performance_summary[tool] = {\n",
        "            'mean_accuracy': float(np.mean(accs)) if accs else 0.0,\n",
        "            'std_accuracy': float(np.std(accs)) if accs else 0.0,\n",
        "            'total_sentences': int(d['total_sentences']),\n",
        "            'perfect_sentences': int(d['perfect_sentences'])\n",
        "        }\n",
        "\n",
        "print(\"Tool Performance with Wilson 95% Confidence Intervals (STRICT):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "wilson_results = {}\n",
        "aligned_sentence_counts = {}  # per-tool count of sentences with m>0\n",
        "\n",
        "for tool_name, stats_data in performance_summary.items():\n",
        "    if tool_name in EXCLUDE_TOOLS:\n",
        "        continue\n",
        "\n",
        "    total_sentences = stats_data['total_sentences']\n",
        "    perfect_sentences = stats_data['perfect_sentences']\n",
        "\n",
        "    # Perfect-sentence Wilson CI\n",
        "    perfect_rate, perfect_lower, perfect_upper = wilson_confidence_interval(\n",
        "        perfect_sentences, total_sentences\n",
        "    )\n",
        "\n",
        "    # Token-level totals + aligned sentence count\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "    aligned_sents = 0\n",
        "\n",
        "    for res in expanded_batch_results:\n",
        "        tri = res.get('tool_results', {}).get(tool_name, {})\n",
        "        if not tri or 'error' in tri:\n",
        "            continue\n",
        "        gt = res.get('ground_truth', [])\n",
        "        pred = tri.get('tags', [])\n",
        "        toks = tri.get('tokens', [])\n",
        "        m = min(len(gt), len(pred), len(toks))\n",
        "        if m == 0:\n",
        "            continue\n",
        "        aligned_sents += 1\n",
        "        gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "        correct_tokens += sum(1 for i in range(m) if gt_penn[i] == pred[i])\n",
        "        total_tokens += m\n",
        "\n",
        "    aligned_sentence_counts[tool_name] = aligned_sents\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        # No usable tokens for this tool; skip it\n",
        "        continue\n",
        "\n",
        "    token_accuracy, token_lower, token_upper = wilson_confidence_interval(\n",
        "        correct_tokens, total_tokens\n",
        "    )\n",
        "\n",
        "    wilson_results[tool_name] = {\n",
        "        'token_accuracy': token_accuracy,\n",
        "        'token_ci_lower': token_lower,\n",
        "        'token_ci_upper': token_upper,\n",
        "        'perfect_rate': perfect_rate,\n",
        "        'perfect_ci_lower': perfect_lower,\n",
        "        'perfect_ci_upper': perfect_upper,\n",
        "        'total_tokens': total_tokens,\n",
        "        'correct_tokens': correct_tokens,\n",
        "        'total_sentences': total_sentences,\n",
        "        'perfect_sentences': perfect_sentences,\n",
        "        'aligned_sentences': aligned_sents\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{tool_name.upper()}:\")\n",
        "    print(f\"  Token Accuracy: {token_accuracy:.3f} [{token_lower:.3f}, {token_upper:.3f}]\")\n",
        "    print(f\"  Perfect Sentences: {perfect_rate:.3f} [{perfect_lower:.3f}, {perfect_upper:.3f}]\")\n",
        "    print(f\"  Sample size: {total_tokens:,} tokens, {total_sentences} sentences \"\n",
        "          f\"(aligned sentences used: {aligned_sents})\")\n",
        "\n",
        "# Guard and significance test\n",
        "if not wilson_results:\n",
        "    print(\"\\nNo token-level data available to compute Wilson intervals. \"\n",
        "          \"Check that expanded_batch_results is populated and tool predictions exist.\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    tools_by_accuracy = sorted(wilson_results.items(),\n",
        "                               key=lambda x: x[1]['token_accuracy'],\n",
        "                               reverse=True)\n",
        "    best_tool, best_stats = tools_by_accuracy[0]\n",
        "    worst_tool, worst_stats = tools_by_accuracy[-1]\n",
        "\n",
        "    z_stat, p_value = proportion_z_test(\n",
        "        best_stats['correct_tokens'], best_stats['total_tokens'],\n",
        "        worst_stats['correct_tokens'], worst_stats['total_tokens']\n",
        "    )\n",
        "\n",
        "    print(f\"Comparison: {best_tool} vs {worst_tool}\")\n",
        "    print(f\"Accuracy difference: {best_stats['token_accuracy'] - worst_stats['token_accuracy']:.3f}\")\n",
        "    print(f\"Z-statistic: {z_stat:.3f}\")\n",
        "    print(f\"P-value: {p_value:.3f}\")\n",
        "    sig = \"Yes\" if p_value < 0.05 else \"No\"\n",
        "    print(f\"Significant at α=0.05: {sig}\")\n",
        "\n",
        "    effect_size = cohens_h(best_stats['token_accuracy'], worst_stats['token_accuracy'])\n",
        "    if abs(effect_size) < 0.2:\n",
        "        magnitude = \"negligible\"\n",
        "    elif abs(effect_size) < 0.5:\n",
        "        magnitude = \"small\"\n",
        "    elif abs(effect_size) < 0.8:\n",
        "        magnitude = \"medium\"\n",
        "    else:\n",
        "        magnitude = \"large\"\n",
        "    print(f\"Effect size (Cohen's h): {effect_size:.3f} ({magnitude})\")\n",
        "\n",
        "# Optional sanity check: show aligned sentence counts per tool\n",
        "print(\"\\nAligned sentences used per tool (should be close to 383 if all align):\")\n",
        "for t, n in sorted(aligned_sentence_counts.items()):\n",
        "    print(f\"  {t:10s}: {n}\")\n"
      ],
      "metadata": {
        "id": "ShkhL401_8b1",
        "outputId": "17ee48b7-c9e8-4f39-d2e3-7e66c6b6d60a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool Performance with Wilson 95% Confidence Intervals (STRICT):\n",
            "============================================================\n",
            "\n",
            "SPACY_SM:\n",
            "  Token Accuracy: 0.527 [0.518, 0.537]\n",
            "  Perfect Sentences: 0.000 [0.000, 0.010]\n",
            "  Sample size: 9,994 tokens, 383 sentences (aligned sentences used: 383)\n",
            "\n",
            "SPACY_LG:\n",
            "  Token Accuracy: 0.530 [0.520, 0.540]\n",
            "  Perfect Sentences: 0.000 [0.000, 0.010]\n",
            "  Sample size: 9,994 tokens, 383 sentences (aligned sentences used: 383)\n",
            "\n",
            "NLTK:\n",
            "  Token Accuracy: 0.531 [0.521, 0.540]\n",
            "  Perfect Sentences: 0.000 [0.000, 0.010]\n",
            "  Sample size: 9,941 tokens, 383 sentences (aligned sentences used: 383)\n",
            "\n",
            "FLAIR:\n",
            "  Token Accuracy: 0.559 [0.549, 0.569]\n",
            "  Perfect Sentences: 0.000 [0.000, 0.010]\n",
            "  Sample size: 9,994 tokens, 383 sentences (aligned sentences used: 383)\n",
            "\n",
            "============================================================\n",
            "STATISTICAL SIGNIFICANCE TESTS\n",
            "============================================================\n",
            "Comparison: flair vs spacy_sm\n",
            "Accuracy difference: 0.032\n",
            "Z-statistic: 4.487\n",
            "P-value: 0.000\n",
            "Significant at α=0.05: Yes\n",
            "Effect size (Cohen's h): 0.063 (negligible)\n",
            "\n",
            "Aligned sentences used per tool (should be close to 383 if all align):\n",
            "  flair     : 383\n",
            "  nltk      : 383\n",
            "  spacy_lg  : 383\n",
            "  spacy_sm  : 383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Pairwise McNemar’s Tests (with Holm–Bonferroni) + Cluster Bootstrap CIs\n",
        "# ==============================================================================\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "from scipy.stats import binomtest\n",
        "\n",
        "# Try to import statsmodels' McNemar; fall back to an exact/binomial implementation\n",
        "try:\n",
        "    from statsmodels.stats.contingency_tables import mcnemar as _sm_mcnemar\n",
        "    HAVE_STATSMODELS = True\n",
        "except Exception:\n",
        "    HAVE_STATSMODELS = False\n",
        "\n",
        "def _mcnemar_from_table(table, exact):\n",
        "    \"\"\"\n",
        "    Return (statistic, pvalue) for McNemar from 2x2 table:\n",
        "      [[both_correct, a_correct_b_wrong],\n",
        "       [b_correct_a_wrong, both_wrong]]\n",
        "    If statsmodels is present, use it; otherwise compute:\n",
        "      - exact: two-sided binomial on min(b01,b10) with n=b01+b10, p=0.5\n",
        "      - chi^2 (no continuity) when exact=False   (continuity not added in fallback)\n",
        "    \"\"\"\n",
        "    b01 = table[0][1]\n",
        "    b10 = table[1][0]\n",
        "    discordant = b01 + b10\n",
        "    if discordant == 0:\n",
        "        # no information to test; define stat=0, p=1\n",
        "        return 0.0, 1.0\n",
        "\n",
        "    if HAVE_STATSMODELS:\n",
        "        res = _sm_mcnemar(table, exact=exact, correction=(not exact))\n",
        "        stat = float(res.statistic) if res.statistic is not None else np.nan\n",
        "        return stat, float(res.pvalue)\n",
        "\n",
        "    # Fallbacks\n",
        "    if exact:\n",
        "        # Two-sided exact binomial test under H0: p = 0.5\n",
        "        k = min(b01, b10)\n",
        "        p = binomtest(k, n=discordant, p=0.5, alternative='two-sided').pvalue\n",
        "        # A chi^2-like descriptive stat (not used for decision when exact=True)\n",
        "        stat = (b01 - b10) ** 2 / discordant\n",
        "        return float(stat), float(p)\n",
        "    else:\n",
        "        # Large-sample chi-square without continuity (approx.)\n",
        "        stat = (b01 - b10) ** 2 / discordant\n",
        "        # Two-sided p-value from chi-square(1)\n",
        "        from scipy.stats import chi2\n",
        "        p = 1 - chi2.cdf(stat, df=1)\n",
        "        return float(stat), float(p)\n",
        "\n",
        "def build_mcnemar_table(tool_a, tool_b, results):\n",
        "    \"\"\"\n",
        "    Build 2x2 contingency for paired token outcomes:\n",
        "      [[ both_correct,  a_correct_b_wrong ],\n",
        "       [ b_correct_a_wrong, both_wrong     ]]\n",
        "    STRICT mapping for gold.\n",
        "    \"\"\"\n",
        "    both_correct = both_wrong = a_correct_b_wrong = b_correct_a_wrong = 0\n",
        "\n",
        "    for res in results:\n",
        "        gt = res['ground_truth']\n",
        "        tri_a = res['tool_results'].get(tool_a, {})\n",
        "        tri_b = res['tool_results'].get(tool_b, {})\n",
        "        if not isinstance(tri_a, dict) or not isinstance(tri_b, dict):\n",
        "            continue\n",
        "        if 'error' in tri_a or 'error' in tri_b:\n",
        "            continue\n",
        "\n",
        "        pred_a = tri_a.get('tags', [])\n",
        "        pred_b = tri_b.get('tags', [])\n",
        "        toks_a = tri_a.get('tokens', [])\n",
        "        toks_b = tri_b.get('tokens', [])\n",
        "\n",
        "        m = min(len(gt), len(pred_a), len(pred_b), len(toks_a), len(toks_b))\n",
        "        if m == 0:\n",
        "            continue\n",
        "\n",
        "        gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "\n",
        "        for i in range(m):\n",
        "            a_correct = (gt_penn[i] == pred_a[i])\n",
        "            b_correct = (gt_penn[i] == pred_b[i])\n",
        "            if a_correct and b_correct:\n",
        "                both_correct += 1\n",
        "            elif (not a_correct) and (not b_correct):\n",
        "                both_wrong += 1\n",
        "            elif a_correct and (not b_correct):\n",
        "                a_correct_b_wrong += 1\n",
        "            else:  # b_correct and not a_correct\n",
        "                b_correct_a_wrong += 1\n",
        "\n",
        "    return [[both_correct, a_correct_b_wrong],\n",
        "            [b_correct_a_wrong, both_wrong]]\n",
        "\n",
        "def run_mcnemar_for_all_pairs(results, tool_names):\n",
        "    \"\"\"\n",
        "    Runs McNemar for each unordered tool pair.\n",
        "    Uses exact test when discordant count < 25, else chi^2 with continuity (if statsmodels).\n",
        "    Returns list of dicts with stats and raw p-values.\n",
        "    \"\"\"\n",
        "    outcomes = []\n",
        "    for a, b in itertools.combinations(tool_names, 2):\n",
        "        table = build_mcnemar_table(a, b, results)\n",
        "        b01 = table[0][1]\n",
        "        b10 = table[1][0]\n",
        "        discordant = b01 + b10\n",
        "        exact = discordant < 25  # common rule-of-thumb\n",
        "        chi2, p = _mcnemar_from_table(table, exact=exact)\n",
        "        outcomes.append({\n",
        "            'tool_a': a, 'tool_b': b,\n",
        "            'table': table,\n",
        "            'discordant': discordant,\n",
        "            'exact': exact,\n",
        "            'chi2': chi2,\n",
        "            'p_value': p\n",
        "        })\n",
        "    return outcomes\n",
        "\n",
        "def holm_bonferroni_adjust(pvals):\n",
        "    \"\"\"\n",
        "    Holm–Bonferroni step-down procedure for FWER control.\n",
        "    Returns adjusted p-values in the original order.\n",
        "    \"\"\"\n",
        "    m = len(pvals)\n",
        "    if m == 0:\n",
        "        return np.array([])\n",
        "    order = np.argsort(pvals)\n",
        "    adj = np.empty(m, dtype=float)\n",
        "    running_max = 0.0\n",
        "    for rank, idx in enumerate(order, start=1):\n",
        "        p = pvals[idx]\n",
        "        adj_p = (m - rank + 1) * p\n",
        "        running_max = max(running_max, adj_p)\n",
        "        adj[idx] = min(1.0, running_max)\n",
        "    return adj\n",
        "\n",
        "def sentence_accuracy(tool_name, res):\n",
        "    \"\"\"\n",
        "    Returns (#correct, #total) tokens for a single sentence result and tool,\n",
        "    under STRICT mapping.\n",
        "    \"\"\"\n",
        "    tri = res['tool_results'].get(tool_name, {})\n",
        "    if not isinstance(tri, dict) or 'error' in tri:\n",
        "        return 0, 0\n",
        "    gt = res['ground_truth']\n",
        "    pred = tri.get('tags', [])\n",
        "    toks = tri.get('tokens', [])\n",
        "    m = min(len(gt), len(pred), len(toks))\n",
        "    if m == 0:\n",
        "        return 0, 0\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "    correct = sum(1 for i in range(m) if gt_penn[i] == pred[i])\n",
        "    return correct, m\n",
        "\n",
        "def bootstrap_accuracy_diff_sentence_cluster(tool_a, tool_b, results, n_iter=3000, seed=123):\n",
        "    \"\"\"\n",
        "    Cluster bootstrap by sentence: resample sentences with replacement,\n",
        "    compute token-level accuracy per tool, then take difference acc_a - acc_b.\n",
        "    Returns mean diff and 95% percentile CI.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(results)\n",
        "    diffs = np.empty(n_iter, dtype=float)\n",
        "\n",
        "    # Precompute per-sentence (correct, total) for speed\n",
        "    per_sent = []\n",
        "    for res in results:\n",
        "        ca, na = sentence_accuracy(tool_a, res)\n",
        "        cb, nb = sentence_accuracy(tool_b, res)\n",
        "        per_sent.append((ca, na, cb, nb))\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        ca_sum = na_sum = cb_sum = nb_sum = 0\n",
        "        for j in idx:\n",
        "            ca, na, cb, nb = per_sent[j]\n",
        "            ca_sum += ca; na_sum += na\n",
        "            cb_sum += cb; nb_sum += nb\n",
        "        acc_a = ca_sum / na_sum if na_sum else 0.0\n",
        "        acc_b = cb_sum / nb_sum if nb_sum else 0.0\n",
        "        diffs[i] = acc_a - acc_b\n",
        "\n",
        "    mean_diff = float(np.mean(diffs))\n",
        "    low, high = np.percentile(diffs, [2.5, 97.5])\n",
        "    return mean_diff, float(low), float(high)\n",
        "\n",
        "# --- Choose tools from your performance_summary; drop any with no token data ---\n",
        "all_tools = sorted(performance_summary.keys())\n",
        "# If you ever want to explicitly exclude a tool (e.g., 'textblob'), uncomment:\n",
        "# all_tools = [t for t in all_tools if t != 'textblob']\n",
        "\n",
        "if len(all_tools) < 2:\n",
        "    print(\"Not enough tools for pairwise tests.\")\n",
        "else:\n",
        "    # 1) McNemar for all pairs\n",
        "    mcnemar_outcomes = run_mcnemar_for_all_pairs(expanded_batch_results, all_tools)\n",
        "    pvals = np.array([o['p_value'] for o in mcnemar_outcomes])\n",
        "    adj_pvals = holm_bonferroni_adjust(pvals)\n",
        "\n",
        "    print(\"\\nPAIRWISE McNemar’s tests (token-level, STRICT gold projection):\")\n",
        "    print(\"=\" * 80)\n",
        "    for o, adjp in zip(mcnemar_outcomes, adj_pvals):\n",
        "        a, b = o['tool_a'], o['tool_b']\n",
        "        t = o['table']\n",
        "        print(f\"\\n{a} vs {b}\")\n",
        "        print(f\"  Table [[both_correct, a_correct_b_wrong], [b_correct_a_wrong, both_wrong]] = {t}\")\n",
        "        print(f\"  Discordant = {o['discordant']} | {'exact' if o['exact'] else 'chi^2'} test\")\n",
        "        print(f\"  McNemar χ² = {o['chi2']:.3f} | p = {o['p_value']:.4g} | Holm-adjusted p = {adjp:.4g}\")\n",
        "        if adjp < 0.05:\n",
        "            print(\"  → Significant asymmetry in disagreements (after Holm–Bonferroni).\")\n",
        "        else:\n",
        "            print(\"  → No significant asymmetry in disagreements (after correction).\")\n",
        "\n",
        "    # 2) Cluster bootstrap CIs for accuracy differences for all pairs\n",
        "    print(\"\\nPAIRWISE Bootstrap 95% CIs for accuracy differences (acc_A − acc_B):\")\n",
        "    print(\"=\" * 80)\n",
        "    for a, b in itertools.combinations(all_tools, 2):\n",
        "        mean_diff, lo, hi = bootstrap_accuracy_diff_sentence_cluster(a, b, expanded_batch_results,\n",
        "                                                                     n_iter=3000, seed=123)\n",
        "        note = \" (A>B)\" if (lo > 0) else (\" (B>A)\" if (hi < 0) else \" (no clear difference)\")\n",
        "        print(f\"{a:10s} − {b:10s}: mean = {mean_diff:.3f}, 95% CI = [{lo:.3f}, {hi:.3f}]{note}\")\n"
      ],
      "metadata": {
        "id": "oIA9qfQUAf1j",
        "outputId": "c1f1d9b7-fc10-421f-ac52-03cd9d8dba6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PAIRWISE McNemar’s tests (token-level, STRICT gold projection):\n",
            "================================================================================\n",
            "\n",
            "flair vs nltk\n",
            "  Table [[both_correct, a_correct_b_wrong], [b_correct_a_wrong, both_wrong]] = [[4815, 741], [458, 3926]]\n",
            "  Discordant = 1199 | chi^2 test\n",
            "  McNemar χ² = 66.325 | p = 3.823e-16 | Holm-adjusted p = 1.529e-15\n",
            "  → Significant asymmetry in disagreements (after Holm–Bonferroni).\n",
            "\n",
            "flair vs spacy_lg\n",
            "  Table [[both_correct, a_correct_b_wrong], [b_correct_a_wrong, both_wrong]] = [[5079, 508], [220, 4186]]\n",
            "  Discordant = 728 | chi^2 test\n",
            "  McNemar χ² = 113.144 | p = 2.006e-26 | Holm-adjusted p = 1.003e-25\n",
            "  → Significant asymmetry in disagreements (after Holm–Bonferroni).\n",
            "\n",
            "flair vs spacy_sm\n",
            "  Table [[both_correct, a_correct_b_wrong], [b_correct_a_wrong, both_wrong]] = [[5054, 533], [217, 4189]]\n",
            "  Discordant = 750 | chi^2 test\n",
            "  McNemar χ² = 132.300 | p = 1.286e-30 | Holm-adjusted p = 7.718e-30\n",
            "  → Significant asymmetry in disagreements (after Holm–Bonferroni).\n",
            "\n",
            "nltk vs spacy_lg\n",
            "  Table [[both_correct, a_correct_b_wrong], [b_correct_a_wrong, both_wrong]] = [[4616, 658], [657, 4010]]\n",
            "  Discordant = 1315 | chi^2 test\n",
            "  McNemar χ² = 0.000 | p = 1 | Holm-adjusted p = 1\n",
            "  → No significant asymmetry in disagreements (after correction).\n",
            "\n",
            "nltk vs spacy_sm\n",
            "  Table [[both_correct, a_correct_b_wrong], [b_correct_a_wrong, both_wrong]] = [[4600, 674], [645, 4022]]\n",
            "  Discordant = 1319 | chi^2 test\n",
            "  McNemar χ² = 0.594 | p = 0.4407 | Holm-adjusted p = 0.8815\n",
            "  → No significant asymmetry in disagreements (after correction).\n",
            "\n",
            "spacy_lg vs spacy_sm\n",
            "  Table [[both_correct, a_correct_b_wrong], [b_correct_a_wrong, both_wrong]] = [[5223, 76], [48, 4647]]\n",
            "  Discordant = 124 | chi^2 test\n",
            "  McNemar χ² = 5.879 | p = 0.01532 | Holm-adjusted p = 0.04597\n",
            "  → Significant asymmetry in disagreements (after Holm–Bonferroni).\n",
            "\n",
            "PAIRWISE Bootstrap 95% CIs for accuracy differences (acc_A − acc_B):\n",
            "================================================================================\n",
            "flair      − nltk      : mean = 0.028, 95% CI = [0.002, 0.054] (A>B)\n",
            "flair      − spacy_lg  : mean = 0.029, 95% CI = [0.009, 0.052] (A>B)\n",
            "flair      − spacy_sm  : mean = 0.031, 95% CI = [0.012, 0.054] (A>B)\n",
            "nltk       − spacy_lg  : mean = 0.001, 95% CI = [-0.026, 0.026] (no clear difference)\n",
            "nltk       − spacy_sm  : mean = 0.003, 95% CI = [-0.024, 0.029] (no clear difference)\n",
            "spacy_lg   − spacy_sm  : mean = 0.003, 95% CI = [0.001, 0.005] (A>B)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STRICT metrics: Precision/Recall/F1 (micro/macro/weighted)\n",
        "# + Newcombe (Wilson) CIs for differences in accuracy\n",
        "# (filters out TextBlob; includes spaCy_sm, spaCy_lg, Flair, NLTK)\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def collect_token_outcomes_strict(results, allowed_tools=None):\n",
        "    \"\"\"\n",
        "    Build per-tool gold/pred arrays under STRICT projection.\n",
        "    Only tools in allowed_tools are kept (if provided).\n",
        "    Returns:\n",
        "      per_tool = {\n",
        "         tool: {\n",
        "            'y_true': [... Penn tags ...],\n",
        "            'y_pred': [... Penn tags ...],\n",
        "            'correct_tokens': int,\n",
        "            'total_tokens': int\n",
        "         }, ...\n",
        "      }\n",
        "    \"\"\"\n",
        "    per_tool = defaultdict(lambda: {'y_true': [], 'y_pred': [], 'correct_tokens': 0, 'total_tokens': 0})\n",
        "\n",
        "    for res in results:\n",
        "        gt_claws = res.get('ground_truth', [])\n",
        "        tri_map = res.get('tool_results', {})\n",
        "        for tool, tri in tri_map.items():\n",
        "            if allowed_tools is not None and tool not in allowed_tools:\n",
        "                continue\n",
        "            if not isinstance(tri, dict) or ('error' in tri):\n",
        "                continue\n",
        "\n",
        "            pred = tri.get('tags', []) or []\n",
        "            toks = tri.get('tokens', []) or []\n",
        "            m = min(len(gt_claws), len(pred), len(toks))\n",
        "            if m <= 0:\n",
        "                continue\n",
        "\n",
        "            gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt_claws[:m]]\n",
        "            y_true = gt_penn\n",
        "            y_pred = pred[:m]\n",
        "\n",
        "            per_tool[tool]['y_true'].extend(y_true)\n",
        "            per_tool[tool]['y_pred'].extend(y_pred)\n",
        "            per_tool[tool]['total_tokens'] += m\n",
        "            per_tool[tool]['correct_tokens'] += sum(1 for i in range(m) if y_true[i] == y_pred[i])\n",
        "\n",
        "    return per_tool\n",
        "\n",
        "def wilson_interval(successes, n, confidence=0.95):\n",
        "    \"\"\"Wilson score interval for a single proportion.\"\"\"\n",
        "    if n == 0:\n",
        "        return (0.0, 0.0, 0.0)\n",
        "    from scipy.stats import norm\n",
        "    z = norm.ppf(1 - (1 - confidence)/2)\n",
        "    p = successes / n\n",
        "    denom = 1 + z*z/n\n",
        "    centre = (p + z*z/(2*n)) / denom\n",
        "    half = z * np.sqrt((p*(1-p) + z*z/(4*n))/n) / denom\n",
        "    return (p, max(0.0, centre - half), min(1.0, centre + half))\n",
        "\n",
        "def newcombe_wilson_diff(x1, n1, x2, n2, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Newcombe method 10 (1998): CI for difference of two independent proportions.\n",
        "    This complements paired analyses (McNemar / bootstrap) with a descriptive CI.\n",
        "    \"\"\"\n",
        "    p1, L1, U1 = wilson_interval(x1, n1, confidence)\n",
        "    p2, L2, U2 = wilson_interval(x2, n2, confidence)\n",
        "    diff = p1 - p2\n",
        "    lower = L1 - U2\n",
        "    upper = U1 - L2\n",
        "    return diff, lower, upper, (p1, L1, U1), (p2, L2, U2)\n",
        "\n",
        "# Choose tools explicitly (exclude textblob)\n",
        "discovered = {t for r in expanded_batch_results for t in r.get('tool_results', {}).keys()}\n",
        "tools_for_eval = [t for t in ('spacy_sm','spacy_lg','flair','nltk') if t in discovered]\n",
        "\n",
        "per_tool = collect_token_outcomes_strict(expanded_batch_results, allowed_tools=tools_for_eval)\n",
        "\n",
        "if not per_tool:\n",
        "    print(\"No token-level data found. Make sure expanded_batch_results is populated.\")\n",
        "else:\n",
        "    # Build a consistent label set across the included tools\n",
        "    label_set = set()\n",
        "    for d in per_tool.values():\n",
        "        label_set.update(d['y_true'])\n",
        "        label_set.update(d['y_pred'])\n",
        "    labels = sorted(label_set)\n",
        "\n",
        "    print(\"Token-level Precision / Recall / F1 under STRICT gold\")\n",
        "    print(\"=\"*70)\n",
        "    metrics_summary = {}\n",
        "\n",
        "    # Print tools in a stable order (by name)\n",
        "    for tool in sorted(per_tool.keys()):\n",
        "        d = per_tool[tool]\n",
        "        y_true = np.array(d['y_true'])\n",
        "        y_pred = np.array(d['y_pred'])\n",
        "        if y_true.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Averages\n",
        "        prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, labels=labels, average='micro', zero_division=0\n",
        "        )\n",
        "        prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, labels=labels, average='macro', zero_division=0\n",
        "        )\n",
        "        prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, labels=labels, average='weighted', zero_division=0\n",
        "        )\n",
        "\n",
        "        # Per-class (optional): show top classes by support\n",
        "        _, _, f1_per_class, support = precision_recall_fscore_support(\n",
        "            y_true, y_pred, labels=labels, average=None, zero_division=0\n",
        "        )\n",
        "        support_idx = np.argsort(support)[::-1]\n",
        "        topK = 10\n",
        "        top_rows = [(labels[i], int(support[i]), float(f1_per_class[i])) for i in support_idx[:topK]]\n",
        "\n",
        "        acc = d['correct_tokens'] / d['total_tokens'] if d['total_tokens'] else 0.0\n",
        "        metrics_summary[tool] = {\n",
        "            'accuracy': acc,\n",
        "            'micro':  (prec_micro, rec_micro, f1_micro),\n",
        "            'macro':  (prec_macro, rec_macro, f1_macro),\n",
        "            'weighted': (prec_weighted, rec_weighted, f1_weighted),\n",
        "            'top_classes': top_rows,\n",
        "            'total_tokens': d['total_tokens'],\n",
        "            'correct_tokens': d['correct_tokens']\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{tool.upper()}:\")\n",
        "        print(f\"  Tokens: {d['total_tokens']:,} | Accuracy: {acc:.3f}\")\n",
        "        print(f\"  Micro  P/R/F1: {prec_micro:.3f} / {rec_micro:.3f} / {f1_micro:.3f}\")\n",
        "        print(f\"  Macro  P/R/F1: {prec_macro:.3f} / {rec_macro:.3f} / {f1_macro:.3f}\")\n",
        "        print(f\"  Weight P/R/F1: {prec_weighted:.3f} / {rec_weighted:.3f} / {f1_weighted:.3f}\")\n",
        "        print(f\"  Top {topK} classes by support (label, support, F1):\")\n",
        "        for lbl, sup, f1c in top_rows:\n",
        "            print(f\"    {lbl:>4s}  {sup:6d}  F1={f1c:.3f}\")\n",
        "\n",
        "    # Newcombe (Wilson) CIs for accuracy differences between every pair of tools\n",
        "    if metrics_summary:\n",
        "        print(\"\\nNewcombe (Wilson) 95% CIs for differences in token accuracy (acc_A − acc_B)\")\n",
        "        print(\"=\"*70)\n",
        "        tools = sorted(metrics_summary.keys())\n",
        "        for a, b in itertools.combinations(tools, 2):\n",
        "            ca, na = metrics_summary[a]['correct_tokens'], metrics_summary[a]['total_tokens']\n",
        "            cb, nb = metrics_summary[b]['correct_tokens'], metrics_summary[b]['total_tokens']\n",
        "            diff, lo, hi, p1_info, p2_info = newcombe_wilson_diff(ca, na, cb, nb, confidence=0.95)\n",
        "            print(f\"{a:10s} − {b:10s}: Δ = {diff:.3f}, 95% CI = [{lo:.3f}, {hi:.3f}]  \"\n",
        "                  f\"(p1={p1_info[0]:.3f}, p2={p2_info[0]:.3f})\")\n",
        "    else:\n",
        "        print(\"No metrics to compare.\")\n"
      ],
      "metadata": {
        "id": "mTyI7cchBnM0",
        "outputId": "95066e08-68dc-4fd5-a5a5-32010290739f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token-level Precision / Recall / F1 under STRICT gold\n",
            "======================================================================\n",
            "\n",
            "FLAIR:\n",
            "  Tokens: 9,994 | Accuracy: 0.559\n",
            "  Micro  P/R/F1: 0.559 / 0.559 / 0.559\n",
            "  Macro  P/R/F1: 0.417 / 0.527 / 0.437\n",
            "  Weight P/R/F1: 0.503 / 0.559 / 0.523\n",
            "  Top 10 classes by support (label, support, F1):\n",
            "     UNK    3148  F1=0.000\n",
            "      IN    1195  F1=0.766\n",
            "      NN    1153  F1=0.807\n",
            "      JJ     597  F1=0.767\n",
            "      RB     525  F1=0.780\n",
            "       .     407  F1=0.861\n",
            "     VBD     389  F1=0.620\n",
            "      VB     347  F1=0.658\n",
            "      CC     333  F1=0.902\n",
            "     NNS     330  F1=0.822\n",
            "\n",
            "NLTK:\n",
            "  Tokens: 9,941 | Accuracy: 0.531\n",
            "  Micro  P/R/F1: 0.531 / 0.531 / 0.531\n",
            "  Macro  P/R/F1: 0.365 / 0.489 / 0.396\n",
            "  Weight P/R/F1: 0.473 / 0.531 / 0.494\n",
            "  Top 10 classes by support (label, support, F1):\n",
            "     UNK    3140  F1=0.000\n",
            "      IN    1195  F1=0.709\n",
            "      NN    1144  F1=0.767\n",
            "      JJ     594  F1=0.697\n",
            "      RB     522  F1=0.759\n",
            "     VBD     388  F1=0.580\n",
            "       .     383  F1=0.876\n",
            "      VB     346  F1=0.590\n",
            "      CC     333  F1=0.882\n",
            "     NNS     329  F1=0.774\n",
            "\n",
            "SPACY_LG:\n",
            "  Tokens: 9,994 | Accuracy: 0.530\n",
            "  Micro  P/R/F1: 0.530 / 0.530 / 0.530\n",
            "  Macro  P/R/F1: 0.364 / 0.491 / 0.397\n",
            "  Weight P/R/F1: 0.478 / 0.530 / 0.497\n",
            "  Top 10 classes by support (label, support, F1):\n",
            "     UNK    3148  F1=0.000\n",
            "      IN    1195  F1=0.728\n",
            "      NN    1153  F1=0.758\n",
            "      JJ     597  F1=0.731\n",
            "      RB     525  F1=0.761\n",
            "       .     407  F1=0.848\n",
            "     VBD     389  F1=0.606\n",
            "      VB     347  F1=0.613\n",
            "      CC     333  F1=0.849\n",
            "     NNS     330  F1=0.775\n",
            "\n",
            "SPACY_SM:\n",
            "  Tokens: 9,994 | Accuracy: 0.527\n",
            "  Micro  P/R/F1: 0.527 / 0.527 / 0.527\n",
            "  Macro  P/R/F1: 0.382 / 0.488 / 0.402\n",
            "  Weight P/R/F1: 0.476 / 0.527 / 0.494\n",
            "  Top 10 classes by support (label, support, F1):\n",
            "     UNK    3148  F1=0.000\n",
            "      IN    1195  F1=0.725\n",
            "      NN    1153  F1=0.750\n",
            "      JJ     597  F1=0.716\n",
            "      RB     525  F1=0.765\n",
            "       .     407  F1=0.849\n",
            "     VBD     389  F1=0.602\n",
            "      VB     347  F1=0.612\n",
            "      CC     333  F1=0.847\n",
            "     NNS     330  F1=0.778\n",
            "\n",
            "Newcombe (Wilson) 95% CIs for differences in token accuracy (acc_A − acc_B)\n",
            "======================================================================\n",
            "flair      − nltk      : Δ = 0.029, 95% CI = [0.009, 0.048]  (p1=0.559, p2=0.531)\n",
            "flair      − spacy_lg  : Δ = 0.029, 95% CI = [0.009, 0.048]  (p1=0.559, p2=0.530)\n",
            "flair      − spacy_sm  : Δ = 0.032, 95% CI = [0.012, 0.051]  (p1=0.559, p2=0.527)\n",
            "nltk       − spacy_lg  : Δ = 0.000, 95% CI = [-0.019, 0.020]  (p1=0.531, p2=0.530)\n",
            "nltk       − spacy_sm  : Δ = 0.003, 95% CI = [-0.016, 0.023]  (p1=0.531, p2=0.527)\n",
            "spacy_lg   − spacy_sm  : Δ = 0.003, 95% CI = [-0.017, 0.022]  (p1=0.530, p2=0.527)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encodable-only (exclude UNK gold) token accuracy per tool\n",
        "def encodable_accuracy(results, tools):\n",
        "    acc = {}\n",
        "    for tool in tools:\n",
        "        correct = total = 0\n",
        "        for res in results:\n",
        "            tri = res['tool_results'].get(tool, {})\n",
        "            if 'error' in tri:\n",
        "                continue\n",
        "            gt = res['ground_truth']\n",
        "            pred = tri.get('tags', []) or []\n",
        "            toks = tri.get('tokens', []) or []\n",
        "            m = min(len(gt), len(pred), len(toks))\n",
        "            if m == 0:\n",
        "                continue\n",
        "            gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "            for i in range(m):\n",
        "                if gt_penn[i] == 'UNK':\n",
        "                    continue\n",
        "                total += 1\n",
        "                if gt_penn[i] == pred[i]:\n",
        "                    correct += 1\n",
        "        acc[tool] = (correct / total) if total else 0.0\n",
        "    return acc\n",
        "\n",
        "enc_only = encodable_accuracy(expanded_batch_results, ['flair','spacy_lg','spacy_sm','nltk'])\n",
        "for k, v in sorted(enc_only.items()):\n",
        "    print(f\"{k:9s}: {v:.3f}\")\n"
      ],
      "metadata": {
        "id": "QjWMJ83cCV5_",
        "outputId": "1755fcc5-7a1e-4143-a8cc-2ed1439b26df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flair    : 0.816\n",
            "nltk     : 0.775\n",
            "spacy_lg : 0.774\n",
            "spacy_sm : 0.770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Tests under **Strict** CLAWS→Penn Projection\n",
        "\n",
        "#### Wilson 95% Confidence Intervals (token accuracy)\n",
        "*(383 sentences; ~9,994 tokens for spaCy/Flair; ~9,941 for NLTK)*\n",
        "\n",
        "- **Flair**: **0.559**  [0.549, 0.569]  \n",
        "- **NLTK**: **0.531**  [0.521, 0.540]  \n",
        "- **spaCy (lg)**: **0.530**  [0.520, 0.540]  \n",
        "- **spaCy (sm)**: **0.527**  [0.518, 0.537]  \n",
        "\n",
        "_Perfect-sentence rate ≈ 0% for all models (95% CI upper bound ≈ 1%)._\n",
        "\n",
        "#### Two-Proportion Z-Test (best vs comparator)\n",
        "- **Flair vs spaCy (sm)**: Δ = **0.032**, z = **4.487**, p < **0.001**, **significant**  \n",
        "  Cohen’s *h* = **0.063** → *negligible* effect size.\n",
        "\n",
        "#### Pairwise McNemar’s Tests (token-level; Holm–Bonferroni corrected)\n",
        "- **Flair vs NLTK**: χ² = **66.325**, p = **3.823e-16** → **significant**  \n",
        "- **Flair vs spaCy (lg)**: χ² = **113.144**, p = **2.006e-26** → **significant**  \n",
        "- **Flair vs spaCy (sm)**: χ² = **132.300**, p = **1.286e-30** → **significant**  \n",
        "- **NLTK vs spaCy (lg)**: χ² = **0.000**, p = **1.000** → **no difference**  \n",
        "- **NLTK vs spaCy (sm)**: χ² = **0.594**, p = **0.4407** → **no difference**  \n",
        "- **spaCy (lg) vs spaCy (sm)**: χ² = **5.879**, p = **0.0153** → **significant** (small, after correction)\n",
        "\n",
        "_Example contingency (Flair vs spaCy-lg): [[both correct, A-correct/B-wrong], [B-correct/A-wrong, both wrong]] = [[5079, 508], [220, 4186]] (discordant = 728)._\n",
        "\n",
        "#### Pairwise Cluster Bootstrap 95% CIs for Accuracy Differences (accₐ − accᵦ)\n",
        "- **Flair − NLTK**: mean = **+0.028**, CI = **[+0.002, +0.054]** → A>B  \n",
        "- **Flair − spaCy (lg)**: mean = **+0.029**, CI = **[+0.009, +0.052]** → A>B  \n",
        "- **Flair − spaCy (sm)**: mean = **+0.031**, CI = **[+0.012, +0.054]** → A>B  \n",
        "- **NLTK − spaCy (lg)**: mean = **+0.001**, CI = **[−0.026, +0.026]** → no clear diff  \n",
        "- **NLTK − spaCy (sm)**: mean = **+0.003**, CI = **[−0.024, +0.029]** → no clear diff  \n",
        "- **spaCy (lg) − spaCy (sm)**: mean = **+0.003**, CI = **[+0.001, +0.005]** → A>B\n",
        "\n",
        "#### Alignment/coverage (sentences used)\n",
        "- **Flair**: 383  \n",
        "- **NLTK**: 383  \n",
        "- **spaCy (lg)**: 383  \n",
        "- **spaCy (sm)**: 383  \n",
        "\n",
        "**Takeaway:** Flair is **consistently but modestly** better than NLTK and both spaCy variants under strict projection; spaCy-lg and spaCy-sm are nearly indistinguishable. Effects are statistically reliable yet **practically small**, reflecting the structural loss from CLAWS→Penn (`UNK`) rather than large modeling differences.\n"
      ],
      "metadata": {
        "id": "hxJ1HgufD8Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token-level performance under **strict** CLAWS→Penn projection\n",
        "\n",
        "| Tool        | Accuracy | Micro-F1 | Macro-F1 | Weighted-F1 |\n",
        "|-------------|:--------:|:--------:|:--------:|:-----------:|\n",
        "| spaCy (sm)  | 0.527    | 0.527    | 0.402    | 0.494       |\n",
        "| spaCy (lg)  | 0.530    | 0.530    | 0.397    | 0.497       |\n",
        "| NLTK        | 0.531    | 0.531    | 0.396    | 0.494       |\n",
        "| **Flair**   | **0.559**| **0.559**| **0.437**| **0.523**   |\n",
        "\n",
        "*Notes (selected, high-support tags):* Under strict projection, `UNK`—which collapses CLAWS distinctions invisible to Penn—accounts for ≈31% of tokens and has F1 = 0.000 for all tools. Frequent tractable tags achieve materially better F1; e.g., with Flair: `IN` ≈ 0.77, `NN` ≈ 0.81, `JJ` ≈ 0.77, `RB` ≈ 0.78, `NNS` ≈ 0.82, `CC` ≈ 0.90.\n",
        "\n",
        "#### Pairwise accuracy differences (Newcombe Wilson 95% CIs; Δ = Acc\\_A − Acc\\_B)\n",
        "\n",
        "| Comparison          | Δ        | 95% CI            | Interpretation               |\n",
        "|---------------------|---------:|-------------------|------------------------------|\n",
        "| **Flair − NLTK**    | +0.029   | [0.009, 0.048]    | Flair significantly better   |\n",
        "| **Flair − spaCy (lg)** | +0.029| [0.009, 0.048]    | Flair significantly better   |\n",
        "| **Flair − spaCy (sm)** | +0.032| [0.012, 0.051]    | Flair significantly better   |\n",
        "| NLTK − spaCy (lg)   | +0.000   | [−0.019, 0.020]   | No clear difference          |\n",
        "| NLTK − spaCy (sm)   | +0.003   | [−0.016, 0.023]   | No clear difference          |\n",
        "| spaCy (lg) − (sm)   | +0.003   | [−0.017, 0.022]   | No clear difference          |\n",
        "\n",
        "#### Paired significance (McNemar) and robustness (bootstrap)\n",
        "\n",
        "- **McNemar (Holm–Bonferroni):** Disagreements are asymmetric in favor of **Flair** vs each of NLTK, spaCy-lg, and spaCy-sm (all adjusted *p* < 0.05). NLTK vs spaCy models show no asymmetry; spaCy-lg vs spaCy-sm shows a small but significant asymmetry (adjusted *p* ≈ 0.046).\n",
        "- **Cluster bootstrap CIs (sentence-level resampling):** Accuracy gaps of **Flair** over NLTK/spaCy are small but positive (≈ +0.02 to +0.03) with CIs excluding zero; differences among NLTK and spaCy models are not clearly different from zero.\n",
        "\n",
        "#### Interpretation under strict projection\n",
        "\n",
        "The strict CLAWS→Penn protocol intentionally collapses any CLAWS category that Penn cannot encode into `UNK`. The resulting `UNK` mass therefore quantifies **representational loss in Penn**, not model error per se. Within this constrained regime, **Flair** exhibits a consistent but **small** advantage (≈3 percentage points). Differences among NLTK and spaCy models are negligible. The central finding is thus **structural**: performance ceilings are dominated by the tagset mismatch—evidence that CLAWS captures fine-grained morphosyntactic distinctions that Penn-based evaluation cannot reward.\n"
      ],
      "metadata": {
        "id": "ouTJTd8JHMQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# VISUALIZATIONS — per dataset (Dubliners, BNC) + pooled\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# --- helper: strict, sentence-level accuracy for a tool on one sentence ---\n",
        "def _strict_sentence_acc(gt_claws, pred_tags, pred_tokens):\n",
        "    m = min(len(gt_claws), len(pred_tags), len(pred_tokens))\n",
        "    if m == 0:\n",
        "        return None\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt_claws[:m]]\n",
        "    correct = sum(1 for i in range(m) if gt_penn[i] == pred_tags[i])\n",
        "    return correct / m\n",
        "\n",
        "# --- build per-dataset stats from expanded_batch_results ---\n",
        "datasets = sorted({rec.get('dataset', 'Unknown') for rec in expanded_batch_results})\n",
        "tools_seen = sorted({t for rec in expanded_batch_results for t in rec.get('tool_results', {}).keys()})\n",
        "\n",
        "stats = {ds: {tool: {'accs': [], 'total_sentences': 0, 'perfect_sentences': 0}\n",
        "              for tool in tools_seen}\n",
        "         for ds in datasets}\n",
        "\n",
        "for rec in expanded_batch_results:\n",
        "    ds = rec.get('dataset', 'Unknown')\n",
        "    gt = rec.get('ground_truth', [])\n",
        "    tri_map = rec.get('tool_results', {})\n",
        "    for tool, tri in tri_map.items():\n",
        "        if not isinstance(tri, dict) or ('error' in tri):\n",
        "            continue\n",
        "        pred = tri.get('tags') or []\n",
        "        toks = tri.get('tokens') or []\n",
        "        acc = _strict_sentence_acc(gt, pred, toks)\n",
        "        if acc is None:\n",
        "            continue\n",
        "        s = stats[ds][tool]\n",
        "        s['accs'].append(acc)\n",
        "        s['total_sentences'] += 1\n",
        "        if acc == 1.0:\n",
        "            s['perfect_sentences'] += 1\n",
        "\n",
        "# finalize (replace accs list with summary numbers) and also compute pooled (\"All\")\n",
        "pooled = {tool: {'accs': [], 'total_sentences': 0, 'perfect_sentences': 0} for tool in tools_seen}\n",
        "for ds in datasets:\n",
        "    for tool in tools_seen:\n",
        "        d = stats[ds][tool]\n",
        "        accs = d['accs']\n",
        "        stats[ds][tool] = {\n",
        "            'mean_accuracy': float(np.mean(accs)) if accs else 0.0,\n",
        "            'std_accuracy': float(np.std(accs)) if accs else 0.0,\n",
        "            'total_sentences': int(d['total_sentences']),\n",
        "            'perfect_sentences': int(d['perfect_sentences'])\n",
        "        }\n",
        "        pooled[tool]['accs'].extend(accs)\n",
        "        pooled[tool]['total_sentences'] += d['total_sentences']\n",
        "        pooled[tool]['perfect_sentences'] += d['perfect_sentences']\n",
        "\n",
        "stats['All (pooled)'] = {}\n",
        "for tool in tools_seen:\n",
        "    p = pooled[tool]\n",
        "    accs = p['accs']\n",
        "    stats['All (pooled)'][tool] = {\n",
        "        'mean_accuracy': float(np.mean(accs)) if accs else 0.0,\n",
        "        'std_accuracy': float(np.std(accs)) if accs else 0.0,\n",
        "        'total_sentences': int(p['total_sentences']),\n",
        "        'perfect_sentences': int(p['perfect_sentences'])\n",
        "    }\n",
        "\n",
        "# --- Figure 1: grouped bar of mean sentence accuracy per tool, by dataset ---\n",
        "ordered_datasets = [ds for ds in ['Dubliners', 'BNC', 'All (pooled)'] if ds in stats]\n",
        "tools = tools_seen\n",
        "\n",
        "fig_acc = go.Figure()\n",
        "for ds in ordered_datasets:\n",
        "    y_vals = [stats[ds].get(t, {}).get('mean_accuracy', 0.0) for t in tools]\n",
        "    y_errs = [stats[ds].get(t, {}).get('std_accuracy', 0.0) for t in tools]\n",
        "    fig_acc.add_trace(go.Bar(\n",
        "        name=ds,\n",
        "        x=tools,\n",
        "        y=y_vals,\n",
        "        error_y=dict(type='data', array=y_errs),\n",
        "        text=[f\"{v:.3f}\" for v in y_vals],\n",
        "        textposition='auto'\n",
        "    ))\n",
        "\n",
        "fig_acc.update_layout(\n",
        "    barmode='group',\n",
        "    title=\"Mean sentence accuracy (STRICT) by tool and dataset\",\n",
        "    xaxis_title=\"Tool\",\n",
        "    yaxis_title=\"Mean sentence accuracy\",\n",
        "    yaxis=dict(range=[0, 1])\n",
        ")\n",
        "fig_acc.show()\n",
        "\n",
        "# --- Conditionally render perfect-sentence plot only if any rate > 0 ---\n",
        "any_perfect = False\n",
        "rates_by_ds = {}\n",
        "for ds in ordered_datasets:\n",
        "    ds_rates = []\n",
        "    for t in tools:\n",
        "        d = stats[ds].get(t, {})\n",
        "        tot = d.get('total_sentences', 0)\n",
        "        perf = d.get('perfect_sentences', 0)\n",
        "        rate = (perf / tot) if tot else 0.0\n",
        "        ds_rates.append(rate)\n",
        "        if rate > 0:\n",
        "            any_perfect = True\n",
        "    rates_by_ds[ds] = ds_rates\n",
        "\n",
        "if any_perfect:\n",
        "    fig_perf = go.Figure()\n",
        "    for ds in ordered_datasets:\n",
        "        fig_perf.add_trace(go.Bar(\n",
        "            name=ds,\n",
        "            x=tools,\n",
        "            y=rates_by_ds[ds],\n",
        "            text=[f\"{r:.1%}\" for r in rates_by_ds[ds]],\n",
        "            textposition='auto'\n",
        "        ))\n",
        "    fig_perf.update_layout(\n",
        "        barmode='group',\n",
        "        title=\"Perfect-sentence tagging rate (STRICT) by tool and dataset\",\n",
        "        xaxis_title=\"Tool\",\n",
        "        yaxis_title=\"Perfect sentences\",\n",
        "        yaxis=dict(range=[0, 1])\n",
        "    )\n",
        "    fig_perf.show()\n",
        "else:\n",
        "    print(\"Perfect-sentence tagging rate is 0% for all tools/datasets under STRICT; plot omitted.\")\n",
        "\n",
        "print(\"Visualizations complete.\")\n"
      ],
      "metadata": {
        "id": "mi70I58hI1el",
        "outputId": "cf5db3da-0975-417f-a883-9886840585ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"19f72a9f-c0dd-441c-8e59-98e4096a8a38\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"19f72a9f-c0dd-441c-8e59-98e4096a8a38\")) {                    Plotly.newPlot(                        \"19f72a9f-c0dd-441c-8e59-98e4096a8a38\",                        [{\"error_y\":{\"array\":[0.18674301307661612,0.16320169047415198,0.19883203262310312,0.19911789472318323],\"type\":\"data\"},\"name\":\"Dubliners\",\"text\":[\"0.560\",\"0.558\",\"0.535\",\"0.533\"],\"textposition\":\"auto\",\"x\":[\"flair\",\"nltk\",\"spacy_lg\",\"spacy_sm\"],\"y\":[0.5596202200071031,0.5582264617293192,0.5353480095760559,0.5328066337927893],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.15677084735069702,0.1301017564262949,0.1674788138394109,0.1702072068692877],\"type\":\"data\"},\"name\":\"BNC\",\"text\":[\"0.583\",\"0.576\",\"0.569\",\"0.567\"],\"textposition\":\"auto\",\"x\":[\"flair\",\"nltk\",\"spacy_lg\",\"spacy_sm\"],\"y\":[0.5831631892027838,0.5761702473448012,0.5689547155068868,0.5671564778869985],\"type\":\"bar\"},{\"error_y\":{\"array\":[0.17214767652345775,0.14712412028051014,0.18389826525680772,0.18538242819956377],\"type\":\"data\"},\"name\":\"All (pooled)\",\"text\":[\"0.572\",\"0.568\",\"0.553\",\"0.551\"],\"textposition\":\"auto\",\"x\":[\"flair\",\"nltk\",\"spacy_lg\",\"spacy_sm\"],\"y\":[0.5719141986993644,0.5675965847661244,0.5528972032736177,0.5507438891944652],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis\":{\"title\":{\"text\":\"Mean sentence accuracy\"},\"range\":[0,1]},\"barmode\":\"group\",\"title\":{\"text\":\"Mean sentence accuracy (STRICT) by tool and dataset\"},\"xaxis\":{\"title\":{\"text\":\"Tool\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('19f72a9f-c0dd-441c-8e59-98e4096a8a38');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perfect-sentence tagging rate is 0% for all tools/datasets under STRICT; plot omitted.\n",
            "Visualizations complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ADDITIONAL DATA VISUALIZATIONS (STRICT, pooled Dubliners + BNC)\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# --- ensure wilson_results exists (fallback compute if missing) ----------------\n",
        "def _wilson_interval(successes, n, confidence=0.95):\n",
        "    if n == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    from math import sqrt\n",
        "    from scipy.stats import norm\n",
        "    z = norm.ppf(1 - (1 - confidence) / 2)\n",
        "    p = successes / n\n",
        "    denom = 1 + z*z/n\n",
        "    centre = (p + z*z/(2*n)) / denom\n",
        "    half = z * np.sqrt((p*(1-p) + z*z/(4*n))/n) / denom\n",
        "    return p, max(0.0, centre - half), min(1.0, centre + half)\n",
        "\n",
        "if 'wilson_results' not in globals() or not wilson_results:\n",
        "    # build from expanded_batch_results\n",
        "    wilson_results = {}\n",
        "    discovered_tools = sorted({t for r in expanded_batch_results for t in r.get('tool_results', {})})\n",
        "    for tool_name in discovered_tools:\n",
        "        total_tokens = 0\n",
        "        correct_tokens = 0\n",
        "        total_sentences = 0\n",
        "        perfect_sentences = 0\n",
        "        for res in expanded_batch_results:\n",
        "            tri = res['tool_results'].get(tool_name, {})\n",
        "            if not isinstance(tri, dict) or ('error' in tri):\n",
        "                continue\n",
        "            gt = res['ground_truth']\n",
        "            pred = tri.get('tags', [])\n",
        "            toks = tri.get('tokens', [])\n",
        "            m = min(len(gt), len(pred), len(toks))\n",
        "            if m == 0:\n",
        "                continue\n",
        "            gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "            corr = sum(1 for i in range(m) if gt_penn[i] == pred[i])\n",
        "            correct_tokens += corr\n",
        "            total_tokens += m\n",
        "            total_sentences += 1\n",
        "            if corr == m:\n",
        "                perfect_sentences += 1\n",
        "        if total_tokens > 0:\n",
        "            p, lo, hi = _wilson_interval(correct_tokens, total_tokens, 0.95)\n",
        "            pr, pr_lo, pr_hi = _wilson_interval(perfect_sentences, total_sentences, 0.95)\n",
        "            wilson_results[tool_name] = {\n",
        "                'token_accuracy': p,\n",
        "                'token_ci_lower': lo,\n",
        "                'token_ci_upper': hi,\n",
        "                'perfect_rate': pr,\n",
        "                'perfect_ci_lower': pr_lo,\n",
        "                'perfect_ci_upper': pr_hi,\n",
        "                'total_tokens': total_tokens,\n",
        "                'correct_tokens': correct_tokens,\n",
        "                'total_sentences': total_sentences,\n",
        "                'perfect_sentences': perfect_sentences\n",
        "            }\n",
        "\n",
        "# --- 1) Wilson CI comparison plot ---------------------------------------------\n",
        "tools_order = sorted(wilson_results.keys())\n",
        "ci_points = [(t, wilson_results[t]['token_accuracy'],\n",
        "              wilson_results[t]['token_ci_lower'],\n",
        "              wilson_results[t]['token_ci_upper']) for t in tools_order]\n",
        "\n",
        "x_min = min(lo for _, _, lo, _ in ci_points) if ci_points else 0.0\n",
        "x_max = max(hi for _, _, _, hi in ci_points) if ci_points else 1.0\n",
        "pad = max(0.01, (x_max - x_min) * 0.1)\n",
        "x_range = [max(0.0, x_min - pad), min(1.0, x_max + pad)]\n",
        "\n",
        "fig_ci = go.Figure()\n",
        "for tool, p, lo, hi in ci_points:\n",
        "    # point\n",
        "    fig_ci.add_trace(go.Scatter(\n",
        "        x=[p], y=[tool], mode='markers',\n",
        "        marker=dict(size=12),\n",
        "        name=tool, showlegend=False,\n",
        "        hovertemplate=f\"{tool}<br>Accuracy={p:.3%}<br>95% CI=[{lo:.3%}, {hi:.3%}]<extra></extra>\"\n",
        "    ))\n",
        "    # CI segment\n",
        "    fig_ci.add_trace(go.Scatter(\n",
        "        x=[lo, hi], y=[tool, tool], mode='lines',\n",
        "        line=dict(width=4), showlegend=False\n",
        "    ))\n",
        "\n",
        "fig_ci.update_layout(\n",
        "    title=\"Token Accuracy with 95% Wilson CIs (STRICT)<br><sub>Dubliners + BNC, pooled</sub>\",\n",
        "    xaxis_title=\"Token-level accuracy\",\n",
        "    yaxis_title=\"Tool\",\n",
        "    xaxis=dict(range=x_range, tickformat='.0%'),\n",
        "    height=420\n",
        ")\n",
        "fig_ci.show()\n",
        "\n",
        "# --- helper: strict sentence accuracy -----------------------------------------\n",
        "def _strict_sentence_acc(gt_claws, pred_tags, pred_tokens):\n",
        "    m = min(len(gt_claws), len(pred_tags), len(pred_tokens))\n",
        "    if m <= 0:\n",
        "        return None\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt_claws[:m]]\n",
        "    return sum(1 for i in range(m) if gt_penn[i] == pred_tags[i]) / m\n",
        "\n",
        "# --- 2) Error heatmap: most problematic CLAWS tags (by strict mismatch) -------\n",
        "# We compare strict-projected gold vs predicted Penn, but attribute errors to the\n",
        "# original CLAWS source tag (optionally collapsed via _strip_ditto if desired).\n",
        "strip_ditto = True  # set False if you want the raw CLAWS variants\n",
        "\n",
        "tag_errors = defaultdict(lambda: defaultdict(int))  # CLAWS -> tool -> count\n",
        "tools_for_heatmap = tools_order  # same tools\n",
        "\n",
        "for res in expanded_batch_results:\n",
        "    gt_claws = res.get('ground_truth', [])\n",
        "    # precompute strict-projected gold\n",
        "    gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt_claws]\n",
        "    for tool in tools_for_heatmap:\n",
        "        tri = res.get('tool_results', {}).get(tool, {})\n",
        "        if not isinstance(tri, dict) or ('error' in tri):\n",
        "            continue\n",
        "        pred = tri.get('tags', []) or []\n",
        "        toks = tri.get('tokens', []) or []\n",
        "        m = min(len(gt_penn), len(pred), len(toks), len(gt_claws))\n",
        "        if m == 0:\n",
        "            continue\n",
        "        for i in range(m):\n",
        "            if gt_penn[i] != pred[i]:\n",
        "                claws_src = _strip_ditto(gt_claws[i]) if strip_ditto else gt_claws[i]\n",
        "                tag_errors[claws_src][tool] += 1\n",
        "\n",
        "# pick top 12 tags by total error across tools\n",
        "tag_totals = {tag: sum(cnts.values()) for tag, cnts in tag_errors.items()}\n",
        "top_tags = [t for t, _ in sorted(tag_totals.items(), key=lambda x: x[1], reverse=True)[:12]]\n",
        "\n",
        "heatmap_data = []\n",
        "for tag in top_tags:\n",
        "    row = [tag_errors[tag].get(tool, 0) for tool in tools_for_heatmap]\n",
        "    heatmap_data.append(row)\n",
        "\n",
        "fig_heatmap = go.Figure(data=go.Heatmap(\n",
        "    z=heatmap_data,\n",
        "    x=tools_for_heatmap,\n",
        "    y=top_tags,\n",
        "    colorscale='Reds',\n",
        "    text=heatmap_data,\n",
        "    texttemplate=\"%{text}\",\n",
        "    textfont={\"size\": 10},\n",
        "    hovertemplate=\"Tag=%{y}<br>Tool=%{x}<br>Errors=%{z}<extra></extra>\"\n",
        "))\n",
        "fig_heatmap.update_layout(\n",
        "    title=\"Most Problematic CLAWS Tags (STRICT mismatches attributed to CLAWS source)\",\n",
        "    xaxis_title=\"Tool\",\n",
        "    yaxis_title=\"CLAWS tag\",\n",
        "    height=520\n",
        ")\n",
        "fig_heatmap.show()\n",
        "\n",
        "# --- 3) Sentence length vs accuracy (STRICT), colored by tool -----------------\n",
        "points = []\n",
        "for res in expanded_batch_results:\n",
        "    ds = res.get('dataset', 'Unknown')\n",
        "    gt = res.get('ground_truth', [])\n",
        "    sent_len = len(gt)\n",
        "    for tool in tools_order:\n",
        "        tri = res.get('tool_results', {}).get(tool, {})\n",
        "        if not isinstance(tri, dict) or ('error' in tri):\n",
        "            continue\n",
        "        pred = tri.get('tags', []) or []\n",
        "        toks = tri.get('tokens', []) or []\n",
        "        acc = _strict_sentence_acc(gt, pred, toks)\n",
        "        if acc is not None:\n",
        "            points.append((sent_len, acc, tool, ds))\n",
        "\n",
        "import pandas as pd\n",
        "df_scatter = pd.DataFrame(points, columns=['length', 'accuracy', 'tool', 'dataset'])\n",
        "\n",
        "fig_scatter = go.Figure()\n",
        "palette = {\n",
        "    'flair': '#2ca02c',\n",
        "    'spacy_lg': '#ff7f0e',\n",
        "    'spacy_sm': '#1f77b4',\n",
        "    'nltk': '#9467bd'\n",
        "}\n",
        "for tool in sorted(df_scatter['tool'].unique()):\n",
        "    sub = df_scatter[df_scatter['tool'] == tool]\n",
        "    fig_scatter.add_trace(go.Scatter(\n",
        "        x=sub['length'], y=sub['accuracy'],\n",
        "        mode='markers', name=tool,\n",
        "        marker=dict(size=6, opacity=0.6, color=palette.get(tool, '#636EFA')),\n",
        "        hovertemplate=(\"Tool=\"+tool+\"<br>Len=%{x}<br>Acc=%{y:.1%}\"\n",
        "                       \"<br>Dataset=%{text}<extra></extra>\"),\n",
        "        text=sub['dataset']\n",
        "    ))\n",
        "\n",
        "# Trend line (overall)\n",
        "if not df_scatter.empty:\n",
        "    z = np.polyfit(df_scatter['length'], df_scatter['accuracy'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    x_trend = np.linspace(df_scatter['length'].min(), df_scatter['length'].max(), 100)\n",
        "    fig_scatter.add_trace(go.Scatter(\n",
        "        x=x_trend, y=p(x_trend), mode='lines', name='Trend',\n",
        "        line=dict(color='red', dash='dash')\n",
        "    ))\n",
        "\n",
        "fig_scatter.update_layout(\n",
        "    title=\"Sentence Length vs Token Accuracy (STRICT)<br><sub>Dubliners + BNC</sub>\",\n",
        "    xaxis_title=\"Sentence length (tokens in CLAWS gold)\",\n",
        "    yaxis_title=\"Token accuracy\",\n",
        "    yaxis=dict(range=[0, 1], tickformat='.0%'),\n",
        "    height=520\n",
        ")\n",
        "fig_scatter.show()\n",
        "\n",
        "# --- 4) Distribution of per-sentence accuracies (STRICT) ----------------------\n",
        "tool_performance = defaultdict(list)\n",
        "for res in expanded_batch_results:\n",
        "    gt = res.get('ground_truth', [])\n",
        "    tri_map = res.get('tool_results', {})\n",
        "    for tool in tools_order:\n",
        "        tri = tri_map.get(tool, {})\n",
        "        if not isinstance(tri, dict) or ('error' in tri):\n",
        "            continue\n",
        "        pred = tri.get('tags', []) or []\n",
        "        toks = tri.get('tokens', []) or []\n",
        "        acc = _strict_sentence_acc(gt, pred, toks)\n",
        "        if acc is not None:\n",
        "            tool_performance[tool].append(acc)\n",
        "\n",
        "fig_dist = go.Figure()\n",
        "for tool in tools_order:\n",
        "    accs = tool_performance.get(tool, [])\n",
        "    if not accs:\n",
        "        continue\n",
        "    fig_dist.add_trace(go.Box(\n",
        "        y=accs, name=tool, boxpoints='outliers',\n",
        "        hovertemplate=f\"{tool}<br>Q1–Q3 / median shown<extra></extra>\"\n",
        "    ))\n",
        "fig_dist.update_layout(\n",
        "    title=\"Distribution of Sentence-Level Accuracies (STRICT)\",\n",
        "    xaxis_title=\"Tool\",\n",
        "    yaxis_title=\"Sentence accuracy\",\n",
        "    yaxis=dict(range=[0, 1], tickformat='.0%'),\n",
        "    height=520\n",
        ")\n",
        "fig_dist.show()\n",
        "\n",
        "print(\"Enhanced visualizations complete!\")\n"
      ],
      "metadata": {
        "id": "YO2ne_dwK-cE",
        "outputId": "8501c4ad-5681-43f1-d26b-759734bc4820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"e67aa7de-1d48-4a21-9489-deee889814d9\" class=\"plotly-graph-div\" style=\"height:420px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e67aa7de-1d48-4a21-9489-deee889814d9\")) {                    Plotly.newPlot(                        \"e67aa7de-1d48-4a21-9489-deee889814d9\",                        [{\"hovertemplate\":\"flair\\u003cbr\\u003eAccuracy=55.904%\\u003cbr\\u003e95% CI=[54.928%, 56.875%]\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"size\":12},\"mode\":\"markers\",\"name\":\"flair\",\"showlegend\":false,\"x\":[0.5590354212527516],\"y\":[\"flair\"],\"type\":\"scatter\"},{\"line\":{\"width\":4},\"mode\":\"lines\",\"showlegend\":false,\"x\":[0.549280389380944,0.5687450869041327],\"y\":[\"flair\",\"flair\"],\"type\":\"scatter\"},{\"hovertemplate\":\"nltk\\u003cbr\\u003eAccuracy=53.053%\\u003cbr\\u003e95% CI=[52.071%, 54.033%]\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"size\":12},\"mode\":\"markers\",\"name\":\"nltk\",\"showlegend\":false,\"x\":[0.5305301277537471],\"y\":[\"nltk\"],\"type\":\"scatter\"},{\"line\":{\"width\":4},\"mode\":\"lines\",\"showlegend\":false,\"x\":[0.5207097044590667,0.5403269649049898],\"y\":[\"nltk\",\"nltk\"],\"type\":\"scatter\"},{\"hovertemplate\":\"spacy_lg\\u003cbr\\u003eAccuracy=53.022%\\u003cbr\\u003e95% CI=[52.042%, 53.999%]\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"size\":12},\"mode\":\"markers\",\"name\":\"spacy_lg\",\"showlegend\":false,\"x\":[0.5302181308785271],\"y\":[\"spacy_lg\"],\"type\":\"scatter\"},{\"line\":{\"width\":4},\"mode\":\"lines\",\"showlegend\":false,\"x\":[0.5204235509838586,0.539989489419689],\"y\":[\"spacy_lg\",\"spacy_lg\"],\"type\":\"scatter\"},{\"hovertemplate\":\"spacy_sm\\u003cbr\\u003eAccuracy=52.742%\\u003cbr\\u003e95% CI=[51.762%, 53.719%]\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"size\":12},\"mode\":\"markers\",\"name\":\"spacy_sm\",\"showlegend\":false,\"x\":[0.527416449869922],\"y\":[\"spacy_sm\"],\"type\":\"scatter\"},{\"line\":{\"width\":4},\"mode\":\"lines\",\"showlegend\":false,\"x\":[0.5176197772312601,0.5371920541282502],\"y\":[\"spacy_sm\",\"spacy_sm\"],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"title\":{\"text\":\"Token-level accuracy\"},\"range\":[0.5076197772312601,0.5787450869041327],\"tickformat\":\".0%\"},\"title\":{\"text\":\"Token Accuracy with 95% Wilson CIs (STRICT)\\u003cbr\\u003e\\u003csub\\u003eDubliners + BNC, pooled\\u003c\\u002fsub\\u003e\"},\"yaxis\":{\"title\":{\"text\":\"Tool\"}},\"height\":420},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e67aa7de-1d48-4a21-9489-deee889814d9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f21152f0-9fd9-4764-b621-975f865670a4\" class=\"plotly-graph-div\" style=\"height:520px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f21152f0-9fd9-4764-b621-975f865670a4\")) {                    Plotly.newPlot(                        \"f21152f0-9fd9-4764-b621-975f865670a4\",                        [{\"colorscale\":[[0.0,\"rgb(255,245,240)\"],[0.125,\"rgb(254,224,210)\"],[0.25,\"rgb(252,187,161)\"],[0.375,\"rgb(252,146,114)\"],[0.5,\"rgb(251,106,74)\"],[0.625,\"rgb(239,59,44)\"],[0.75,\"rgb(203,24,29)\"],[0.875,\"rgb(165,15,21)\"],[1.0,\"rgb(103,0,13)\"]],\"hovertemplate\":\"Tag=%{y}\\u003cbr\\u003eTool=%{x}\\u003cbr\\u003eErrors=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"text\":[[472,472,472,472],[301,298,301,301],[297,297,297,297],[290,289,290,290],[187,202,245,253],[213,213,213,213],[139,184,168,179],[147,147,147,147],[80,175,121,122],[121,121,121,121],[107,107,107,107],[105,105,105,105]],\"textfont\":{\"size\":10},\"texttemplate\":\"%{text}\",\"x\":[\"flair\",\"nltk\",\"spacy_lg\",\"spacy_sm\"],\"y\":[\"AT\",\"AT1\",\"PPHS1\",\"APPGE\",\"NN1\",\"IO\",\"JJ\",\"VBDZ\",\"II\",\"PPH1\",\"VHD\",\"PPIS1\"],\"z\":[[472,472,472,472],[301,298,301,301],[297,297,297,297],[290,289,290,290],[187,202,245,253],[213,213,213,213],[139,184,168,179],[147,147,147,147],[80,175,121,122],[121,121,121,121],[107,107,107,107],[105,105,105,105]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Most Problematic CLAWS Tags (STRICT mismatches attributed to CLAWS source)\"},\"xaxis\":{\"title\":{\"text\":\"Tool\"}},\"yaxis\":{\"title\":{\"text\":\"CLAWS tag\"}},\"height\":520},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f21152f0-9fd9-4764-b621-975f865670a4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"a988eab7-9e65-4314-a13e-41d34674c383\" class=\"plotly-graph-div\" style=\"height:520px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a988eab7-9e65-4314-a13e-41d34674c383\")) {                    Plotly.newPlot(                        \"a988eab7-9e65-4314-a13e-41d34674c383\",                        [{\"hovertemplate\":\"Tool=flair\\u003cbr\\u003eLen=%{x}\\u003cbr\\u003eAcc=%{y:.1%}\\u003cbr\\u003eDataset=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"#2ca02c\",\"opacity\":0.6,\"size\":6},\"mode\":\"markers\",\"name\":\"flair\",\"text\":[\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\"],\"x\":[15,81,21,20,23,15,29,40,78,34,25,61,27,29,16,28,8,6,30,14,89,14,12,31,28,104,22,80,9,28,20,14,23,25,27,14,19,11,52,15,20,40,27,18,83,36,39,27,21,29,28,14,37,8,36,36,36,15,9,16,10,88,20,49,18,47,29,21,21,14,10,8,11,18,23,8,14,18,19,25,14,33,25,27,21,38,53,41,45,64,40,36,33,11,30,9,6,22,31,30,24,22,11,12,11,33,33,15,18,43,26,27,47,21,36,32,24,30,87,23,15,40,12,15,13,15,23,11,20,10,20,8,58,18,63,34,56,33,39,14,27,28,43,28,48,22,10,15,11,4,36,191,45,17,32,20,33,155,51,27,23,24,16,8,17,24,29,34,15,13,30,29,19,21,24,21,32,40,13,23,100,21,34,14,19,5,12,29,16,12,6,9,29,33,27,12,7,31,14,22,39,7,47,10,14,20,18,23,11,37,19,18,24,24,9,23,19,31,21,19,34,19,13,13,8,14,19,21,21,11,11,19,11,21,20,10,38,13,11,36,24,15,9,10,15,73,27,21,9,10,19,18,10,16,43,18,33,19,12,13,26,14,30,24,23,40,29,20,24,16,29,10,24,14,40,5,42,30,24,16,17,34,35,41,18,20,18,15,18,21,24,19,25,26,26,17,29,19,31,30,25,24,18,31,20,20,22,8,33,20,26,24,28,32,27,34,26,52,27,19,30,36,9,28,7,16,27,10,17,13,12,12,24,52,42,18,20,15,25,45,19,26,32,34,19,22,11,17,26,24,20,52,42,15,9,30,29,16,48,20,20,30,5,8,17,17,18,37,32,24,34,21,48,47,26,27,23,29,57,12,44,12,26],\"y\":[0.4,0.6296296296296297,0.7142857142857143,0.65,0.6521739130434783,0.4666666666666667,0.6551724137931034,0.6,0.5641025641025641,0.4117647058823529,0.64,0.5409836065573771,0.5185185185185185,0.5862068965517241,0.6875,0.6428571428571429,0.875,0.6666666666666666,0.5666666666666667,0.5714285714285714,0.5842696629213483,0.7142857142857143,0.5833333333333334,0.6774193548387096,0.7142857142857143,0.6057692307692307,0.7272727272727273,0.625,0.7777777777777778,0.6071428571428571,0.65,0.6428571428571429,0.5652173913043478,0.72,0.6666666666666666,0.6428571428571429,0.7368421052631579,0.5454545454545454,0.5576923076923077,0.7333333333333333,0.7,0.65,0.6296296296296297,0.4444444444444444,0.6265060240963856,0.6388888888888888,0.5641025641025641,0.6666666666666666,0.7142857142857143,0.6551724137931034,0.6071428571428571,0.5714285714285714,0.7567567567567568,0.625,0.5277777777777778,0.5833333333333334,0.5555555555555556,0.6666666666666666,0.6666666666666666,0.75,0.4,0.5909090909090909,0.65,0.6326530612244898,0.5555555555555556,0.6382978723404256,0.6551724137931034,0.5238095238095238,0.14285714285714285,0.14285714285714285,0.7,0.625,0.09090909090909091,0.6111111111111112,0.043478260869565216,0.75,0.7142857142857143,0.7222222222222222,0.6842105263157895,0.2,0.6428571428571429,0.6060606060606061,0.72,0.5555555555555556,0.14285714285714285,0.18421052631578946,0.6981132075471698,0.12195121951219512,0.6,0.578125,0.7,0.19444444444444445,0.5757575757575758,0.45454545454545453,0.7,0.0,0.8333333333333334,0.7272727272727273,0.06451612903225806,0.5333333333333333,0.75,0.22727272727272727,0.5454545454545454,0.75,0.6363636363636364,0.5454545454545454,0.6666666666666666,0.7333333333333333,0.5555555555555556,0.5813953488372093,0.8076923076923077,0.5925925925925926,0.3617021276595745,0.5714285714285714,0.4166666666666667,0.65625,0.625,0.1,0.6436781609195402,0.782608695652174,0.4666666666666667,0.525,0.6666666666666666,0.13333333333333333,0.5384615384615384,0.6666666666666666,0.6521739130434783,0.0,0.7,0.4,0.75,0.5,0.15517241379310345,0.7777777777777778,0.5079365079365079,0.20588235294117646,0.625,0.696969696969697,0.15384615384615385,0.7142857142857143,0.5925925925925926,0.6785714285714286,0.09302325581395349,0.75,0.6875,0.6363636363636364,0.2,0.26666666666666666,0.5454545454545454,0.5,0.6944444444444444,0.13612565445026178,0.6888888888888889,0.47058823529411764,0.65625,0.25,0.48484848484848486,0.6387096774193548,0.6862745098039216,0.6666666666666666,0.5652173913043478,0.75,0.5625,0.5,0.5882352941176471,0.625,0.27586206896551724,0.14705882352941177,0.5333333333333333,0.6923076923076923,0.6333333333333333,0.5517241379310345,0.5789473684210527,0.5238095238095238,0.7083333333333334,0.5714285714285714,0.5625,0.75,0.6153846153846154,0.6086956521739131,0.64,0.6190476190476191,0.6764705882352942,0.7142857142857143,0.5789473684210527,0.4,0.75,0.4482758620689655,0.8125,0.5,0.8,0.6666666666666666,0.5862068965517241,0.6363636363636364,0.48148148148148145,0.75,0.42857142857142855,0.7096774193548387,0.5,0.6363636363636364,0.6052631578947368,0.42857142857142855,0.6382978723404256,0.6,0.6428571428571429,0.45,0.7222222222222222,0.43478260869565216,0.5454545454545454,0.4864864864864865,0.7368421052631579,0.5555555555555556,0.6666666666666666,0.625,0.6666666666666666,0.6521739130434783,0.6842105263157895,0.6129032258064516,0.5238095238095238,0.631578947368421,0.5294117647058824,0.631578947368421,0.6153846153846154,0.5384615384615384,0.375,0.6428571428571429,0.7368421052631579,0.6190476190476191,0.5714285714285714,0.45454545454545453,0.6363636363636364,0.5263157894736842,0.6363636363636364,0.7142857142857143,0.7,0.6,0.34210526315789475,0.6153846153846154,0.6363636363636364,0.2777777777777778,0.5833333333333334,0.7333333333333333,0.5555555555555556,0.7,0.5333333333333333,0.6575342465753424,0.2962962962962963,0.6190476190476191,0.7777777777777778,0.6,0.631578947368421,0.7777777777777778,0.8,0.6875,0.7209302325581395,0.0,0.5454545454545454,0.47368421052631576,0.6363636363636364,0.7692307692307693,0.6538461538461539,0.7142857142857143,0.5,0.6666666666666666,0.5652173913043478,0.725,0.6428571428571429,0.65,0.75,0.625,0.4827586206896552,0.7,0.5416666666666666,0.7142857142857143,0.65,0.8,0.6666666666666666,0.4666666666666667,0.5416666666666666,0.625,0.5882352941176471,0.5882352941176471,0.5714285714285714,0.36585365853658536,0.2777777777777778,0.6,0.6111111111111112,0.6666666666666666,0.6666666666666666,0.7142857142857143,0.625,0.5789473684210527,0.64,0.6923076923076923,0.6153846153846154,0.5882352941176471,0.5517241379310345,0.631578947368421,0.6774193548387096,0.5666666666666667,0.6,0.5833333333333334,0.5555555555555556,0.25806451612903225,0.6,0.7,0.6818181818181818,0.625,0.36363636363636365,0.7,0.5,0.625,0.5714285714285714,0.59375,0.5925925925925926,0.7058823529411765,0.6923076923076923,0.5,0.7037037037037037,0.7894736842105263,0.4,0.5555555555555556,0.6666666666666666,0.6296296296296297,0.8571428571428571,0.625,0.037037037037037035,0.7,0.7058823529411765,0.5833333333333334,0.8333333333333334,0.75,0.125,0.5,0.6428571428571429,0.6666666666666666,0.1,0.7333333333333333,0.5,0.6590909090909091,0.631578947368421,0.5769230769230769,0.46875,0.7058823529411765,0.3157894736842105,0.45454545454545453,0.5454545454545454,0.7647058823529411,0.2692307692307692,0.75,0.7,0.6078431372549019,0.6341463414634146,0.7333333333333333,0.7777777777777778,0.6333333333333333,0.5862068965517241,0.5625,0.6041666666666666,0.7,0.45,0.26666666666666666,0.8,0.0,0.5882352941176471,0.47058823529411764,0.7777777777777778,0.4594594594594595,0.75,0.125,0.6764705882352942,0.14285714285714285,0.20833333333333334,0.23404255319148937,0.38461538461538464,0.5925925925925926,0.5217391304347826,0.6551724137931034,0.6607142857142857,0.4166666666666667,0.5909090909090909,0.6666666666666666,0.6538461538461539],\"type\":\"scatter\"},{\"hovertemplate\":\"Tool=nltk\\u003cbr\\u003eLen=%{x}\\u003cbr\\u003eAcc=%{y:.1%}\\u003cbr\\u003eDataset=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"#9467bd\",\"opacity\":0.6,\"size\":6},\"mode\":\"markers\",\"name\":\"nltk\",\"text\":[\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\"],\"x\":[15,81,21,20,23,15,29,40,78,34,25,61,27,29,16,28,8,6,30,14,89,14,12,31,28,104,22,80,9,28,20,14,23,25,27,14,19,11,52,15,20,40,27,18,83,36,39,27,21,29,28,14,37,8,36,36,36,15,9,16,10,88,20,49,18,47,29,21,21,14,10,8,11,18,23,8,14,18,19,25,14,33,25,27,21,38,53,41,45,64,40,36,33,11,30,9,6,22,31,30,24,22,11,12,11,33,33,15,18,43,26,27,47,21,36,32,24,30,87,23,15,40,12,15,13,15,23,11,20,10,20,8,58,18,63,34,56,33,39,14,27,28,43,28,48,22,10,15,11,4,36,191,45,17,32,20,33,155,51,27,23,24,16,8,17,24,29,34,15,13,30,29,19,21,24,21,32,40,13,23,100,21,34,14,19,5,12,29,16,12,6,9,29,33,27,12,7,31,14,22,39,7,47,10,14,20,18,23,11,37,19,18,24,24,9,23,19,31,21,19,34,19,13,13,8,14,19,21,21,11,11,19,11,21,20,10,38,13,11,36,24,15,9,10,15,73,27,21,9,10,19,18,10,16,43,18,33,19,12,13,26,14,30,24,23,40,29,20,24,16,29,10,24,14,40,5,42,30,24,16,17,34,35,41,18,20,18,15,18,21,24,19,25,26,26,17,29,19,31,30,25,24,18,31,20,20,22,8,33,20,26,24,28,32,27,34,26,52,27,19,30,36,9,28,7,16,27,10,17,13,12,12,24,52,42,18,20,15,25,45,19,26,32,34,19,22,11,17,26,24,20,52,42,15,9,30,29,16,48,20,20,30,5,8,17,17,18,37,32,24,34,21,48,47,26,27,23,29,57,12,44,12,26],\"y\":[0.4,0.16666666666666666,0.6666666666666666,0.6,0.6086956521739131,0.5333333333333333,0.6551724137931034,0.55,0.48717948717948717,0.5882352941176471,0.6,0.14035087719298245,0.5185185185185185,0.5517241379310345,0.6875,0.5714285714285714,0.875,0.6666666666666666,0.5666666666666667,0.6428571428571429,0.21839080459770116,0.5714285714285714,0.5,0.6129032258064516,0.7142857142857143,0.23529411764705882,0.7727272727272727,0.15384615384615385,0.7777777777777778,0.5357142857142857,0.7,0.6428571428571429,0.5217391304347826,0.68,0.6296296296296297,0.6428571428571429,0.7368421052631579,0.5454545454545454,0.5384615384615384,0.6,0.65,0.675,0.5925925925925926,0.4444444444444444,0.19753086419753085,0.6111111111111112,0.5128205128205128,0.6296296296296297,0.5714285714285714,0.6206896551724138,0.6428571428571429,0.5714285714285714,0.7027027027027027,0.625,0.11764705882352941,0.4722222222222222,0.5277777777777778,0.6666666666666666,0.5555555555555556,0.75,0.4,0.08333333333333333,0.6,0.6122448979591837,0.6111111111111112,0.5957446808510638,0.6206896551724138,0.47619047619047616,0.14285714285714285,0.6428571428571429,0.7,0.625,0.5454545454545454,0.6111111111111112,0.7391304347826086,0.75,0.6428571428571429,0.6666666666666666,0.631578947368421,0.6,0.7142857142857143,0.6060606060606061,0.72,0.6296296296296297,0.6190476190476191,0.5789473684210527,0.6792452830188679,0.5121951219512195,0.5777777777777777,0.59375,0.7,0.5277777777777778,0.5454545454545454,0.6363636363636364,0.7,0.4444444444444444,0.8333333333333334,0.6818181818181818,0.45161290322580644,0.4482758620689655,0.625,0.6818181818181818,0.0,0.75,0.6363636363636364,0.6060606060606061,0.6666666666666666,0.7333333333333333,0.5,0.5348837209302325,0.7692307692307693,0.5925925925925926,0.3111111111111111,0.5714285714285714,0.5555555555555556,0.59375,0.5833333333333334,0.13793103448275862,0.6206896551724138,0.6521739130434783,0.5333333333333333,0.675,0.6666666666666666,0.21428571428571427,0.6153846153846154,0.6666666666666666,0.6521739130434783,0.5454545454545454,0.65,0.0,0.7,0.5,0.11320754716981132,0.6666666666666666,0.4166666666666667,0.6176470588235294,0.625,0.6666666666666666,0.6666666666666666,0.7857142857142857,0.19230769230769232,0.36,0.40476190476190477,0.7142857142857143,0.6875,0.6363636363636364,0.5,0.5333333333333333,0.5454545454545454,0.5,0.5833333333333334,0.1443850267379679,0.6888888888888889,0.5294117647058824,0.625,0.55,0.48484848484848486,0.23529411764705882,0.6078431372549019,0.5925925925925926,0.5652173913043478,0.7083333333333334,0.625,0.375,0.5882352941176471,0.625,0.5517241379310345,0.4411764705882353,0.4666666666666667,0.6923076923076923,0.6,0.5172413793103449,0.47368421052631576,0.47619047619047616,0.6666666666666666,0.5714285714285714,0.5625,0.725,0.6153846153846154,0.6521739130434783,0.15306122448979592,0.5714285714285714,0.6764705882352942,0.7142857142857143,0.5789473684210527,0.4,0.75,0.4482758620689655,0.8125,0.5,0.6,0.6666666666666666,0.5172413793103449,0.5454545454545454,0.4444444444444444,0.75,0.42857142857142855,0.7096774193548387,0.5,0.6363636363636364,0.5789473684210527,0.5714285714285714,0.574468085106383,0.6,0.6428571428571429,0.45,0.7222222222222222,0.391304347826087,0.5454545454545454,0.4864864864864865,0.7368421052631579,0.5555555555555556,0.6666666666666666,0.4583333333333333,0.5555555555555556,0.6521739130434783,0.631578947368421,0.5483870967741935,0.5238095238095238,0.631578947368421,0.5,0.631578947368421,0.6153846153846154,0.46153846153846156,0.375,0.6428571428571429,0.6842105263157895,0.6190476190476191,0.5238095238095238,0.45454545454545453,0.6363636363636364,0.5263157894736842,0.45454545454545453,0.5714285714285714,0.65,0.5,0.2894736842105263,0.6153846153846154,0.6363636363636364,0.25,0.5833333333333334,0.7333333333333333,0.5555555555555556,0.7,0.6,0.6027397260273972,0.25925925925925924,0.5714285714285714,0.7777777777777778,0.6,0.631578947368421,0.26666666666666666,0.7,0.625,0.6976744186046512,0.6666666666666666,0.48484848484848486,0.47368421052631576,0.6363636363636364,0.7692307692307693,0.6538461538461539,0.5714285714285714,0.4666666666666667,0.625,0.5217391304347826,0.725,0.6428571428571429,0.55,0.6666666666666666,0.625,0.4827586206896552,0.7,0.5833333333333334,0.7142857142857143,0.575,0.8,0.5714285714285714,0.5,0.5416666666666666,0.5625,0.5294117647058824,0.5294117647058824,0.5714285714285714,0.34146341463414637,0.2777777777777778,0.65,0.6111111111111112,0.6,0.6111111111111112,0.7142857142857143,0.5833333333333334,0.5789473684210527,0.6,0.6923076923076923,0.5769230769230769,0.5882352941176471,0.5862068965517241,0.5789473684210527,0.7096774193548387,0.5333333333333333,0.56,0.5833333333333334,0.5555555555555556,0.25806451612903225,0.45,0.65,0.6363636363636364,0.625,0.36363636363636365,0.7,0.5,0.5,0.5714285714285714,0.53125,0.48148148148148145,0.6764705882352942,0.6153846153846154,0.3333333333333333,0.7037037037037037,0.6842105263157895,0.43333333333333335,0.5,0.5555555555555556,0.5925925925925926,0.8571428571428571,0.5625,0.7037037037037037,0.6,0.7058823529411765,0.5,0.8333333333333334,0.75,0.5833333333333334,0.4807692307692308,0.5714285714285714,0.6111111111111112,0.5263157894736842,0.7333333333333333,0.4583333333333333,0.6363636363636364,0.5263157894736842,0.5384615384615384,0.625,0.6764705882352942,0.631578947368421,0.45454545454545453,0.5454545454545454,0.7058823529411765,0.7307692307692307,0.7083333333333334,0.6,0.5686274509803921,0.5609756097560976,0.7333333333333333,0.7777777777777778,0.6666666666666666,0.5517241379310345,0.5,0.625,0.65,0.6,0.5333333333333333,0.8,0.0,0.5882352941176471,0.4117647058823529,0.6111111111111112,0.3783783783783784,0.75,0.125,0.5882352941176471,0.09523809523809523,0.6875,0.5319148936170213,0.4230769230769231,0.5925925925925926,0.43478260869565216,0.6551724137931034,0.5892857142857143,0.9166666666666666,0.5681818181818182,0.5833333333333334,0.5769230769230769],\"type\":\"scatter\"},{\"hovertemplate\":\"Tool=spacy_lg\\u003cbr\\u003eLen=%{x}\\u003cbr\\u003eAcc=%{y:.1%}\\u003cbr\\u003eDataset=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"#ff7f0e\",\"opacity\":0.6,\"size\":6},\"mode\":\"markers\",\"name\":\"spacy_lg\",\"text\":[\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\"],\"x\":[15,81,21,20,23,15,29,40,78,34,25,61,27,29,16,28,8,6,30,14,89,14,12,31,28,104,22,80,9,28,20,14,23,25,27,14,19,11,52,15,20,40,27,18,83,36,39,27,21,29,28,14,37,8,36,36,36,15,9,16,10,88,20,49,18,47,29,21,21,14,10,8,11,18,23,8,14,18,19,25,14,33,25,27,21,38,53,41,45,64,40,36,33,11,30,9,6,22,31,30,24,22,11,12,11,33,33,15,18,43,26,27,47,21,36,32,24,30,87,23,15,40,12,15,13,15,23,11,20,10,20,8,58,18,63,34,56,33,39,14,27,28,43,28,48,22,10,15,11,4,36,191,45,17,32,20,33,155,51,27,23,24,16,8,17,24,29,34,15,13,30,29,19,21,24,21,32,40,13,23,100,21,34,14,19,5,12,29,16,12,6,9,29,33,27,12,7,31,14,22,39,7,47,10,14,20,18,23,11,37,19,18,24,24,9,23,19,31,21,19,34,19,13,13,8,14,19,21,21,11,11,19,11,21,20,10,38,13,11,36,24,15,9,10,15,73,27,21,9,10,19,18,10,16,43,18,33,19,12,13,26,14,30,24,23,40,29,20,24,16,29,10,24,14,40,5,42,30,24,16,17,34,35,41,18,20,18,15,18,21,24,19,25,26,26,17,29,19,31,30,25,24,18,31,20,20,22,8,33,20,26,24,28,32,27,34,26,52,27,19,30,36,9,28,7,16,27,10,17,13,12,12,24,52,42,18,20,15,25,45,19,26,32,34,19,22,11,17,26,24,20,52,42,15,9,30,29,16,48,20,20,30,5,8,17,17,18,37,32,24,34,21,48,47,26,27,23,29,57,12,44,12,26],\"y\":[0.4,0.654320987654321,0.7142857142857143,0.6,0.6521739130434783,0.5333333333333333,0.6551724137931034,0.6,0.5641025641025641,0.4117647058823529,0.64,0.5901639344262295,0.5185185185185185,0.6206896551724138,0.6875,0.42857142857142855,0.875,0.3333333333333333,0.3,0.6428571428571429,0.6179775280898876,0.6428571428571429,0.5833333333333334,0.6774193548387096,0.7142857142857143,0.6153846153846154,0.7727272727272727,0.65,0.7777777777777778,0.6071428571428571,0.65,0.6428571428571429,0.5652173913043478,0.72,0.6666666666666666,0.6428571428571429,0.7368421052631579,0.5454545454545454,0.5576923076923077,0.7333333333333333,0.75,0.65,0.6296296296296297,0.4444444444444444,0.6506024096385542,0.6944444444444444,0.5641025641025641,0.6666666666666666,0.7142857142857143,0.6551724137931034,0.5714285714285714,0.5714285714285714,0.7837837837837838,0.625,0.5555555555555556,0.5833333333333334,0.5277777777777778,0.4666666666666667,0.5555555555555556,0.75,0.4,0.32954545454545453,0.65,0.6326530612244898,0.5555555555555556,0.6170212765957447,0.6896551724137931,0.5238095238095238,0.23809523809523808,0.14285714285714285,0.7,0.625,0.09090909090909091,0.6111111111111112,0.043478260869565216,0.5,0.7142857142857143,0.6666666666666666,0.6842105263157895,0.2,0.35714285714285715,0.6363636363636364,0.72,0.5555555555555556,0.14285714285714285,0.18421052631578946,0.7169811320754716,0.0975609756097561,0.08888888888888889,0.59375,0.7,0.19444444444444445,0.5757575757575758,0.6363636363636364,0.7,0.1111111111111111,0.8333333333333334,0.6818181818181818,0.0967741935483871,0.5333333333333333,0.75,0.22727272727272727,0.5454545454545454,0.75,0.6363636363636364,0.5454545454545454,0.6666666666666666,0.7333333333333333,0.5555555555555556,0.5581395348837209,0.8076923076923077,0.2962962962962963,0.40425531914893614,0.5714285714285714,0.4166666666666667,0.65625,0.625,0.1,0.41379310344827586,0.782608695652174,0.4666666666666667,0.375,0.6666666666666666,0.13333333333333333,0.6923076923076923,0.6666666666666666,0.6521739130434783,0.0,0.7,0.4,0.75,0.5,0.15517241379310345,0.7777777777777778,0.5079365079365079,0.23529411764705882,0.42857142857142855,0.6060606060606061,0.15384615384615385,0.7857142857142857,0.5925925925925926,0.6785714285714286,0.09302325581395349,0.25,0.6666666666666666,0.6363636363636364,0.2,0.26666666666666666,0.5454545454545454,0.5,0.6944444444444444,0.1256544502617801,0.7111111111111111,0.47058823529411764,0.6875,0.25,0.45454545454545453,0.09032258064516129,0.7058823529411765,0.18518518518518517,0.5217391304347826,0.125,0.625,0.5,0.5882352941176471,0.625,0.2413793103448276,0.14705882352941177,0.5333333333333333,0.6923076923076923,0.6333333333333333,0.5862068965517241,0.5789473684210527,0.5238095238095238,0.7083333333333334,0.5714285714285714,0.5625,0.75,0.6153846153846154,0.6521739130434783,0.64,0.6190476190476191,0.7058823529411765,0.7142857142857143,0.5263157894736842,0.4,0.75,0.41379310344827586,0.8125,0.5,0.8,0.6666666666666666,0.13793103448275862,0.18181818181818182,0.4444444444444444,0.75,0.42857142857142855,0.7096774193548387,0.5,0.6363636363636364,0.6052631578947368,0.42857142857142855,0.5957446808510638,0.6,0.6428571428571429,0.35,0.7222222222222222,0.43478260869565216,0.45454545454545453,0.4864864864864865,0.7368421052631579,0.5555555555555556,0.20833333333333334,0.4583333333333333,0.6666666666666666,0.08695652173913043,0.6842105263157895,0.6129032258064516,0.5714285714285714,0.631578947368421,0.5294117647058824,0.631578947368421,0.6153846153846154,0.46153846153846156,0.375,0.6428571428571429,0.10526315789473684,0.6190476190476191,0.5714285714285714,0.45454545454545453,0.6363636363636364,0.5789473684210527,0.5454545454545454,0.7142857142857143,0.75,0.7,0.34210526315789475,0.3076923076923077,0.6363636363636364,0.25,0.5833333333333334,0.7333333333333333,0.5555555555555556,0.7,0.4666666666666667,0.6164383561643836,0.3333333333333333,0.6190476190476191,0.6666666666666666,0.6,0.631578947368421,0.7777777777777778,0.8,0.625,0.6976744186046512,0.2777777777777778,0.5151515151515151,0.47368421052631576,0.6363636363636364,0.7692307692307693,0.6538461538461539,0.7142857142857143,0.1,0.6666666666666666,0.6086956521739131,0.725,0.6428571428571429,0.65,0.5,0.625,0.4827586206896552,0.7,0.5833333333333334,0.7142857142857143,0.6,0.8,0.6428571428571429,0.4666666666666667,0.16666666666666666,0.625,0.5882352941176471,0.6176470588235294,0.17142857142857143,0.3902439024390244,0.2777777777777778,0.6,0.6111111111111112,0.6666666666666666,0.6666666666666666,0.7142857142857143,0.625,0.5789473684210527,0.68,0.6923076923076923,0.6153846153846154,0.5882352941176471,0.5517241379310345,0.5789473684210527,0.6774193548387096,0.6,0.6,0.5833333333333334,0.5555555555555556,0.2903225806451613,0.6,0.7,0.6818181818181818,0.625,0.36363636363636365,0.7,0.5,0.625,0.5714285714285714,0.59375,0.5925925925925926,0.35294117647058826,0.6538461538461539,0.4807692307692308,0.7037037037037037,0.7894736842105263,0.3333333333333333,0.25,0.6666666666666666,0.6296296296296297,0.8571428571428571,0.625,0.7037037037037037,0.7,0.6470588235294118,0.5833333333333334,0.8333333333333334,0.75,0.7083333333333334,0.4807692307692308,0.6428571428571429,0.6111111111111112,0.5263157894736842,0.7333333333333333,0.5,0.6590909090909091,0.631578947368421,0.5384615384615384,0.65625,0.08823529411764706,0.631578947368421,0.45454545454545453,0.5454545454545454,0.7058823529411765,0.7307692307692307,0.75,0.7,0.6078431372549019,0.6341463414634146,0.7333333333333333,0.5555555555555556,0.6666666666666666,0.5862068965517241,0.5,0.2916666666666667,0.65,0.6,0.5,0.8,0.0,0.5882352941176471,0.47058823529411764,0.7777777777777778,0.43243243243243246,0.75,0.125,0.6470588235294118,0.14285714285714285,0.7291666666666666,0.6382978723404256,0.34615384615384615,0.5925925925925926,0.5217391304347826,0.6206896551724138,0.6785714285714286,0.9166666666666666,0.6136363636363636,0.3333333333333333,0.6923076923076923],\"type\":\"scatter\"},{\"hovertemplate\":\"Tool=spacy_sm\\u003cbr\\u003eLen=%{x}\\u003cbr\\u003eAcc=%{y:.1%}\\u003cbr\\u003eDataset=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"#1f77b4\",\"opacity\":0.6,\"size\":6},\"mode\":\"markers\",\"name\":\"spacy_sm\",\"text\":[\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"Dubliners\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\",\"BNC\"],\"x\":[15,81,21,20,23,15,29,40,78,34,25,61,27,29,16,28,8,6,30,14,89,14,12,31,28,104,22,80,9,28,20,14,23,25,27,14,19,11,52,15,20,40,27,18,83,36,39,27,21,29,28,14,37,8,36,36,36,15,9,16,10,88,20,49,18,47,29,21,21,14,10,8,11,18,23,8,14,18,19,25,14,33,25,27,21,38,53,41,45,64,40,36,33,11,30,9,6,22,31,30,24,22,11,12,11,33,33,15,18,43,26,27,47,21,36,32,24,30,87,23,15,40,12,15,13,15,23,11,20,10,20,8,58,18,63,34,56,33,39,14,27,28,43,28,48,22,10,15,11,4,36,191,45,17,32,20,33,155,51,27,23,24,16,8,17,24,29,34,15,13,30,29,19,21,24,21,32,40,13,23,100,21,34,14,19,5,12,29,16,12,6,9,29,33,27,12,7,31,14,22,39,7,47,10,14,20,18,23,11,37,19,18,24,24,9,23,19,31,21,19,34,19,13,13,8,14,19,21,21,11,11,19,11,21,20,10,38,13,11,36,24,15,9,10,15,73,27,21,9,10,19,18,10,16,43,18,33,19,12,13,26,14,30,24,23,40,29,20,24,16,29,10,24,14,40,5,42,30,24,16,17,34,35,41,18,20,18,15,18,21,24,19,25,26,26,17,29,19,31,30,25,24,18,31,20,20,22,8,33,20,26,24,28,32,27,34,26,52,27,19,30,36,9,28,7,16,27,10,17,13,12,12,24,52,42,18,20,15,25,45,19,26,32,34,19,22,11,17,26,24,20,52,42,15,9,30,29,16,48,20,20,30,5,8,17,17,18,37,32,24,34,21,48,47,26,27,23,29,57,12,44,12,26],\"y\":[0.4,0.654320987654321,0.7142857142857143,0.6,0.6521739130434783,0.4666666666666667,0.6551724137931034,0.575,0.5641025641025641,0.4117647058823529,0.64,0.5573770491803278,0.5185185185185185,0.5862068965517241,0.6875,0.42857142857142855,0.875,0.3333333333333333,0.3,0.6428571428571429,0.6179775280898876,0.6428571428571429,0.5833333333333334,0.6774193548387096,0.7142857142857143,0.6153846153846154,0.7727272727272727,0.65,0.7777777777777778,0.6071428571428571,0.65,0.6428571428571429,0.5652173913043478,0.72,0.7037037037037037,0.6428571428571429,0.7368421052631579,0.5454545454545454,0.5576923076923077,0.7333333333333333,0.75,0.65,0.6296296296296297,0.4444444444444444,0.6385542168674698,0.6388888888888888,0.5641025641025641,0.6666666666666666,0.6666666666666666,0.6551724137931034,0.6428571428571429,0.5714285714285714,0.7837837837837838,0.625,0.5555555555555556,0.5833333333333334,0.5277777777777778,0.4666666666666667,0.5555555555555556,0.75,0.4,0.3181818181818182,0.7,0.6122448979591837,0.5555555555555556,0.6595744680851063,0.6551724137931034,0.5238095238095238,0.23809523809523808,0.14285714285714285,0.7,0.625,0.09090909090909091,0.6111111111111112,0.0,0.5,0.7142857142857143,0.7222222222222222,0.6842105263157895,0.2,0.35714285714285715,0.6060606060606061,0.72,0.5555555555555556,0.14285714285714285,0.18421052631578946,0.7169811320754716,0.0975609756097561,0.08888888888888889,0.59375,0.7,0.19444444444444445,0.5757575757575758,0.5454545454545454,0.7,0.1111111111111111,0.8333333333333334,0.6818181818181818,0.0967741935483871,0.5,0.75,0.22727272727272727,0.5454545454545454,0.75,0.6363636363636364,0.5454545454545454,0.6666666666666666,0.7333333333333333,0.5555555555555556,0.5581395348837209,0.8076923076923077,0.2962962962962963,0.40425531914893614,0.5714285714285714,0.4166666666666667,0.6875,0.5833333333333334,0.1,0.42528735632183906,0.782608695652174,0.4666666666666667,0.375,0.6666666666666666,0.13333333333333333,0.6153846153846154,0.6666666666666666,0.6521739130434783,0.0,0.7,0.4,0.7,0.5,0.15517241379310345,0.7777777777777778,0.49206349206349204,0.20588235294117646,0.42857142857142855,0.6060606060606061,0.1282051282051282,0.7857142857142857,0.5925925925925926,0.7142857142857143,0.09302325581395349,0.25,0.6666666666666666,0.6363636363636364,0.2,0.26666666666666666,0.5454545454545454,0.5,0.6944444444444444,0.1256544502617801,0.7111111111111111,0.47058823529411764,0.6875,0.25,0.45454545454545453,0.0967741935483871,0.6862745098039216,0.18518518518518517,0.5217391304347826,0.125,0.625,0.5,0.5882352941176471,0.625,0.27586206896551724,0.14705882352941177,0.5333333333333333,0.6923076923076923,0.6333333333333333,0.5862068965517241,0.5789473684210527,0.5238095238095238,0.7083333333333334,0.5714285714285714,0.5625,0.75,0.6153846153846154,0.6086956521739131,0.64,0.6190476190476191,0.7058823529411765,0.7142857142857143,0.5263157894736842,0.4,0.75,0.41379310344827586,0.8125,0.5,0.8,0.6666666666666666,0.13793103448275862,0.15151515151515152,0.4074074074074074,0.75,0.42857142857142855,0.7096774193548387,0.5714285714285714,0.6363636363636364,0.6052631578947368,0.5714285714285714,0.5957446808510638,0.6,0.6428571428571429,0.35,0.7222222222222222,0.391304347826087,0.6363636363636364,0.4864864864864865,0.7368421052631579,0.5555555555555556,0.20833333333333334,0.4583333333333333,0.6666666666666666,0.08695652173913043,0.6842105263157895,0.6129032258064516,0.5714285714285714,0.631578947368421,0.5294117647058824,0.631578947368421,0.6153846153846154,0.46153846153846156,0.375,0.6428571428571429,0.05263157894736842,0.6190476190476191,0.5238095238095238,0.36363636363636365,0.6363636363636364,0.5263157894736842,0.5454545454545454,0.6190476190476191,0.65,0.7,0.34210526315789475,0.3076923076923077,0.6363636363636364,0.2777777777777778,0.5833333333333334,0.7333333333333333,0.5555555555555556,0.7,0.5333333333333333,0.6301369863013698,0.2962962962962963,0.6190476190476191,0.7777777777777778,0.6,0.631578947368421,0.7777777777777778,0.8,0.6875,0.6976744186046512,0.2222222222222222,0.48484848484848486,0.47368421052631576,0.6363636363636364,0.7692307692307693,0.6538461538461539,0.7142857142857143,0.1,0.6666666666666666,0.6086956521739131,0.725,0.6428571428571429,0.65,0.5,0.625,0.4827586206896552,0.7,0.5833333333333334,0.7142857142857143,0.625,0.8,0.6428571428571429,0.4666666666666667,0.125,0.625,0.5294117647058824,0.6176470588235294,0.14285714285714285,0.3902439024390244,0.2777777777777778,0.65,0.6111111111111112,0.6666666666666666,0.6666666666666666,0.7142857142857143,0.5833333333333334,0.5789473684210527,0.68,0.6538461538461539,0.6153846153846154,0.5882352941176471,0.5517241379310345,0.5789473684210527,0.7096774193548387,0.6,0.6,0.5833333333333334,0.5555555555555556,0.25806451612903225,0.55,0.7,0.6818181818181818,0.625,0.36363636363636365,0.75,0.5,0.625,0.5714285714285714,0.59375,0.5555555555555556,0.35294117647058826,0.6153846153846154,0.5,0.6666666666666666,0.6842105263157895,0.3333333333333333,0.25,0.6666666666666666,0.5925925925925926,0.8571428571428571,0.625,0.7037037037037037,0.7,0.6470588235294118,0.5,0.8333333333333334,0.75,0.7083333333333334,0.5192307692307693,0.6428571428571429,0.6666666666666666,0.5263157894736842,0.7333333333333333,0.5,0.6363636363636364,0.631578947368421,0.5384615384615384,0.59375,0.08823529411764706,0.6842105263157895,0.45454545454545453,0.5454545454545454,0.6470588235294118,0.6923076923076923,0.75,0.75,0.6078431372549019,0.6341463414634146,0.6666666666666666,0.6666666666666666,0.6666666666666666,0.5862068965517241,0.5625,0.2916666666666667,0.65,0.6,0.5,0.8,0.0,0.5882352941176471,0.47058823529411764,0.8333333333333334,0.43243243243243246,0.75,0.125,0.6470588235294118,0.14285714285714285,0.7083333333333334,0.6382978723404256,0.34615384615384615,0.5925925925925926,0.4782608695652174,0.6551724137931034,0.6428571428571429,0.9166666666666666,0.5909090909090909,0.3333333333333333,0.6923076923076923],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Trend\",\"x\":[4.0,5.888888888888889,7.777777777777778,9.666666666666666,11.555555555555555,13.444444444444445,15.333333333333332,17.22222222222222,19.11111111111111,21.0,22.88888888888889,24.77777777777778,26.666666666666664,28.555555555555554,30.444444444444443,32.33333333333333,34.22222222222222,36.11111111111111,38.0,39.888888888888886,41.77777777777778,43.666666666666664,45.55555555555556,47.44444444444444,49.33333333333333,51.22222222222222,53.11111111111111,55.0,56.888888888888886,58.77777777777778,60.666666666666664,62.55555555555556,64.44444444444444,66.33333333333333,68.22222222222221,70.11111111111111,72.0,73.88888888888889,75.77777777777777,77.66666666666667,79.55555555555556,81.44444444444444,83.33333333333333,85.22222222222221,87.11111111111111,89.0,90.88888888888889,92.77777777777777,94.66666666666666,96.55555555555556,98.44444444444444,100.33333333333333,102.22222222222221,104.11111111111111,106.0,107.88888888888889,109.77777777777777,111.66666666666666,113.55555555555556,115.44444444444444,117.33333333333333,119.22222222222221,121.11111111111111,123.0,124.88888888888889,126.77777777777777,128.66666666666666,130.55555555555554,132.44444444444443,134.33333333333334,136.22222222222223,138.11111111111111,140.0,141.88888888888889,143.77777777777777,145.66666666666666,147.55555555555554,149.44444444444443,151.33333333333334,153.22222222222223,155.11111111111111,157.0,158.88888888888889,160.77777777777777,162.66666666666666,164.55555555555554,166.44444444444443,168.33333333333334,170.22222222222223,172.11111111111111,174.0,175.88888888888889,177.77777777777777,179.66666666666666,181.55555555555554,183.44444444444443,185.33333333333331,187.22222222222223,189.11111111111111,191.0],\"y\":[0.6016046374700015,0.5981200266536966,0.5946354158373917,0.5911508050210867,0.5876661942047818,0.5841815833884769,0.5806969725721721,0.5772123617558672,0.5737277509395623,0.5702431401232574,0.5667585293069525,0.5632739184906476,0.5597893076743427,0.5563046968580377,0.5528200860417328,0.5493354752254279,0.545850864409123,0.5423662535928181,0.5388816427765132,0.5353970319602083,0.5319124211439034,0.5284278103275984,0.5249431995112935,0.5214585886949886,0.5179739778786838,0.5144893670623788,0.511004756246074,0.5075201454297691,0.5040355346134642,0.5005509237971593,0.4970663129808543,0.4935817021645494,0.49009709134824453,0.4866124805319396,0.4831278697156347,0.4796432588993298,0.4761586480830249,0.47267403726671997,0.46918942645041506,0.46570481563411015,0.46222020481780524,0.4587355940015003,0.4552509831851954,0.4517663723688905,0.4482817615525856,0.4447971507362807,0.4413125399199758,0.4378279291036709,0.43434331828736605,0.4308587074710611,0.4273740966547562,0.4238894858384513,0.4204048750221464,0.4169202642058415,0.4134356533895366,0.40995104257323167,0.40646643175692676,0.40298182094062185,0.39949721012431694,0.396012599308012,0.3925279884917071,0.3890433776754022,0.3855587668590973,0.3820741560427924,0.37858954522648747,0.37510493441018256,0.3716203235938777,0.3681357127775728,0.36465110196126793,0.3611664911449629,0.35768188032865805,0.35419726951235314,0.35071265869604823,0.3472280478797433,0.34374343706343846,0.34025882624713355,0.33677421543082864,0.3332896046145237,0.32980499379821876,0.3263203829819139,0.322835772165609,0.3193511613493041,0.31586655053299917,0.31238193971669426,0.3088973289003894,0.3054127180840845,0.3019281072677796,0.2984434964514746,0.2949588856351697,0.29147427481886484,0.28798966400255993,0.284505053186255,0.2810204423699501,0.2775358315536452,0.27405122073734034,0.27056660992103543,0.2670819991047305,0.26359738828842555,0.26011277747212064,0.2566281666558158],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis\":{\"title\":{\"text\":\"Token accuracy\"},\"range\":[0,1],\"tickformat\":\".0%\"},\"title\":{\"text\":\"Sentence Length vs Token Accuracy (STRICT)\\u003cbr\\u003e\\u003csub\\u003eDubliners + BNC\\u003c\\u002fsub\\u003e\"},\"xaxis\":{\"title\":{\"text\":\"Sentence length (tokens in CLAWS gold)\"}},\"height\":520},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a988eab7-9e65-4314-a13e-41d34674c383');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"0736ad5e-fb0a-4ec9-b6a9-2e50c81afe9f\" class=\"plotly-graph-div\" style=\"height:520px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0736ad5e-fb0a-4ec9-b6a9-2e50c81afe9f\")) {                    Plotly.newPlot(                        \"0736ad5e-fb0a-4ec9-b6a9-2e50c81afe9f\",                        [{\"boxpoints\":\"outliers\",\"hovertemplate\":\"flair\\u003cbr\\u003eQ1–Q3 \\u002f median shown\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"name\":\"flair\",\"y\":[0.4,0.6296296296296297,0.7142857142857143,0.65,0.6521739130434783,0.4666666666666667,0.6551724137931034,0.6,0.5641025641025641,0.4117647058823529,0.64,0.5409836065573771,0.5185185185185185,0.5862068965517241,0.6875,0.6428571428571429,0.875,0.6666666666666666,0.5666666666666667,0.5714285714285714,0.5842696629213483,0.7142857142857143,0.5833333333333334,0.6774193548387096,0.7142857142857143,0.6057692307692307,0.7272727272727273,0.625,0.7777777777777778,0.6071428571428571,0.65,0.6428571428571429,0.5652173913043478,0.72,0.6666666666666666,0.6428571428571429,0.7368421052631579,0.5454545454545454,0.5576923076923077,0.7333333333333333,0.7,0.65,0.6296296296296297,0.4444444444444444,0.6265060240963856,0.6388888888888888,0.5641025641025641,0.6666666666666666,0.7142857142857143,0.6551724137931034,0.6071428571428571,0.5714285714285714,0.7567567567567568,0.625,0.5277777777777778,0.5833333333333334,0.5555555555555556,0.6666666666666666,0.6666666666666666,0.75,0.4,0.5909090909090909,0.65,0.6326530612244898,0.5555555555555556,0.6382978723404256,0.6551724137931034,0.5238095238095238,0.14285714285714285,0.14285714285714285,0.7,0.625,0.09090909090909091,0.6111111111111112,0.043478260869565216,0.75,0.7142857142857143,0.7222222222222222,0.6842105263157895,0.2,0.6428571428571429,0.6060606060606061,0.72,0.5555555555555556,0.14285714285714285,0.18421052631578946,0.6981132075471698,0.12195121951219512,0.6,0.578125,0.7,0.19444444444444445,0.5757575757575758,0.45454545454545453,0.7,0.0,0.8333333333333334,0.7272727272727273,0.06451612903225806,0.5333333333333333,0.75,0.22727272727272727,0.5454545454545454,0.75,0.6363636363636364,0.5454545454545454,0.6666666666666666,0.7333333333333333,0.5555555555555556,0.5813953488372093,0.8076923076923077,0.5925925925925926,0.3617021276595745,0.5714285714285714,0.4166666666666667,0.65625,0.625,0.1,0.6436781609195402,0.782608695652174,0.4666666666666667,0.525,0.6666666666666666,0.13333333333333333,0.5384615384615384,0.6666666666666666,0.6521739130434783,0.0,0.7,0.4,0.75,0.5,0.15517241379310345,0.7777777777777778,0.5079365079365079,0.20588235294117646,0.625,0.696969696969697,0.15384615384615385,0.7142857142857143,0.5925925925925926,0.6785714285714286,0.09302325581395349,0.75,0.6875,0.6363636363636364,0.2,0.26666666666666666,0.5454545454545454,0.5,0.6944444444444444,0.13612565445026178,0.6888888888888889,0.47058823529411764,0.65625,0.25,0.48484848484848486,0.6387096774193548,0.6862745098039216,0.6666666666666666,0.5652173913043478,0.75,0.5625,0.5,0.5882352941176471,0.625,0.27586206896551724,0.14705882352941177,0.5333333333333333,0.6923076923076923,0.6333333333333333,0.5517241379310345,0.5789473684210527,0.5238095238095238,0.7083333333333334,0.5714285714285714,0.5625,0.75,0.6153846153846154,0.6086956521739131,0.64,0.6190476190476191,0.6764705882352942,0.7142857142857143,0.5789473684210527,0.4,0.75,0.4482758620689655,0.8125,0.5,0.8,0.6666666666666666,0.5862068965517241,0.6363636363636364,0.48148148148148145,0.75,0.42857142857142855,0.7096774193548387,0.5,0.6363636363636364,0.6052631578947368,0.42857142857142855,0.6382978723404256,0.6,0.6428571428571429,0.45,0.7222222222222222,0.43478260869565216,0.5454545454545454,0.4864864864864865,0.7368421052631579,0.5555555555555556,0.6666666666666666,0.625,0.6666666666666666,0.6521739130434783,0.6842105263157895,0.6129032258064516,0.5238095238095238,0.631578947368421,0.5294117647058824,0.631578947368421,0.6153846153846154,0.5384615384615384,0.375,0.6428571428571429,0.7368421052631579,0.6190476190476191,0.5714285714285714,0.45454545454545453,0.6363636363636364,0.5263157894736842,0.6363636363636364,0.7142857142857143,0.7,0.6,0.34210526315789475,0.6153846153846154,0.6363636363636364,0.2777777777777778,0.5833333333333334,0.7333333333333333,0.5555555555555556,0.7,0.5333333333333333,0.6575342465753424,0.2962962962962963,0.6190476190476191,0.7777777777777778,0.6,0.631578947368421,0.7777777777777778,0.8,0.6875,0.7209302325581395,0.0,0.5454545454545454,0.47368421052631576,0.6363636363636364,0.7692307692307693,0.6538461538461539,0.7142857142857143,0.5,0.6666666666666666,0.5652173913043478,0.725,0.6428571428571429,0.65,0.75,0.625,0.4827586206896552,0.7,0.5416666666666666,0.7142857142857143,0.65,0.8,0.6666666666666666,0.4666666666666667,0.5416666666666666,0.625,0.5882352941176471,0.5882352941176471,0.5714285714285714,0.36585365853658536,0.2777777777777778,0.6,0.6111111111111112,0.6666666666666666,0.6666666666666666,0.7142857142857143,0.625,0.5789473684210527,0.64,0.6923076923076923,0.6153846153846154,0.5882352941176471,0.5517241379310345,0.631578947368421,0.6774193548387096,0.5666666666666667,0.6,0.5833333333333334,0.5555555555555556,0.25806451612903225,0.6,0.7,0.6818181818181818,0.625,0.36363636363636365,0.7,0.5,0.625,0.5714285714285714,0.59375,0.5925925925925926,0.7058823529411765,0.6923076923076923,0.5,0.7037037037037037,0.7894736842105263,0.4,0.5555555555555556,0.6666666666666666,0.6296296296296297,0.8571428571428571,0.625,0.037037037037037035,0.7,0.7058823529411765,0.5833333333333334,0.8333333333333334,0.75,0.125,0.5,0.6428571428571429,0.6666666666666666,0.1,0.7333333333333333,0.5,0.6590909090909091,0.631578947368421,0.5769230769230769,0.46875,0.7058823529411765,0.3157894736842105,0.45454545454545453,0.5454545454545454,0.7647058823529411,0.2692307692307692,0.75,0.7,0.6078431372549019,0.6341463414634146,0.7333333333333333,0.7777777777777778,0.6333333333333333,0.5862068965517241,0.5625,0.6041666666666666,0.7,0.45,0.26666666666666666,0.8,0.0,0.5882352941176471,0.47058823529411764,0.7777777777777778,0.4594594594594595,0.75,0.125,0.6764705882352942,0.14285714285714285,0.20833333333333334,0.23404255319148937,0.38461538461538464,0.5925925925925926,0.5217391304347826,0.6551724137931034,0.6607142857142857,0.4166666666666667,0.5909090909090909,0.6666666666666666,0.6538461538461539],\"type\":\"box\"},{\"boxpoints\":\"outliers\",\"hovertemplate\":\"nltk\\u003cbr\\u003eQ1–Q3 \\u002f median shown\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"name\":\"nltk\",\"y\":[0.4,0.16666666666666666,0.6666666666666666,0.6,0.6086956521739131,0.5333333333333333,0.6551724137931034,0.55,0.48717948717948717,0.5882352941176471,0.6,0.14035087719298245,0.5185185185185185,0.5517241379310345,0.6875,0.5714285714285714,0.875,0.6666666666666666,0.5666666666666667,0.6428571428571429,0.21839080459770116,0.5714285714285714,0.5,0.6129032258064516,0.7142857142857143,0.23529411764705882,0.7727272727272727,0.15384615384615385,0.7777777777777778,0.5357142857142857,0.7,0.6428571428571429,0.5217391304347826,0.68,0.6296296296296297,0.6428571428571429,0.7368421052631579,0.5454545454545454,0.5384615384615384,0.6,0.65,0.675,0.5925925925925926,0.4444444444444444,0.19753086419753085,0.6111111111111112,0.5128205128205128,0.6296296296296297,0.5714285714285714,0.6206896551724138,0.6428571428571429,0.5714285714285714,0.7027027027027027,0.625,0.11764705882352941,0.4722222222222222,0.5277777777777778,0.6666666666666666,0.5555555555555556,0.75,0.4,0.08333333333333333,0.6,0.6122448979591837,0.6111111111111112,0.5957446808510638,0.6206896551724138,0.47619047619047616,0.14285714285714285,0.6428571428571429,0.7,0.625,0.5454545454545454,0.6111111111111112,0.7391304347826086,0.75,0.6428571428571429,0.6666666666666666,0.631578947368421,0.6,0.7142857142857143,0.6060606060606061,0.72,0.6296296296296297,0.6190476190476191,0.5789473684210527,0.6792452830188679,0.5121951219512195,0.5777777777777777,0.59375,0.7,0.5277777777777778,0.5454545454545454,0.6363636363636364,0.7,0.4444444444444444,0.8333333333333334,0.6818181818181818,0.45161290322580644,0.4482758620689655,0.625,0.6818181818181818,0.0,0.75,0.6363636363636364,0.6060606060606061,0.6666666666666666,0.7333333333333333,0.5,0.5348837209302325,0.7692307692307693,0.5925925925925926,0.3111111111111111,0.5714285714285714,0.5555555555555556,0.59375,0.5833333333333334,0.13793103448275862,0.6206896551724138,0.6521739130434783,0.5333333333333333,0.675,0.6666666666666666,0.21428571428571427,0.6153846153846154,0.6666666666666666,0.6521739130434783,0.5454545454545454,0.65,0.0,0.7,0.5,0.11320754716981132,0.6666666666666666,0.4166666666666667,0.6176470588235294,0.625,0.6666666666666666,0.6666666666666666,0.7857142857142857,0.19230769230769232,0.36,0.40476190476190477,0.7142857142857143,0.6875,0.6363636363636364,0.5,0.5333333333333333,0.5454545454545454,0.5,0.5833333333333334,0.1443850267379679,0.6888888888888889,0.5294117647058824,0.625,0.55,0.48484848484848486,0.23529411764705882,0.6078431372549019,0.5925925925925926,0.5652173913043478,0.7083333333333334,0.625,0.375,0.5882352941176471,0.625,0.5517241379310345,0.4411764705882353,0.4666666666666667,0.6923076923076923,0.6,0.5172413793103449,0.47368421052631576,0.47619047619047616,0.6666666666666666,0.5714285714285714,0.5625,0.725,0.6153846153846154,0.6521739130434783,0.15306122448979592,0.5714285714285714,0.6764705882352942,0.7142857142857143,0.5789473684210527,0.4,0.75,0.4482758620689655,0.8125,0.5,0.6,0.6666666666666666,0.5172413793103449,0.5454545454545454,0.4444444444444444,0.75,0.42857142857142855,0.7096774193548387,0.5,0.6363636363636364,0.5789473684210527,0.5714285714285714,0.574468085106383,0.6,0.6428571428571429,0.45,0.7222222222222222,0.391304347826087,0.5454545454545454,0.4864864864864865,0.7368421052631579,0.5555555555555556,0.6666666666666666,0.4583333333333333,0.5555555555555556,0.6521739130434783,0.631578947368421,0.5483870967741935,0.5238095238095238,0.631578947368421,0.5,0.631578947368421,0.6153846153846154,0.46153846153846156,0.375,0.6428571428571429,0.6842105263157895,0.6190476190476191,0.5238095238095238,0.45454545454545453,0.6363636363636364,0.5263157894736842,0.45454545454545453,0.5714285714285714,0.65,0.5,0.2894736842105263,0.6153846153846154,0.6363636363636364,0.25,0.5833333333333334,0.7333333333333333,0.5555555555555556,0.7,0.6,0.6027397260273972,0.25925925925925924,0.5714285714285714,0.7777777777777778,0.6,0.631578947368421,0.26666666666666666,0.7,0.625,0.6976744186046512,0.6666666666666666,0.48484848484848486,0.47368421052631576,0.6363636363636364,0.7692307692307693,0.6538461538461539,0.5714285714285714,0.4666666666666667,0.625,0.5217391304347826,0.725,0.6428571428571429,0.55,0.6666666666666666,0.625,0.4827586206896552,0.7,0.5833333333333334,0.7142857142857143,0.575,0.8,0.5714285714285714,0.5,0.5416666666666666,0.5625,0.5294117647058824,0.5294117647058824,0.5714285714285714,0.34146341463414637,0.2777777777777778,0.65,0.6111111111111112,0.6,0.6111111111111112,0.7142857142857143,0.5833333333333334,0.5789473684210527,0.6,0.6923076923076923,0.5769230769230769,0.5882352941176471,0.5862068965517241,0.5789473684210527,0.7096774193548387,0.5333333333333333,0.56,0.5833333333333334,0.5555555555555556,0.25806451612903225,0.45,0.65,0.6363636363636364,0.625,0.36363636363636365,0.7,0.5,0.5,0.5714285714285714,0.53125,0.48148148148148145,0.6764705882352942,0.6153846153846154,0.3333333333333333,0.7037037037037037,0.6842105263157895,0.43333333333333335,0.5,0.5555555555555556,0.5925925925925926,0.8571428571428571,0.5625,0.7037037037037037,0.6,0.7058823529411765,0.5,0.8333333333333334,0.75,0.5833333333333334,0.4807692307692308,0.5714285714285714,0.6111111111111112,0.5263157894736842,0.7333333333333333,0.4583333333333333,0.6363636363636364,0.5263157894736842,0.5384615384615384,0.625,0.6764705882352942,0.631578947368421,0.45454545454545453,0.5454545454545454,0.7058823529411765,0.7307692307692307,0.7083333333333334,0.6,0.5686274509803921,0.5609756097560976,0.7333333333333333,0.7777777777777778,0.6666666666666666,0.5517241379310345,0.5,0.625,0.65,0.6,0.5333333333333333,0.8,0.0,0.5882352941176471,0.4117647058823529,0.6111111111111112,0.3783783783783784,0.75,0.125,0.5882352941176471,0.09523809523809523,0.6875,0.5319148936170213,0.4230769230769231,0.5925925925925926,0.43478260869565216,0.6551724137931034,0.5892857142857143,0.9166666666666666,0.5681818181818182,0.5833333333333334,0.5769230769230769],\"type\":\"box\"},{\"boxpoints\":\"outliers\",\"hovertemplate\":\"spacy_lg\\u003cbr\\u003eQ1–Q3 \\u002f median shown\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"name\":\"spacy_lg\",\"y\":[0.4,0.654320987654321,0.7142857142857143,0.6,0.6521739130434783,0.5333333333333333,0.6551724137931034,0.6,0.5641025641025641,0.4117647058823529,0.64,0.5901639344262295,0.5185185185185185,0.6206896551724138,0.6875,0.42857142857142855,0.875,0.3333333333333333,0.3,0.6428571428571429,0.6179775280898876,0.6428571428571429,0.5833333333333334,0.6774193548387096,0.7142857142857143,0.6153846153846154,0.7727272727272727,0.65,0.7777777777777778,0.6071428571428571,0.65,0.6428571428571429,0.5652173913043478,0.72,0.6666666666666666,0.6428571428571429,0.7368421052631579,0.5454545454545454,0.5576923076923077,0.7333333333333333,0.75,0.65,0.6296296296296297,0.4444444444444444,0.6506024096385542,0.6944444444444444,0.5641025641025641,0.6666666666666666,0.7142857142857143,0.6551724137931034,0.5714285714285714,0.5714285714285714,0.7837837837837838,0.625,0.5555555555555556,0.5833333333333334,0.5277777777777778,0.4666666666666667,0.5555555555555556,0.75,0.4,0.32954545454545453,0.65,0.6326530612244898,0.5555555555555556,0.6170212765957447,0.6896551724137931,0.5238095238095238,0.23809523809523808,0.14285714285714285,0.7,0.625,0.09090909090909091,0.6111111111111112,0.043478260869565216,0.5,0.7142857142857143,0.6666666666666666,0.6842105263157895,0.2,0.35714285714285715,0.6363636363636364,0.72,0.5555555555555556,0.14285714285714285,0.18421052631578946,0.7169811320754716,0.0975609756097561,0.08888888888888889,0.59375,0.7,0.19444444444444445,0.5757575757575758,0.6363636363636364,0.7,0.1111111111111111,0.8333333333333334,0.6818181818181818,0.0967741935483871,0.5333333333333333,0.75,0.22727272727272727,0.5454545454545454,0.75,0.6363636363636364,0.5454545454545454,0.6666666666666666,0.7333333333333333,0.5555555555555556,0.5581395348837209,0.8076923076923077,0.2962962962962963,0.40425531914893614,0.5714285714285714,0.4166666666666667,0.65625,0.625,0.1,0.41379310344827586,0.782608695652174,0.4666666666666667,0.375,0.6666666666666666,0.13333333333333333,0.6923076923076923,0.6666666666666666,0.6521739130434783,0.0,0.7,0.4,0.75,0.5,0.15517241379310345,0.7777777777777778,0.5079365079365079,0.23529411764705882,0.42857142857142855,0.6060606060606061,0.15384615384615385,0.7857142857142857,0.5925925925925926,0.6785714285714286,0.09302325581395349,0.25,0.6666666666666666,0.6363636363636364,0.2,0.26666666666666666,0.5454545454545454,0.5,0.6944444444444444,0.1256544502617801,0.7111111111111111,0.47058823529411764,0.6875,0.25,0.45454545454545453,0.09032258064516129,0.7058823529411765,0.18518518518518517,0.5217391304347826,0.125,0.625,0.5,0.5882352941176471,0.625,0.2413793103448276,0.14705882352941177,0.5333333333333333,0.6923076923076923,0.6333333333333333,0.5862068965517241,0.5789473684210527,0.5238095238095238,0.7083333333333334,0.5714285714285714,0.5625,0.75,0.6153846153846154,0.6521739130434783,0.64,0.6190476190476191,0.7058823529411765,0.7142857142857143,0.5263157894736842,0.4,0.75,0.41379310344827586,0.8125,0.5,0.8,0.6666666666666666,0.13793103448275862,0.18181818181818182,0.4444444444444444,0.75,0.42857142857142855,0.7096774193548387,0.5,0.6363636363636364,0.6052631578947368,0.42857142857142855,0.5957446808510638,0.6,0.6428571428571429,0.35,0.7222222222222222,0.43478260869565216,0.45454545454545453,0.4864864864864865,0.7368421052631579,0.5555555555555556,0.20833333333333334,0.4583333333333333,0.6666666666666666,0.08695652173913043,0.6842105263157895,0.6129032258064516,0.5714285714285714,0.631578947368421,0.5294117647058824,0.631578947368421,0.6153846153846154,0.46153846153846156,0.375,0.6428571428571429,0.10526315789473684,0.6190476190476191,0.5714285714285714,0.45454545454545453,0.6363636363636364,0.5789473684210527,0.5454545454545454,0.7142857142857143,0.75,0.7,0.34210526315789475,0.3076923076923077,0.6363636363636364,0.25,0.5833333333333334,0.7333333333333333,0.5555555555555556,0.7,0.4666666666666667,0.6164383561643836,0.3333333333333333,0.6190476190476191,0.6666666666666666,0.6,0.631578947368421,0.7777777777777778,0.8,0.625,0.6976744186046512,0.2777777777777778,0.5151515151515151,0.47368421052631576,0.6363636363636364,0.7692307692307693,0.6538461538461539,0.7142857142857143,0.1,0.6666666666666666,0.6086956521739131,0.725,0.6428571428571429,0.65,0.5,0.625,0.4827586206896552,0.7,0.5833333333333334,0.7142857142857143,0.6,0.8,0.6428571428571429,0.4666666666666667,0.16666666666666666,0.625,0.5882352941176471,0.6176470588235294,0.17142857142857143,0.3902439024390244,0.2777777777777778,0.6,0.6111111111111112,0.6666666666666666,0.6666666666666666,0.7142857142857143,0.625,0.5789473684210527,0.68,0.6923076923076923,0.6153846153846154,0.5882352941176471,0.5517241379310345,0.5789473684210527,0.6774193548387096,0.6,0.6,0.5833333333333334,0.5555555555555556,0.2903225806451613,0.6,0.7,0.6818181818181818,0.625,0.36363636363636365,0.7,0.5,0.625,0.5714285714285714,0.59375,0.5925925925925926,0.35294117647058826,0.6538461538461539,0.4807692307692308,0.7037037037037037,0.7894736842105263,0.3333333333333333,0.25,0.6666666666666666,0.6296296296296297,0.8571428571428571,0.625,0.7037037037037037,0.7,0.6470588235294118,0.5833333333333334,0.8333333333333334,0.75,0.7083333333333334,0.4807692307692308,0.6428571428571429,0.6111111111111112,0.5263157894736842,0.7333333333333333,0.5,0.6590909090909091,0.631578947368421,0.5384615384615384,0.65625,0.08823529411764706,0.631578947368421,0.45454545454545453,0.5454545454545454,0.7058823529411765,0.7307692307692307,0.75,0.7,0.6078431372549019,0.6341463414634146,0.7333333333333333,0.5555555555555556,0.6666666666666666,0.5862068965517241,0.5,0.2916666666666667,0.65,0.6,0.5,0.8,0.0,0.5882352941176471,0.47058823529411764,0.7777777777777778,0.43243243243243246,0.75,0.125,0.6470588235294118,0.14285714285714285,0.7291666666666666,0.6382978723404256,0.34615384615384615,0.5925925925925926,0.5217391304347826,0.6206896551724138,0.6785714285714286,0.9166666666666666,0.6136363636363636,0.3333333333333333,0.6923076923076923],\"type\":\"box\"},{\"boxpoints\":\"outliers\",\"hovertemplate\":\"spacy_sm\\u003cbr\\u003eQ1–Q3 \\u002f median shown\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"name\":\"spacy_sm\",\"y\":[0.4,0.654320987654321,0.7142857142857143,0.6,0.6521739130434783,0.4666666666666667,0.6551724137931034,0.575,0.5641025641025641,0.4117647058823529,0.64,0.5573770491803278,0.5185185185185185,0.5862068965517241,0.6875,0.42857142857142855,0.875,0.3333333333333333,0.3,0.6428571428571429,0.6179775280898876,0.6428571428571429,0.5833333333333334,0.6774193548387096,0.7142857142857143,0.6153846153846154,0.7727272727272727,0.65,0.7777777777777778,0.6071428571428571,0.65,0.6428571428571429,0.5652173913043478,0.72,0.7037037037037037,0.6428571428571429,0.7368421052631579,0.5454545454545454,0.5576923076923077,0.7333333333333333,0.75,0.65,0.6296296296296297,0.4444444444444444,0.6385542168674698,0.6388888888888888,0.5641025641025641,0.6666666666666666,0.6666666666666666,0.6551724137931034,0.6428571428571429,0.5714285714285714,0.7837837837837838,0.625,0.5555555555555556,0.5833333333333334,0.5277777777777778,0.4666666666666667,0.5555555555555556,0.75,0.4,0.3181818181818182,0.7,0.6122448979591837,0.5555555555555556,0.6595744680851063,0.6551724137931034,0.5238095238095238,0.23809523809523808,0.14285714285714285,0.7,0.625,0.09090909090909091,0.6111111111111112,0.0,0.5,0.7142857142857143,0.7222222222222222,0.6842105263157895,0.2,0.35714285714285715,0.6060606060606061,0.72,0.5555555555555556,0.14285714285714285,0.18421052631578946,0.7169811320754716,0.0975609756097561,0.08888888888888889,0.59375,0.7,0.19444444444444445,0.5757575757575758,0.5454545454545454,0.7,0.1111111111111111,0.8333333333333334,0.6818181818181818,0.0967741935483871,0.5,0.75,0.22727272727272727,0.5454545454545454,0.75,0.6363636363636364,0.5454545454545454,0.6666666666666666,0.7333333333333333,0.5555555555555556,0.5581395348837209,0.8076923076923077,0.2962962962962963,0.40425531914893614,0.5714285714285714,0.4166666666666667,0.6875,0.5833333333333334,0.1,0.42528735632183906,0.782608695652174,0.4666666666666667,0.375,0.6666666666666666,0.13333333333333333,0.6153846153846154,0.6666666666666666,0.6521739130434783,0.0,0.7,0.4,0.7,0.5,0.15517241379310345,0.7777777777777778,0.49206349206349204,0.20588235294117646,0.42857142857142855,0.6060606060606061,0.1282051282051282,0.7857142857142857,0.5925925925925926,0.7142857142857143,0.09302325581395349,0.25,0.6666666666666666,0.6363636363636364,0.2,0.26666666666666666,0.5454545454545454,0.5,0.6944444444444444,0.1256544502617801,0.7111111111111111,0.47058823529411764,0.6875,0.25,0.45454545454545453,0.0967741935483871,0.6862745098039216,0.18518518518518517,0.5217391304347826,0.125,0.625,0.5,0.5882352941176471,0.625,0.27586206896551724,0.14705882352941177,0.5333333333333333,0.6923076923076923,0.6333333333333333,0.5862068965517241,0.5789473684210527,0.5238095238095238,0.7083333333333334,0.5714285714285714,0.5625,0.75,0.6153846153846154,0.6086956521739131,0.64,0.6190476190476191,0.7058823529411765,0.7142857142857143,0.5263157894736842,0.4,0.75,0.41379310344827586,0.8125,0.5,0.8,0.6666666666666666,0.13793103448275862,0.15151515151515152,0.4074074074074074,0.75,0.42857142857142855,0.7096774193548387,0.5714285714285714,0.6363636363636364,0.6052631578947368,0.5714285714285714,0.5957446808510638,0.6,0.6428571428571429,0.35,0.7222222222222222,0.391304347826087,0.6363636363636364,0.4864864864864865,0.7368421052631579,0.5555555555555556,0.20833333333333334,0.4583333333333333,0.6666666666666666,0.08695652173913043,0.6842105263157895,0.6129032258064516,0.5714285714285714,0.631578947368421,0.5294117647058824,0.631578947368421,0.6153846153846154,0.46153846153846156,0.375,0.6428571428571429,0.05263157894736842,0.6190476190476191,0.5238095238095238,0.36363636363636365,0.6363636363636364,0.5263157894736842,0.5454545454545454,0.6190476190476191,0.65,0.7,0.34210526315789475,0.3076923076923077,0.6363636363636364,0.2777777777777778,0.5833333333333334,0.7333333333333333,0.5555555555555556,0.7,0.5333333333333333,0.6301369863013698,0.2962962962962963,0.6190476190476191,0.7777777777777778,0.6,0.631578947368421,0.7777777777777778,0.8,0.6875,0.6976744186046512,0.2222222222222222,0.48484848484848486,0.47368421052631576,0.6363636363636364,0.7692307692307693,0.6538461538461539,0.7142857142857143,0.1,0.6666666666666666,0.6086956521739131,0.725,0.6428571428571429,0.65,0.5,0.625,0.4827586206896552,0.7,0.5833333333333334,0.7142857142857143,0.625,0.8,0.6428571428571429,0.4666666666666667,0.125,0.625,0.5294117647058824,0.6176470588235294,0.14285714285714285,0.3902439024390244,0.2777777777777778,0.65,0.6111111111111112,0.6666666666666666,0.6666666666666666,0.7142857142857143,0.5833333333333334,0.5789473684210527,0.68,0.6538461538461539,0.6153846153846154,0.5882352941176471,0.5517241379310345,0.5789473684210527,0.7096774193548387,0.6,0.6,0.5833333333333334,0.5555555555555556,0.25806451612903225,0.55,0.7,0.6818181818181818,0.625,0.36363636363636365,0.75,0.5,0.625,0.5714285714285714,0.59375,0.5555555555555556,0.35294117647058826,0.6153846153846154,0.5,0.6666666666666666,0.6842105263157895,0.3333333333333333,0.25,0.6666666666666666,0.5925925925925926,0.8571428571428571,0.625,0.7037037037037037,0.7,0.6470588235294118,0.5,0.8333333333333334,0.75,0.7083333333333334,0.5192307692307693,0.6428571428571429,0.6666666666666666,0.5263157894736842,0.7333333333333333,0.5,0.6363636363636364,0.631578947368421,0.5384615384615384,0.59375,0.08823529411764706,0.6842105263157895,0.45454545454545453,0.5454545454545454,0.6470588235294118,0.6923076923076923,0.75,0.75,0.6078431372549019,0.6341463414634146,0.6666666666666666,0.6666666666666666,0.6666666666666666,0.5862068965517241,0.5625,0.2916666666666667,0.65,0.6,0.5,0.8,0.0,0.5882352941176471,0.47058823529411764,0.8333333333333334,0.43243243243243246,0.75,0.125,0.6470588235294118,0.14285714285714285,0.7083333333333334,0.6382978723404256,0.34615384615384615,0.5925925925925926,0.4782608695652174,0.6551724137931034,0.6428571428571429,0.9166666666666666,0.5909090909090909,0.3333333333333333,0.6923076923076923],\"type\":\"box\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis\":{\"title\":{\"text\":\"Sentence accuracy\"},\"range\":[0,1],\"tickformat\":\".0%\"},\"title\":{\"text\":\"Distribution of Sentence-Level Accuracies (STRICT)\"},\"xaxis\":{\"title\":{\"text\":\"Tool\"}},\"height\":520},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('0736ad5e-fb0a-4ec9-b6a9-2e50c81afe9f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced visualizations complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# POWERFUL ADDITIONAL VISUALS: per-dataset CIs, McNemar composition, per-tag F1\n",
        "# ==============================================================================\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# -- Safety: Wilson CI helper (if not in scope)\n",
        "if 'wilson_confidence_interval' not in globals():\n",
        "    from math import sqrt\n",
        "    import scipy.stats as stats\n",
        "    def wilson_confidence_interval(successes, trials, confidence=0.95):\n",
        "        if trials == 0: return 0.0, 0.0, 0.0\n",
        "        z = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
        "        p = successes / trials\n",
        "        denom = 1 + z**2 / trials\n",
        "        centre = (p + z**2/(2*trials)) / denom\n",
        "        half = z * np.sqrt((p*(1-p) + z**2/(4*trials))/trials) / denom\n",
        "        return p, max(0, centre - half), min(1, centre + half)\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Per-dataset Wilson CIs\n",
        "# -------------------------------\n",
        "def compute_wilson_by_dataset(results):\n",
        "    tool_set = set()\n",
        "    for r in results:\n",
        "        tool_set.update(r.get('tool_results', {}).keys())\n",
        "    datasets = sorted({r['dataset'] for r in results})\n",
        "\n",
        "    out = {ds:{} for ds in datasets}\n",
        "    for ds in datasets:\n",
        "        for tool in sorted(tool_set):\n",
        "            total_tokens = correct = sentences = 0\n",
        "            for r in results:\n",
        "                if r['dataset'] != ds:\n",
        "                    continue\n",
        "                tri = r['tool_results'].get(tool, {})\n",
        "                if not isinstance(tri, dict) or 'error' in tri:\n",
        "                    continue\n",
        "                gt = r['ground_truth']\n",
        "                pred = tri.get('tags', [])\n",
        "                toks = tri.get('tokens', [])\n",
        "                m = min(len(gt), len(pred), len(toks))\n",
        "                if m == 0:\n",
        "                    continue\n",
        "                gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "                correct += sum(1 for i in range(m) if gt_penn[i] == pred[i])\n",
        "                total_tokens += m\n",
        "                sentences += 1\n",
        "            if total_tokens > 0:\n",
        "                p, lo, hi = wilson_confidence_interval(correct, total_tokens)\n",
        "                out[ds][tool] = dict(acc=p, lo=lo, hi=hi, tokens=total_tokens, sents=sentences)\n",
        "    return out\n",
        "\n",
        "ci_by_ds = compute_wilson_by_dataset(expanded_batch_results)\n",
        "\n",
        "# Plot: one combined horizontal CI chart (labels \"Dataset • Tool\")\n",
        "fig_ci_ds = go.Figure()\n",
        "y_labels, x_pts, x_los, x_his = [], [], [], []\n",
        "for ds in ci_by_ds:\n",
        "    for tool, d in sorted(ci_by_ds[ds].items(), key=lambda kv: kv[1]['acc'], reverse=True):\n",
        "        y_labels.append(f\"{ds} • {tool}\")\n",
        "        x_pts.append(d['acc']); x_los.append(d['lo']); x_his.append(d['hi'])\n",
        "\n",
        "for i, y in enumerate(y_labels):\n",
        "    fig_ci_ds.add_trace(go.Scatter(x=[x_pts[i]], y=[y], mode='markers',\n",
        "                                   marker=dict(size=10), name=y, showlegend=False))\n",
        "    fig_ci_ds.add_trace(go.Scatter(x=[x_los[i], x_his[i]], y=[y, y], mode='lines',\n",
        "                                   line=dict(width=3), showlegend=False))\n",
        "\n",
        "fig_ci_ds.update_layout(\n",
        "    title=\"Token Accuracy with 95% Wilson CIs by Dataset (STRICT)\",\n",
        "    xaxis_title=\"Token Accuracy\",\n",
        "    yaxis_title=\"Dataset • Tool\",\n",
        "    xaxis=dict(range=[0.45, 0.65]),\n",
        "    height=400 + 20*len(y_labels)\n",
        ")\n",
        "fig_ci_ds.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2) McNemar composition bars (interpretable disagreements)\n",
        "# ---------------------------------------------------------\n",
        "def build_mcnemar_table(tool_a, tool_b, results):\n",
        "    bc = bw = a_cbw = b_caw = 0\n",
        "    for r in results:\n",
        "        gt = r['ground_truth']\n",
        "        A = r['tool_results'].get(tool_a, {})\n",
        "        B = r['tool_results'].get(tool_b, {})\n",
        "        if 'error' in A or 'error' in B:\n",
        "            continue\n",
        "        pa, pb = A.get('tags', []), B.get('tags', [])\n",
        "        ta, tb = A.get('tokens', []), B.get('tokens', [])\n",
        "        m = min(len(gt), len(pa), len(pb), len(ta), len(tb))\n",
        "        if m == 0:\n",
        "            continue\n",
        "        gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "        for i in range(m):\n",
        "            a_ok = (gt_penn[i] == pa[i]); b_ok = (gt_penn[i] == pb[i])\n",
        "            if a_ok and b_ok: bc += 1\n",
        "            elif (not a_ok) and (not b_ok): bw += 1\n",
        "            elif a_ok and (not b_ok): a_cbw += 1\n",
        "            else: b_caw += 1\n",
        "    return [[bc, a_cbw],[b_caw, bw]]\n",
        "\n",
        "# Build a stacked horizontal bar per pair\n",
        "tools_for_pairs = sorted({t for r in expanded_batch_results for t in r.get('tool_results', {}).keys()})\n",
        "pair_labels = []\n",
        "BC = []; A_C_B_W = []; B_C_A_W = []; BW = []\n",
        "\n",
        "for a, b in itertools.combinations(tools_for_pairs, 2):\n",
        "    t = build_mcnemar_table(a, b, expanded_batch_results)\n",
        "    pair_labels.append(f\"{a} vs {b}\")\n",
        "    BC.append(t[0][0]); A_C_B_W.append(t[0][1]); B_C_A_W.append(t[1][0]); BW.append(t[1][1])\n",
        "\n",
        "fig_mcnemar = go.Figure()\n",
        "fig_mcnemar.add_trace(go.Bar(y=pair_labels, x=BC, name=\"Both correct\", orientation='h'))\n",
        "fig_mcnemar.add_trace(go.Bar(y=pair_labels, x=A_C_B_W, name=\"Only A correct\", orientation='h'))\n",
        "fig_mcnemar.add_trace(go.Bar(y=pair_labels, x=B_C_A_W, name=\"Only B correct\", orientation='h'))\n",
        "fig_mcnemar.add_trace(go.Bar(y=pair_labels, x=BW, name=\"Both wrong\", orientation='h'))\n",
        "\n",
        "fig_mcnemar.update_layout(\n",
        "    barmode='stack',\n",
        "    title=\"Paired Outcomes per Token (McNemar table visualisation)\",\n",
        "    xaxis_title=\"Token count\",\n",
        "    yaxis_title=\"Tool pair\",\n",
        "    height=300 + 30*len(pair_labels)\n",
        ")\n",
        "fig_mcnemar.show()\n",
        "\n",
        "# --------------------------------------------\n",
        "# 3) Per-tag F1 (top support, excluding 'UNK')\n",
        "# --------------------------------------------\n",
        "def per_tool_ytrue_ypred(results):\n",
        "    per_tool = defaultdict(lambda: {'y_true': [], 'y_pred': []})\n",
        "    for r in results:\n",
        "        gt = r['ground_truth']\n",
        "        for tool, tri in r.get('tool_results', {}).items():\n",
        "            if 'error' in tri:\n",
        "                continue\n",
        "            pred = tri.get('tags', [])\n",
        "            toks = tri.get('tokens', [])\n",
        "            m = min(len(gt), len(pred), len(toks))\n",
        "            if m == 0:\n",
        "                continue\n",
        "            gt_penn = [convert_claws_to_penn(t, strict=True) for t in gt[:m]]\n",
        "            per_tool[tool]['y_true'].extend(gt_penn)\n",
        "            per_tool[tool]['y_pred'].extend(pred[:m])\n",
        "    return per_tool\n",
        "\n",
        "per_tool_arrays = per_tool_ytrue_ypred(expanded_batch_results)\n",
        "\n",
        "# Define label set and supports (pooled gold)\n",
        "label_support = Counter()\n",
        "for d in per_tool_arrays.values():\n",
        "    label_support.update(d['y_true'])\n",
        "# exclude UNK for readability\n",
        "labels_ranked = [lbl for lbl, _ in label_support.most_common() if lbl != 'UNK']\n",
        "top_labels = labels_ranked[:10] if len(labels_ranked) >= 10 else labels_ranked\n",
        "\n",
        "fig_f1 = go.Figure()\n",
        "for tool, d in sorted(per_tool_arrays.items()):\n",
        "    y_true = np.array(d['y_true'])\n",
        "    y_pred = np.array(d['y_pred'])\n",
        "    if y_true.size == 0:\n",
        "        continue\n",
        "    # compute per-class metrics restricted to top_labels\n",
        "    p, r, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=top_labels, average=None, zero_division=0\n",
        "    )\n",
        "    fig_f1.add_trace(go.Bar(x=top_labels, y=f1, name=tool))\n",
        "\n",
        "fig_f1.update_layout(\n",
        "    title=\"Per-tag F1 by Tool (Top 10 Gold Labels, STRICT; UNK excluded)\",\n",
        "    xaxis_title=\"Penn tag\",\n",
        "    yaxis_title=\"F1\",\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    barmode='group',\n",
        "    height=450\n",
        ")\n",
        "fig_f1.show()\n",
        "\n",
        "print(\"Additional visuals generated: per-dataset CIs (fig_ci_ds), McNemar composition (fig_mcnemar), per-tag F1 (fig_f1).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "7Um4khrNfOME",
        "outputId": "d795d57f-3e5b-4f07-9e46-0fcd2d650e3d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'norm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1998314394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mci_by_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_wilson_by_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_batch_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Plot: one combined horizontal CI chart (labels \"Dataset • Tool\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1998314394.py\u001b[0m in \u001b[0;36mcompute_wilson_by_dataset\u001b[0;34m(results)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0msentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwilson_confidence_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2034924655.py\u001b[0m in \u001b[0;36mwilson_confidence_interval\u001b[0;34m(successes, trials, confidence)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuccesses\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcentre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'norm'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SAVE VISUALIZATIONS + RICH DASHBOARD (STRICT, pooled Dubliners + BNC)\n",
        "# Includes: more results sections + expanded methods + CLAWS C7 note\n",
        "# ==============================================================================\n",
        "\n",
        "import os, zipfile\n",
        "from datetime import datetime\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def _exists(name):  # object exists and is not None\n",
        "    return name in globals() and globals()[name] is not None\n",
        "\n",
        "assert 'wilson_results' in globals() and wilson_results, \"wilson_results not found. Run the CI cell first.\"\n",
        "\n",
        "# ---------------- Core stats for dashboard header ----------------\n",
        "_tool_stats = {k: v for k, v in wilson_results.items() if v.get('total_tokens', 0) > 0}\n",
        "tools_sorted = sorted(_tool_stats.keys(), key=lambda t: _tool_stats[t]['token_accuracy'], reverse=True)\n",
        "best_tool = tools_sorted[0]\n",
        "best = _tool_stats[best_tool]\n",
        "sentences_evaluated = max(s['total_sentences'] for s in _tool_stats.values())\n",
        "\n",
        "def _proportion_z_test(x1, n1, x2, n2):\n",
        "    p_pool = (x1 + x2) / (n1 + n2)\n",
        "    se = sqrt(p_pool * (1 - p_pool) * (1/n1 + 1/n2))\n",
        "    z = (x1/n1 - x2/n2) / se\n",
        "    p = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "    return z, p\n",
        "\n",
        "def _cohens_h(p1, p2):\n",
        "    return 2 * (np.arcsin(sqrt(p1)) - np.arcsin(sqrt(p2)))\n",
        "\n",
        "worst_tool = tools_sorted[-1]\n",
        "worst = _tool_stats[worst_tool]\n",
        "z_stat, p_val = _proportion_z_test(best['correct_tokens'], best['total_tokens'],\n",
        "                                   worst['correct_tokens'], worst['total_tokens'])\n",
        "h = _cohens_h(best['token_accuracy'], worst['token_accuracy'])\n",
        "h_mag = (\"negligible\" if abs(h) < 0.2 else\n",
        "         \"small\" if abs(h) < 0.5 else\n",
        "         \"medium\" if abs(h) < 0.8 else \"large\")\n",
        "\n",
        "best_acc = best['token_accuracy']\n",
        "best_lo  = best['token_ci_lower']\n",
        "best_hi  = best['token_ci_upper']\n",
        "best_perfect = best['perfect_rate']\n",
        "best_perfect_hi = best['perfect_ci_upper']\n",
        "\n",
        "# ---------------- Pairwise Newcombe–Wilson diffs -----------------\n",
        "if 'newcombe_wilson_diff' not in globals():\n",
        "    def wilson_interval(successes, n, confidence=0.95):\n",
        "        if n == 0: return (0.0, 0.0, 0.0)\n",
        "        z = stats.norm.ppf(1 - (1 - confidence)/2)\n",
        "        p = successes / n\n",
        "        denom = 1 + z*z/n\n",
        "        centre = (p + z*z/(2*n)) / denom\n",
        "        half = z * np.sqrt((p*(1-p) + z*z/(4*n))/n) / denom\n",
        "        return (p, max(0.0, centre - half), min(1.0, centre + half))\n",
        "\n",
        "    def newcombe_wilson_diff(x1, n1, x2, n2, confidence=0.95):\n",
        "        p1, L1, U1 = wilson_interval(x1, n1, confidence)\n",
        "        p2, L2, U2 = wilson_interval(x2, n2, confidence)\n",
        "        diff = p1 - p2\n",
        "        lo = L1 - U2\n",
        "        hi = U1 - L2\n",
        "        return diff, lo, hi, (p1, L1, U1), (p2, L2, U2)\n",
        "\n",
        "pair_rows = []\n",
        "for i in range(len(tools_sorted)):\n",
        "    for j in range(i+1, len(tools_sorted)):\n",
        "        a, b = tools_sorted[i], tools_sorted[j]\n",
        "        A, B = _tool_stats[a], _tool_stats[b]\n",
        "        d, lo, hi, p1info, p2info = newcombe_wilson_diff(\n",
        "            A['correct_tokens'], A['total_tokens'], B['correct_tokens'], B['total_tokens']\n",
        "        )\n",
        "        pair_rows.append((a, b, d, lo, hi))\n",
        "\n",
        "# -------------- Optional: per-dataset CI table data ---------------\n",
        "have_ci_by_ds = _exists('ci_by_ds') and bool(ci_by_ds)\n",
        "if have_ci_by_ds:\n",
        "    # flatten for a small table\n",
        "    per_ds_rows = []\n",
        "    for ds, d in ci_by_ds.items():\n",
        "        for tool, vals in d.items():\n",
        "            per_ds_rows.append((ds, tool, vals['acc'], vals['lo'], vals['hi'], vals['tokens'], vals['sents']))\n",
        "    # sort by dataset then descending acc\n",
        "    per_ds_rows.sort(key=lambda x: (x[0], -x[2]))\n",
        "\n",
        "# -------------- Optional: McNemar outcomes table data -------------\n",
        "have_mcnemar = _exists('mcnemar_outcomes') and _exists('adj_pvals') and len(mcnemar_outcomes) == len(adj_pvals)\n",
        "if have_mcnemar:\n",
        "    # prepare rows: a, b, table, discordant, test, chi2, p, adj\n",
        "    mcn_rows = []\n",
        "    for o, adjp in zip(mcnemar_outcomes, adj_pvals):\n",
        "        a = o['tool_a']; b = o['tool_b']; t = o['table']\n",
        "        test_name = 'exact' if o.get('exact') else 'chi²'\n",
        "        mcn_rows.append((a, b, t, o['discordant'], test_name, o['chi2'], o['p_value'], float(adjp)))\n",
        "\n",
        "# -------------- Optional: CLAWS C7 UNK share & top unmappables ----\n",
        "have_strict_report = _exists('strict_report') and isinstance(strict_report, dict) and 'unk_rate' in strict_report\n",
        "unk_share_pct = f\"{strict_report['unk_rate']*100:.1f}%\" if have_strict_report else None\n",
        "top_unks = None\n",
        "if have_strict_report:\n",
        "    # top 10 unmappable CLAWS sources\n",
        "    from collections import Counter\n",
        "    cnts: Counter = strict_report.get('counts', Counter())\n",
        "    top_unks = cnts.most_common(10)\n",
        "\n",
        "# ---------------- Save figures to disk --------------------------------\n",
        "results_dir = \"nlp_validation_results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "print(f\"Saving visualizations to {results_dir}/ directory...\")\n",
        "\n",
        "saved = []\n",
        "def _save(fig_var, filename):\n",
        "    if _exists(fig_var):\n",
        "        globals()[fig_var].write_html(\n",
        "            f\"{results_dir}/{filename}\",\n",
        "            config={'displayModeBar': True, 'displaylogo': False}\n",
        "        )\n",
        "        saved.append(filename)\n",
        "\n",
        "# Existing\n",
        "_save('fig_ci',        \"confidence_intervals.html\")\n",
        "_save('fig_heatmap',   \"error_heatmap.html\")\n",
        "_save('fig_scatter',   \"sentence_length_analysis.html\")\n",
        "_save('fig_dist',      \"accuracy_distributions.html\")\n",
        "_save('fig',           \"accuracy_comparison.html\")\n",
        "\n",
        "# New visuals included earlier\n",
        "_save('fig_ci_ds',     \"per_dataset_confidence_intervals.html\")\n",
        "_save('fig_mcnemar',   \"mcnemar_composition.html\")\n",
        "_save('fig_f1',        \"per_tag_f1.html\")\n",
        "\n",
        "# ---------------- Build dashboard HTML ----------------------------\n",
        "now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "def _link(fn, label):\n",
        "    return f'<a href=\"{fn}\" class=\"chart-link\">{label}</a>\\n' if fn in saved else ''\n",
        "\n",
        "dashboard_html = f\"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <meta charset=\"utf-8\" />\n",
        "    <title>NLP POS Tagging Validation — Dubliners + BNC (STRICT)</title>\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
        "        .header {{ background-color: #2c3e50; color: white; padding: 20px; margin-bottom: 20px; border-radius: 6px; }}\n",
        "        .summary, .chart-container {{ background-color: white; padding: 20px; margin-bottom: 20px; border-radius: 6px; }}\n",
        "        .chart-link {{ display: inline-block; background-color: #3498db; color: white; padding: 10px 16px;\n",
        "                       text-decoration: none; border-radius: 5px; margin: 5px 8px 0 0; }}\n",
        "        .chart-link:hover {{ background-color: #2980b9; }}\n",
        "        .stats-table {{ width: 100%; border-collapse: collapse; margin-top: 10px; }}\n",
        "        .stats-table th, .stats-table td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "        .stats-table th {{ background-color: #f2f2f2; }}\n",
        "        .muted {{ color: #666; }}\n",
        "        .small {{ font-size: 0.95em; }}\n",
        "        code {{ background: #f0f0f0; padding: 1px 4px; border-radius: 3px; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\">\n",
        "        <h1>NLP POS Tagging Validation — Dubliners + BNC (STRICT)</h1>\n",
        "        <p>spaCy (sm, lg), Flair, and NLTK vs. CLAWS7 gold after strict CLAWS→Penn projection</p>\n",
        "        <p class=\"muted\">Analysis Date: {now_str}</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Executive Summary</h2>\n",
        "        <p><strong>Design:</strong> Token-level evaluation across {sentences_evaluated} sentences, pooled over Dubliners + BNC.</p>\n",
        "        <ul>\n",
        "            <li><strong>Best observed accuracy:</strong> {best_tool} = {best_acc:.3f} (95% CI {best_lo:.3f}–{best_hi:.3f}).</li>\n",
        "            <li><strong>Perfect-sentence rate (illustrative):</strong> ≈ {best_perfect:.1%} (upper 95% CI ≈ {best_perfect_hi:.1%}).</li>\n",
        "            <li><strong>Best vs worst:</strong> z = {z_stat:.3f}, p = {p_val:.3g}; Cohen’s h = {h:.3f} ({h_mag}).</li>\n",
        "        </ul>\n",
        "        <p class=\"muted small\">Strict projection maps CLAWS distinctions lacking Penn equivalents to <code>UNK</code>, capping achievable accuracy.</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Token Accuracy (STRICT)</h2>\n",
        "        <table class=\"stats-table\">\n",
        "            <tr>\n",
        "                <th>Tool</th>\n",
        "                <th>Token Accuracy</th>\n",
        "                <th>95% CI Lower</th>\n",
        "                <th>95% CI Upper</th>\n",
        "                <th>Tokens</th>\n",
        "                <th>Sentences</th>\n",
        "                <th>Perfect Sentence Rate</th>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "\n",
        "for tool_name in tools_sorted:\n",
        "    s = _tool_stats[tool_name]\n",
        "    dashboard_html += f\"\"\"\n",
        "            <tr>\n",
        "                <td><strong>{tool_name}</strong></td>\n",
        "                <td>{s['token_accuracy']:.3f}</td>\n",
        "                <td>{s['token_ci_lower']:.3f}</td>\n",
        "                <td>{s['token_ci_upper']:.3f}</td>\n",
        "                <td>{s['total_tokens']:,}</td>\n",
        "                <td>{s['total_sentences']}</td>\n",
        "                <td>{s['perfect_rate']:.1%}</td>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "\n",
        "dashboard_html += \"\"\"\n",
        "        </table>\n",
        "    </div>\n",
        "\"\"\"\n",
        "\n",
        "# --------- Per-dataset CI table section (if available) ----------\n",
        "if have_ci_by_ds and per_ds_rows:\n",
        "    dashboard_html += \"\"\"\n",
        "    <div class=\"summary\">\n",
        "        <h2>Per-Dataset Token Accuracy (STRICT)</h2>\n",
        "        <table class=\"stats-table\">\n",
        "            <tr><th>Dataset</th><th>Tool</th><th>Accuracy</th><th>95% CI Lower</th><th>95% CI Upper</th><th>Tokens</th><th>Sentences</th></tr>\n",
        "    \"\"\"\n",
        "    for ds, tool, acc, lo, hi, toks, sents in per_ds_rows:\n",
        "        dashboard_html += f\"<tr><td>{ds}</td><td>{tool}</td><td>{acc:.3f}</td><td>{lo:.3f}</td><td>{hi:.3f}</td><td>{toks:,}</td><td>{sents}</td></tr>\\n\"\n",
        "    dashboard_html += \"\"\"\n",
        "        </table>\n",
        "        <p class=\"muted small\">Wilson score intervals per dataset; useful for checking consistency across Dubliners and BNC.</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# --------- Pairwise Newcombe–Wilson diffs table -----------------\n",
        "if pair_rows:\n",
        "    dashboard_html += \"\"\"\n",
        "    <div class=\"summary\">\n",
        "        <h2>Pairwise Accuracy Differences (Newcombe–Wilson 95% CIs)</h2>\n",
        "        <table class=\"stats-table\">\n",
        "            <tr><th>Comparison</th><th>Δ (A−B)</th><th>95% CI Lower</th><th>95% CI Upper</th><th>Interpretation</th></tr>\n",
        "    \"\"\"\n",
        "    for a, b, d, lo, hi in pair_rows:\n",
        "        interp = \"A>B (CI excludes 0)\" if lo > 0 else (\"B>A (CI excludes 0)\" if hi < 0 else \"No clear difference\")\n",
        "        dashboard_html += f\"<tr><td>{a} − {b}</td><td>{d:.3f}</td><td>{lo:.3f}</td><td>{hi:.3f}</td><td>{interp}</td></tr>\\n\"\n",
        "    dashboard_html += \"\"\"\n",
        "        </table>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# --------- McNemar outcomes summary (if available) --------------\n",
        "if have_mcnemar and mcn_rows:\n",
        "    dashboard_html += \"\"\"\n",
        "    <div class=\"summary\">\n",
        "        <h2>Paired Token Outcomes (McNemar)</h2>\n",
        "        <table class=\"stats-table\">\n",
        "            <tr>\n",
        "                <th>Pair</th>\n",
        "                <th>[both correct, A correct & B wrong]</th>\n",
        "                <th>[B correct & A wrong, both wrong]</th>\n",
        "                <th>Discordant</th>\n",
        "                <th>Test</th>\n",
        "                <th>χ²</th>\n",
        "                <th>p</th>\n",
        "                <th>Holm-adj p</th>\n",
        "            </tr>\n",
        "    \"\"\"\n",
        "    for a, b, t, disc, test_name, chi2, p, adjp in mcn_rows:\n",
        "        t1 = f\"[{t[0][0]}, {t[0][1]}]\"\n",
        "        t2 = f\"[{t[1][0]}, {t[1][1]}]\"\n",
        "        dashboard_html += f\"<tr><td>{a} vs {b}</td><td>{t1}</td><td>{t2}</td><td>{disc}</td><td>{test_name}</td><td>{chi2:.3f}</td><td>{p:.3g}</td><td>{adjp:.3g}</td></tr>\\n\"\n",
        "    dashboard_html += \"\"\"\n",
        "        </table>\n",
        "        <p class=\"muted small\">“Only A correct” and “Only B correct” are the discordant cells that drive McNemar’s test.</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# --------- CLAWS C7 note + live UNK share -----------------------\n",
        "dashboard_html += \"\"\"\n",
        "    <div class=\"summary\">\n",
        "        <h2>About the CLAWS C7 Tagset</h2>\n",
        "        <p>CLAWS C7 encodes fine-grained morphosyntactic distinctions not present in Penn Treebank tags, including article subtypes (<code>AT</code>, <code>AT1</code>), rich pronoun categories by person/number/case (e.g., <code>PPHS1</code> vs <code>PPIS1</code>), auxiliary identity (<code>VB</code>/<code>VH</code>/<code>VD</code> families), preposition subtypes (<code>IO</code>, <code>IW</code>), and semantic noun subclasses (e.g., months/days as <code>NPM*</code>/<code>NPD*</code>). Under <em>strict</em> projection, distinctions that lack Penn equivalents are mapped to <code>UNK</code> to ensure label comparability with modern taggers.</p>\n",
        "\"\"\"\n",
        "\n",
        "if have_strict_report and unk_share_pct:\n",
        "    dashboard_html += f\"\"\"\n",
        "        <p><strong>UNK share under strict projection:</strong> {unk_share_pct} of gold tokens.</p>\n",
        "    \"\"\"\n",
        "    if top_unks:\n",
        "        dashboard_html += \"\"\"\n",
        "        <table class=\"stats-table\">\n",
        "            <tr><th>Top unmappable CLAWS tags</th><th>Count</th></tr>\n",
        "        \"\"\"\n",
        "        for tag, c in top_unks:\n",
        "            dashboard_html += f\"<tr><td>{tag}</td><td>{c}</td></tr>\\n\"\n",
        "        dashboard_html += \"</table>\\n\"\n",
        "\n",
        "dashboard_html += \"\"\"\n",
        "        <p class=\"muted small\">This preserves a fair evaluation against Penn-style outputs but imposes a ceiling on achievable accuracy.</p>\n",
        "    </div>\n",
        "\"\"\"\n",
        "\n",
        "# --------- Visual links (no extra autodiscovery) -----------------\n",
        "dashboard_html += \"\"\"\n",
        "    <div class=\"chart-container\">\n",
        "        <h2>Interactive Visualizations</h2>\n",
        "        <p>Click to open:</p>\n",
        "\"\"\"\n",
        "dashboard_html += _link(\"confidence_intervals.html\",            \"Overall CIs\")\n",
        "dashboard_html += _link(\"per_dataset_confidence_intervals.html\", \"Per-dataset CIs\")\n",
        "dashboard_html += _link(\"mcnemar_composition.html\",             \"McNemar Composition\")\n",
        "dashboard_html += _link(\"per_tag_f1.html\",                      \"Per-tag F1 (Top labels)\")\n",
        "dashboard_html += _link(\"error_heatmap.html\",                   \"Error Heatmap\")\n",
        "dashboard_html += _link(\"sentence_length_analysis.html\",        \"Length vs Accuracy\")\n",
        "dashboard_html += _link(\"accuracy_distributions.html\",          \"Accuracy Distributions\")\n",
        "dashboard_html += _link(\"accuracy_comparison.html\",             \"Tool Comparison\")\n",
        "dashboard_html += \"\"\"\n",
        "    </div>\n",
        "\"\"\"\n",
        "\n",
        "# --------- Expanded Statistical Methods -------------------------\n",
        "dashboard_html += \"\"\"\n",
        "    <div class=\"summary\">\n",
        "        <h2>Statistical Methods</h2>\n",
        "        <ul class=\"small\">\n",
        "            <li><strong>Strict CLAWS→Penn projection:</strong> Gold CLAWS7 tags are mapped to Penn tags; distinctions without Penn equivalents are assigned <code>UNK</code> to maintain label comparability with modern taggers.</li>\n",
        "            <li><strong>Accuracy CIs:</strong> Wilson score intervals for single proportions (token accuracy; perfect-sentence rate). Preferred over Wald due to better coverage, especially away from 0.5.</li>\n",
        "            <li><strong>Pairwise accuracy differences:</strong> Newcombe’s Method 10 (1998) using Wilson intervals for each proportion; CIs reported for Δ = p<sub>A</sub> − p<sub>B</sub>.</li>\n",
        "            <li><strong>Best vs worst significance:</strong> Pooled two-proportion z-test plus effect size Cohen’s h for magnitude (negligible/small/medium/large).</li>\n",
        "            <li><strong>Paired disagreements:</strong> McNemar’s test on discordant token outcomes (exact test when discordant &lt; 25, otherwise continuity-corrected χ²), with Holm–Bonferroni correction for multiple comparisons.</li>\n",
        "            <li><strong>Bootstrap by sentence:</strong> Cluster bootstrap (resampling sentences) for 95% CIs on accuracy differences, preserving within-sentence token dependence.</li>\n",
        "            <li><strong>F1 reporting:</strong> Micro, macro, and weighted F1 computed on Penn labels; per-tag F1 reported for the top gold labels (excluding <code>UNK</code> for readability).</li>\n",
        "            <li><strong>Alignment:</strong> Token comparisons use sentence-internal minimum alignment length across gold/tool tokens to avoid spurious misalignments from tokenizer differences.</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# ---------------- Write dashboard & zip --------------------------\n",
        "results_dir = \"nlp_validation_results\"\n",
        "with open(f\"{results_dir}/index.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(dashboard_html)\n",
        "\n",
        "zip_filename = f\"nlp_validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    for root, dirs, files in os.walk(results_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, results_dir)\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(\"Dashboard updated and all visualizations saved!\")\n",
        "print(f\"Files saved to: {results_dir}/\")\n",
        "print(f\"Zip archive created: {zip_filename}\")\n",
        "\n",
        "# If on Colab, prompt download\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "    colab_files.download(zip_filename)\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "K6pPxv9-feBX",
        "outputId": "5245f5a5-fb64-448e-acf1-60b52fc6c142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving visualizations to nlp_validation_results/ directory...\n",
            "Dashboard updated and all visualizations saved!\n",
            "Files saved to: nlp_validation_results/\n",
            "Zip archive created: nlp_validation_results_20250824_191116.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9e69f8b8-5c27-49e1-ae1c-eff8a1a79288\", \"nlp_validation_results_20250824_191116.zip\", 36624299)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}