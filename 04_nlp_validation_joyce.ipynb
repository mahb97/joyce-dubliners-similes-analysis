{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/joyce-dubliners-similes-analysis/blob/main/04_nlp_validation_joyce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# NLP Tagging Validation for Joyce's Dubliners\n",
        "\n",
        "This notebook validates modern NLP POS tagging tools against expert CLAWS7 annotations for simile sentences from James Joyce's *Dubliners*.\n",
        "\n",
        "## Research Objectives\n",
        "- Compare accuracy of spaCy, NLTK, Flair, Stanza, TextBlob against CLAWS7 annotations\n",
        "- Identify systematic tagging errors in literary text processing\n",
        "- Analyze Joyce-specific linguistic challenges for computational tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SETUP AND INSTALLATION\n",
        "# ==============================================================================\n",
        "\n",
        "# Install packages\n",
        "!pip install -q spacy nltk flair textblob scikit-learn plotly seaborn\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "print(\"Setup complete!\")"
      ],
      "metadata": {
        "id": "6TvxWAx_Fsdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# IMPORTS\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP libraries\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Analysis libraries\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Visualization libraries\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "print(\"All libraries imported!\")\n"
      ],
      "metadata": {
        "id": "H0mtWSZiF6MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# DATA UPLOAD AND PROCESSING\n",
        "# ==============================================================================\n",
        "\n",
        "# Upload file\n",
        "from google.colab import files\n",
        "print(\"Upload your CSV file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load data\n",
        "csv_filename = list(uploaded.keys())[0]\n",
        "# Try reading with different encodings\n",
        "try:\n",
        "    df = pd.read_csv(csv_filename, encoding='cp1252')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Could not decode with cp1252. Trying with latin1...\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_filename, encoding='latin1')\n",
        "    except UnicodeDecodeError:\n",
        "        print(\"Could not decode with latin1. Trying with utf-8...\")\n",
        "        df = pd.read_csv(csv_filename, encoding='utf-8') # Fallback to utf-8\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(df)} rows with columns: {list(df.columns)}\")\n",
        "\n",
        "# Process CLAWS data\n",
        "def parse_claws_tags(claws_string):\n",
        "    \"\"\"Parse CLAWS7 format: 'word_TAG word_TAG ...'\"\"\"\n",
        "    if pd.isna(claws_string) or not claws_string.strip():\n",
        "        return [], []\n",
        "\n",
        "    tokens = []\n",
        "    tags = []\n",
        "\n",
        "    for item in claws_string.strip().split():\n",
        "        if '_' in item:\n",
        "            parts = item.rsplit('_', 1)\n",
        "            if len(parts) == 2:\n",
        "                word, tag = parts\n",
        "                tokens.append(word)\n",
        "                tags.append(tag)\n",
        "\n",
        "    return tokens, tags\n",
        "\n",
        "# Process all sentences\n",
        "processed_data = []\n",
        "# Ensure 'Sentence Context' is used as the column name\n",
        "clean_df = df[['Sentence Context', 'CLAWS']].dropna()\n",
        "\n",
        "for idx, row in clean_df.iterrows():\n",
        "    tokens, tags = parse_claws_tags(row['CLAWS'])\n",
        "    if tokens and tags and len(tokens) == len(tags):\n",
        "        processed_data.append({\n",
        "            'sentence': row['Sentence Context'],\n",
        "            'tokens': tokens,\n",
        "            'claws_tags': tags\n",
        "        })\n",
        "\n",
        "print(f\"Processed {len(processed_data)} valid sentences\")\n",
        "if processed_data:\n",
        "    print(f\"Sample: {processed_data[0]['sentence'][:60]}...\")"
      ],
      "metadata": {
        "id": "dZ4u1hF9JgtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# NLP TOOL SETUP\n",
        "# ==============================================================================\n",
        "\n",
        "# Initialize NLP tools\n",
        "print(\"Loading NLP models...\")\n",
        "\n",
        "# Load spaCy\n",
        "try:\n",
        "    nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "    nlp_lg = spacy.load(\"en_core_web_lg\")\n",
        "    print(\"spaCy models loaded\")\n",
        "except:\n",
        "    print(\"spaCy models failed to load\")\n",
        "    nlp_sm = nlp_lg = None\n",
        "\n",
        "# Load Flair\n",
        "try:\n",
        "    flair_tagger = SequenceTagger.load('pos')\n",
        "    print(\"Flair model loaded\")\n",
        "except:\n",
        "    print(\"Flair model failed to load\")\n",
        "    flair_tagger = None\n",
        "\n",
        "print(\"NLP tools ready!\")"
      ],
      "metadata": {
        "id": "fnIUwpzeJ5w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PENN TREEBANK TO CLAWS7 MAPPING\n",
        "# ==============================================================================\n",
        "\n",
        "# Basic Penn Treebank to CLAWS7 mapping\n",
        "penn_to_claws = {\n",
        "    # Nouns\n",
        "    'NN': 'NN1', 'NNS': 'NN2', 'NNP': 'NP1', 'NNPS': 'NP2',\n",
        "\n",
        "    # Verbs\n",
        "    'VB': 'VV0', 'VBD': 'VVD', 'VBG': 'VVG', 'VBN': 'VVN',\n",
        "    'VBP': 'VV0', 'VBZ': 'VVZ',\n",
        "\n",
        "    # Pronouns\n",
        "    'PRP': 'PPIS1', 'PRP$': 'APPGE', 'WP': 'PNQS',\n",
        "\n",
        "    # Determiners\n",
        "    'DT': 'AT', 'WDT': 'DDQ',\n",
        "\n",
        "    # Adjectives\n",
        "    'JJ': 'JJ', 'JJR': 'JJR', 'JJS': 'JJT',\n",
        "\n",
        "    # Adverbs\n",
        "    'RB': 'RR', 'RBR': 'RRR', 'RBS': 'RRT', 'WRB': 'RRQ',\n",
        "\n",
        "    # Prepositions\n",
        "    'IN': 'II', 'TO': 'TO',\n",
        "\n",
        "    # Conjunctions\n",
        "    'CC': 'CC',\n",
        "\n",
        "    # Others\n",
        "    'CD': 'MC', 'MD': 'VM', 'EX': 'EX', 'FW': 'FW', 'UH': 'UH',\n",
        "    '.': '.', ',': ',', ':': ':', ';': ';', '!': '!', '?': '?'\n",
        "}\n",
        "\n",
        "# Context-specific mappings for key Joyce words\n",
        "def convert_to_claws(token, penn_tag):\n",
        "    \"\"\"Convert Penn tag to CLAWS7 with context awareness\"\"\"\n",
        "    token_lower = token.lower()\n",
        "\n",
        "    # Handle auxiliary verbs\n",
        "    aux_verbs = {\n",
        "        'am': 'VBM', 'is': 'VBZ', 'are': 'VBR', 'was': 'VBDZ', 'were': 'VBDR',\n",
        "        'be': 'VBI', 'been': 'VBN', 'being': 'VBG',\n",
        "        'has': 'VHZ', 'have': 'VH0', 'had': 'VHD',\n",
        "        'do': 'VD0', 'does': 'VDZ', 'did': 'VDD'\n",
        "    }\n",
        "\n",
        "    if token_lower in aux_verbs:\n",
        "        return aux_verbs[token_lower]\n",
        "\n",
        "    # Handle articles\n",
        "    if token_lower in ['a', 'an']:\n",
        "        return 'AT1'\n",
        "    elif token_lower == 'the':\n",
        "        return 'AT'\n",
        "\n",
        "    # Handle negation\n",
        "    if token_lower in ['not', \"n't\"]:\n",
        "        return 'XX'\n",
        "\n",
        "    # Handle key Joyce words\n",
        "    if token_lower == 'like':\n",
        "        return 'II' if penn_tag == 'IN' else 'VV0'\n",
        "    elif token_lower == 'as':\n",
        "        return 'CSA'\n",
        "    elif token_lower == 'if':\n",
        "        return 'CS'\n",
        "    elif token_lower == 'that':\n",
        "        return 'CST' if penn_tag in ['IN', 'WDT'] else 'DD1'\n",
        "\n",
        "    # Default mapping\n",
        "    return penn_to_claws.get(penn_tag, penn_tag)\n",
        "\n",
        "print(\"CLAWS7 mapping system ready!\")\n"
      ],
      "metadata": {
        "id": "wQCMbYNIKRrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# NLP TAGGING FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def tag_with_spacy(sentence, model='sm'):\n",
        "    \"\"\"Tag sentence with spaCy\"\"\"\n",
        "    nlp_model = nlp_sm if model == 'sm' else nlp_lg\n",
        "    if nlp_model is None:\n",
        "        return []\n",
        "\n",
        "    doc = nlp_model(sentence)\n",
        "    return [(token.text, convert_to_claws(token.text, token.tag_)) for token in doc]\n",
        "\n",
        "def tag_with_nltk(sentence):\n",
        "    \"\"\"Tag sentence with NLTK\"\"\"\n",
        "    tokens = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    return [(word, convert_to_claws(word, tag)) for word, tag in pos_tags]\n",
        "\n",
        "def tag_with_flair(sentence):\n",
        "    \"\"\"Tag sentence with Flair\"\"\"\n",
        "    if flair_tagger is None:\n",
        "        return []\n",
        "\n",
        "    flair_sentence = Sentence(sentence)\n",
        "    flair_tagger.predict(flair_sentence)\n",
        "    return [(token.text, convert_to_claws(token.text, token.tag)) for token in flair_sentence]\n",
        "\n",
        "def tag_with_textblob(sentence):\n",
        "    \"\"\"Tag sentence with TextBlob\"\"\"\n",
        "    blob = TextBlob(sentence)\n",
        "    return [(word, convert_to_claws(word, tag)) for word, tag in blob.tags]\n",
        "\n",
        "print(\"Tagging functions ready!\")"
      ],
      "metadata": {
        "id": "xQ_VzXpxKVz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 7: BATCH PROCESSING\n",
        "# ==============================================================================\n",
        "\n",
        "def process_sentence_with_all_tools(sentence):\n",
        "    \"\"\"Process one sentence with all NLP tools\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    tools = {\n",
        "        'spacy_sm': lambda s: tag_with_spacy(s, 'sm'),\n",
        "        'spacy_lg': lambda s: tag_with_spacy(s, 'lg'),\n",
        "        'nltk': tag_with_nltk,\n",
        "        'flair': tag_with_flair,\n",
        "        'textblob': tag_with_textblob\n",
        "    }\n",
        "\n",
        "    for tool_name, tool_func in tools.items():\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            tagged = tool_func(sentence)\n",
        "            processing_time = time.time() - start_time\n",
        "\n",
        "            results[tool_name] = {\n",
        "                'tags': [tag for word, tag in tagged],\n",
        "                'tokens': [word for word, tag in tagged],\n",
        "                'processing_time': processing_time\n",
        "            }\n",
        "        except Exception as e:\n",
        "            results[tool_name] = {'error': str(e)}\n",
        "\n",
        "    return results\n",
        "\n",
        "# Process all sentences\n",
        "print(f\"Processing {len(processed_data)} sentences...\")\n",
        "batch_results = []\n",
        "\n",
        "for i, data in enumerate(processed_data):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Progress: {i}/{len(processed_data)}\")\n",
        "\n",
        "    sentence = data['sentence']\n",
        "    ground_truth = data['claws_tags']\n",
        "\n",
        "    tool_results = process_sentence_with_all_tools(sentence)\n",
        "\n",
        "    batch_results.append({\n",
        "        'sentence': sentence,\n",
        "        'ground_truth': ground_truth,\n",
        "        'tool_results': tool_results\n",
        "    })\n",
        "\n",
        "print(\"Batch processing complete!\")"
      ],
      "metadata": {
        "id": "kE88PHThKbYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ACCURACY EVALUATION\n",
        "# ==============================================================================\n",
        "\n",
        "def calculate_accuracy(ground_truth, predicted):\n",
        "    \"\"\"Calculate accuracy between two tag sequences\"\"\"\n",
        "    if not ground_truth or not predicted:\n",
        "        return 0.0\n",
        "\n",
        "    min_len = min(len(ground_truth), len(predicted))\n",
        "    if min_len == 0:\n",
        "        return 0.0\n",
        "\n",
        "    correct = sum(1 for i in range(min_len) if ground_truth[i] == predicted[i])\n",
        "    return correct / min_len\n",
        "\n",
        "# Evaluate each tool\n",
        "tool_performance = defaultdict(list)\n",
        "\n",
        "for result in batch_results:\n",
        "    ground_truth = result['ground_truth']\n",
        "\n",
        "    for tool_name, tool_result in result['tool_results'].items():\n",
        "        if 'error' not in tool_result:\n",
        "            predicted = tool_result['tags']\n",
        "            accuracy = calculate_accuracy(ground_truth, predicted)\n",
        "            tool_performance[tool_name].append(accuracy)\n",
        "\n",
        "# Calculate summary statistics\n",
        "performance_summary = {}\n",
        "for tool_name, accuracies in tool_performance.items():\n",
        "    if accuracies:\n",
        "        performance_summary[tool_name] = {\n",
        "            'mean_accuracy': np.mean(accuracies),\n",
        "            'std_accuracy': np.std(accuracies),\n",
        "            'total_sentences': len(accuracies),\n",
        "            'perfect_sentences': sum(1 for acc in accuracies if acc == 1.0)\n",
        "        }\n",
        "\n",
        "print(\"Performance Summary:\")\n",
        "for tool, stats in performance_summary.items():\n",
        "    perfect_rate = stats['perfect_sentences'] / stats['total_sentences']\n",
        "    print(f\"{tool}: {stats['mean_accuracy']:.3f} ± {stats['std_accuracy']:.3f} \"\n",
        "          f\"({perfect_rate:.1%} perfect)\")"
      ],
      "metadata": {
        "id": "3nyRxQymK_Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ERROR ANALYSIS\n",
        "# ==============================================================================\n",
        "\n",
        "# Collect errors\n",
        "error_analysis = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "for result in batch_results:\n",
        "    ground_truth = result['ground_truth']\n",
        "\n",
        "    for tool_name, tool_result in result['tool_results'].items():\n",
        "        if 'error' not in tool_result:\n",
        "            predicted = tool_result['tags']\n",
        "\n",
        "            min_len = min(len(ground_truth), len(predicted))\n",
        "            for i in range(min_len):\n",
        "                if ground_truth[i] != predicted[i]:\n",
        "                    error_pattern = f\"{ground_truth[i]}->{predicted[i]}\"\n",
        "                    error_analysis[tool_name][error_pattern].append({\n",
        "                        'sentence': result['sentence'],\n",
        "                        'position': i\n",
        "                    })\n",
        "\n",
        "# Show most common errors\n",
        "print(\"Most Common Error Patterns:\")\n",
        "for tool_name, errors in error_analysis.items():\n",
        "    print(f\"\\n{tool_name}:\")\n",
        "    sorted_errors = sorted(errors.items(), key=lambda x: len(x[1]), reverse=True)[:3]\n",
        "    for pattern, error_list in sorted_errors:\n",
        "        print(f\"  {pattern}: {len(error_list)} occurrences\")"
      ],
      "metadata": {
        "id": "PMkDMsoKLBi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# WILSON CONFIDENCE INTERVALS\n",
        "# ==============================================================================\n",
        "\n",
        "import scipy.stats as stats\n",
        "from math import sqrt\n",
        "\n",
        "def wilson_confidence_interval(successes, trials, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculate Wilson confidence interval for binomial proportion\n",
        "    More robust than normal approximation, especially for small samples\n",
        "    \"\"\"\n",
        "    if trials == 0:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    p = successes / trials\n",
        "    z = stats.norm.ppf(1 - (1 - confidence) / 2)  # z-score for confidence level\n",
        "\n",
        "    # Wilson interval calculation\n",
        "    denominator = 1 + z**2 / trials\n",
        "    centre = (p + z**2 / (2 * trials)) / denominator\n",
        "    half_width = z * sqrt((p * (1 - p) + z**2 / (4 * trials)) / trials) / denominator\n",
        "\n",
        "    lower = max(0, centre - half_width)\n",
        "    upper = min(1, centre + half_width)\n",
        "\n",
        "    return p, lower, upper\n",
        "\n",
        "# Calculate Wilson intervals for each tool\n",
        "print(\"Tool Performance with Wilson 95% Confidence Intervals:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "wilson_results = {}\n",
        "\n",
        "for tool_name, stats_data in performance_summary.items():\n",
        "    total_sentences = stats_data['total_sentences']\n",
        "    perfect_sentences = stats_data['perfect_sentences']\n",
        "\n",
        "    # Wilson interval for perfect sentence rate\n",
        "    perfect_rate, perfect_lower, perfect_upper = wilson_confidence_interval(\n",
        "        perfect_sentences, total_sentences\n",
        "    )\n",
        "\n",
        "    # For overall accuracy, we need to calculate total correct tokens\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "\n",
        "    for result in batch_results:\n",
        "        ground_truth = result['ground_truth']\n",
        "        tool_result = result['tool_results'].get(tool_name, {})\n",
        "\n",
        "        if 'error' not in tool_result:\n",
        "            predicted = tool_result['tags']\n",
        "            min_len = min(len(ground_truth), len(predicted))\n",
        "            total_tokens += min_len\n",
        "            correct_tokens += sum(1 for i in range(min_len)\n",
        "                                if ground_truth[i] == predicted[i])\n",
        "\n",
        "    # Wilson interval for token-level accuracy\n",
        "    token_accuracy, token_lower, token_upper = wilson_confidence_interval(\n",
        "        correct_tokens, total_tokens\n",
        "    )\n",
        "\n",
        "    wilson_results[tool_name] = {\n",
        "        'token_accuracy': token_accuracy,\n",
        "        'token_ci_lower': token_lower,\n",
        "        'token_ci_upper': token_upper,\n",
        "        'perfect_rate': perfect_rate,\n",
        "        'perfect_ci_lower': perfect_lower,\n",
        "        'perfect_ci_upper': perfect_upper,\n",
        "        'total_tokens': total_tokens,\n",
        "        'correct_tokens': correct_tokens\n",
        "    }\n",
        "\n",
        "    print(f\"\\\\n{tool_name.upper()}:\")\n",
        "    print(f\"  Token Accuracy: {token_accuracy:.3f} [{token_lower:.3f}, {token_upper:.3f}]\")\n",
        "    print(f\"  Perfect Sentences: {perfect_rate:.3f} [{perfect_lower:.3f}, {perfect_upper:.3f}]\")\n",
        "    print(f\"  Sample size: {total_tokens:,} tokens, {total_sentences} sentences\")\n",
        "\n",
        "# Statistical significance testing between tools\n",
        "print(\"\\\\n\" + \"=\" * 60)\n",
        "print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def proportion_z_test(x1, n1, x2, n2):\n",
        "    \"\"\"Two-proportion z-test\"\"\"\n",
        "    p1 = x1 / n1\n",
        "    p2 = x2 / n2\n",
        "    p_pool = (x1 + x2) / (n1 + n2)\n",
        "\n",
        "    se = sqrt(p_pool * (1 - p_pool) * (1/n1 + 1/n2))\n",
        "    z = (p1 - p2) / se\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "\n",
        "    return z, p_value\n",
        "\n",
        "# Compare best vs worst performing tools\n",
        "tools_by_accuracy = sorted(wilson_results.items(),\n",
        "                          key=lambda x: x[1]['token_accuracy'],\n",
        "                          reverse=True)\n",
        "\n",
        "best_tool, best_stats = tools_by_accuracy[0]\n",
        "worst_tool, worst_stats = tools_by_accuracy[-1]\n",
        "\n",
        "z_stat, p_value = proportion_z_test(\n",
        "    best_stats['correct_tokens'], best_stats['total_tokens'],\n",
        "    worst_stats['correct_tokens'], worst_stats['total_tokens']\n",
        ")\n",
        "\n",
        "print(f\"Comparison: {best_tool} vs {worst_tool}\")\n",
        "print(f\"Accuracy difference: {best_stats['token_accuracy'] - worst_stats['token_accuracy']:.3f}\")\n",
        "print(f\"Z-statistic: {z_stat:.3f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n",
        "print(f\"Significant at α=0.05: {'Yes' if p_value < 0.05 else 'No'}\")\n",
        "\n",
        "# Effect size (Cohen's h for proportions)\n",
        "def cohens_h(p1, p2):\n",
        "    \"\"\"Cohen's h effect size for proportions\"\"\"\n",
        "    return 2 * (np.arcsin(sqrt(p1)) - np.arcsin(sqrt(p2)))\n",
        "\n",
        "effect_size = cohens_h(best_stats['token_accuracy'], worst_stats['token_accuracy'])\n",
        "print(f\"Effect size (Cohen's h): {effect_size:.3f}\")\n",
        "\n",
        "if abs(effect_size) < 0.2:\n",
        "    magnitude = \"negligible\"\n",
        "elif abs(effect_size) < 0.5:\n",
        "    magnitude = \"small\"\n",
        "elif abs(effect_size) < 0.8:\n",
        "    magnitude = \"medium\"\n",
        "else:\n",
        "    magnitude = \"large\"\n",
        "\n",
        "print(f\"Effect magnitude: {magnitude}\")"
      ],
      "metadata": {
        "id": "5h6O1V1cO-Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Critical Analysis: Statistical Evidence for Computational Literary Studies Limitations\n",
        "\n",
        "These Wilson confidence interval results provide compelling empirical evidence for the methodological concerns raised in computational literary studies debates. With token-level accuracies ranging from 58.2% [56.8%, 59.5%] to 63.0% [61.7%, 64.3%] across 5,437 tokens, modern NLP tools demonstrate systematic underperformance on Joyce's literary prose compared to their claimed 95%+ accuracy on standard text. The extraordinarily low perfect sentence rates (1.1-1.6%) with wide confidence intervals [0.3%, 4.7%] reveal that flawless automated tagging of Joyce's syntactically complex sentences is statistically rare, occurring in fewer than 1 in 20 cases. While Flair's superiority over spaCy achieves statistical significance (p < 0.001), the negligible effect size (Cohen's h = 0.100) demonstrates that technological improvements yield practically minimal gains when confronting modernist literary language. This statistical validation supports Da's critique that computational literary analysis faces fundamental limitations with complex literary texts, while simultaneously validating Wallis's methodological framework for robust corpus linguistic research. The consistent underperformance across all tools, despite their neural architectures and contextual embeddings, suggests that Joyce's stylistic innovations create systematic challenges for automated linguistic analysis that transcend individual algorithmic approaches. These findings provide quantitative evidence that expert linguistic annotation remains essential for literary corpus analysis, particularly when dealing with texts that deliberately exploit syntactic ambiguity and"
      ],
      "metadata": {
        "id": "UxXdaAtEPWKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# VISUALIZATIONS\n",
        "# ==============================================================================\n",
        "\n",
        "# Create accuracy comparison chart\n",
        "tools = list(performance_summary.keys())\n",
        "accuracies = [performance_summary[tool]['mean_accuracy'] for tool in tools]\n",
        "std_devs = [performance_summary[tool]['std_accuracy'] for tool in tools]\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Bar(\n",
        "    x=tools,\n",
        "    y=accuracies,\n",
        "    error_y=dict(type='data', array=std_devs),\n",
        "    text=[f\"{acc:.3f}\" for acc in accuracies],\n",
        "    textposition='auto'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"NLP Tool Accuracy on Joyce's Dubliners\",\n",
        "    xaxis_title=\"NLP Tools\",\n",
        "    yaxis_title=\"Mean Accuracy\",\n",
        "    yaxis=dict(range=[0, 1])\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Perfect sentence rates\n",
        "perfect_rates = [performance_summary[tool]['perfect_sentences'] /\n",
        "                performance_summary[tool]['total_sentences'] for tool in tools]\n",
        "\n",
        "fig2 = go.Figure()\n",
        "fig2.add_trace(go.Bar(\n",
        "    x=tools,\n",
        "    y=perfect_rates,\n",
        "    text=[f\"{rate:.1%}\" for rate in perfect_rates],\n",
        "    textposition='auto'\n",
        "))\n",
        "\n",
        "fig2.update_layout(\n",
        "    title=\"Perfect Sentence Tagging Rates\",\n",
        "    xaxis_title=\"NLP Tools\",\n",
        "    yaxis_title=\"Percentage Perfect\",\n",
        "    yaxis=dict(range=[0, 1])\n",
        ")\n",
        "\n",
        "fig2.show()\n",
        "\n",
        "print(\"Analysis complete!\")\n"
      ],
      "metadata": {
        "id": "bF3gvFhrLLlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ADDITIONAL DATA VISUALIZATIONS\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. CONFIDENCE INTERVAL COMPARISON (Much better than tiny bar chart)\n",
        "fig_ci = go.Figure()\n",
        "\n",
        "for tool_name, stats in wilson_results.items():\n",
        "    # Main accuracy point\n",
        "    fig_ci.add_trace(go.Scatter(\n",
        "        x=[stats['token_accuracy']],\n",
        "        y=[tool_name],\n",
        "        mode='markers',\n",
        "        marker=dict(size=12, color='darkblue'),\n",
        "        name=tool_name,\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "    # Confidence interval line\n",
        "    fig_ci.add_trace(go.Scatter(\n",
        "        x=[stats['token_ci_lower'], stats['token_ci_upper']],\n",
        "        y=[tool_name, tool_name],\n",
        "        mode='lines',\n",
        "        line=dict(color='darkblue', width=3),\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "fig_ci.update_layout(\n",
        "    title=\"NLP Tool Accuracy with 95% Wilson Confidence Intervals<br><sub>On Joyce's Dubliners Simile Sentences</sub>\",\n",
        "    xaxis_title=\"Token-Level Accuracy\",\n",
        "    yaxis_title=\"NLP Tools\",\n",
        "    xaxis=dict(range=[0.5, 0.7], tickformat='.1%'),\n",
        "    height=400,\n",
        "    annotations=[\n",
        "        dict(x=0.52, y=-0.15, xref='x', yref='paper',\n",
        "             text=\"Error bars show 95% Wilson confidence intervals\",\n",
        "             showarrow=False, font=dict(size=10))\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig_ci.show()\n",
        "\n",
        "# 2. ERROR HEATMAP - Shows which CLAWS7 tags are most problematic\n",
        "from collections import defaultdict\n",
        "\n",
        "# Collect tag-level errors\n",
        "tag_errors = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for result in batch_results:\n",
        "    ground_truth = result['ground_truth']\n",
        "\n",
        "    for tool_name, tool_result in result['tool_results'].items():\n",
        "        if 'error' not in tool_result:\n",
        "            predicted = tool_result['tags']\n",
        "            min_len = min(len(ground_truth), len(predicted))\n",
        "\n",
        "            for i in range(min_len):\n",
        "                if ground_truth[i] != predicted[i]:\n",
        "                    tag_errors[ground_truth[i]][tool_name] += 1\n",
        "\n",
        "# Get most problematic tags\n",
        "tag_totals = {tag: sum(tool_errors.values()) for tag, tool_errors in tag_errors.items()}\n",
        "top_tags = sorted(tag_totals.items(), key=lambda x: x[1], reverse=True)[:12]\n",
        "\n",
        "# Create heatmap data\n",
        "heatmap_data = []\n",
        "tag_names = [tag for tag, _ in top_tags]\n",
        "tool_names = list(wilson_results.keys())\n",
        "\n",
        "for tag in tag_names:\n",
        "    row = [tag_errors[tag][tool] for tool in tool_names]\n",
        "    heatmap_data.append(row)\n",
        "\n",
        "fig_heatmap = go.Figure(data=go.Heatmap(\n",
        "    z=heatmap_data,\n",
        "    x=tool_names,\n",
        "    y=tag_names,\n",
        "    colorscale='Reds',\n",
        "    text=heatmap_data,\n",
        "    texttemplate=\"%{text}\",\n",
        "    textfont={\"size\": 10}\n",
        "))\n",
        "\n",
        "fig_heatmap.update_layout(\n",
        "    title=\"Most Problematic CLAWS7 Tags by Tool<br><sub>Error frequency heatmap</sub>\",\n",
        "    xaxis_title=\"NLP Tools\",\n",
        "    yaxis_title=\"CLAWS7 POS Tags\",\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig_heatmap.show()\n",
        "\n",
        "# 3. SENTENCE LENGTH vs ACCURACY SCATTER - Shows if Joyce's longer sentences are harder\n",
        "sentence_difficulties = []\n",
        "\n",
        "for result in batch_results:\n",
        "    sentence_length = len(result['ground_truth'])\n",
        "\n",
        "    for tool_name, tool_result in result['tool_results'].items():\n",
        "        if 'error' not in tool_result:\n",
        "            predicted = tool_result['tags']\n",
        "            accuracy = calculate_accuracy(result['ground_truth'], predicted)\n",
        "\n",
        "            sentence_difficulties.append({\n",
        "                'length': sentence_length,\n",
        "                'accuracy': accuracy,\n",
        "                'tool': tool_name\n",
        "            })\n",
        "\n",
        "df_scatter = pd.DataFrame(sentence_difficulties)\n",
        "\n",
        "fig_scatter = go.Figure()\n",
        "\n",
        "colors = {'spacy_sm': '#1f77b4', 'spacy_lg': '#ff7f0e', 'flair': '#2ca02c'}\n",
        "\n",
        "for tool in df_scatter['tool'].unique():\n",
        "    tool_data = df_scatter[df_scatter['tool'] == tool]\n",
        "\n",
        "    fig_scatter.add_trace(go.Scatter(\n",
        "        x=tool_data['length'],\n",
        "        y=tool_data['accuracy'],\n",
        "        mode='markers',\n",
        "        name=tool,\n",
        "        marker=dict(color=colors.get(tool, '#636EFA'), size=6, opacity=0.6)\n",
        "    ))\n",
        "\n",
        "# Add trend line\n",
        "z = np.polyfit(df_scatter['length'], df_scatter['accuracy'], 1)\n",
        "p = np.poly1d(z)\n",
        "x_trend = np.linspace(df_scatter['length'].min(), df_scatter['length'].max(), 100)\n",
        "\n",
        "fig_scatter.add_trace(go.Scatter(\n",
        "    x=x_trend,\n",
        "    y=p(x_trend),\n",
        "    mode='lines',\n",
        "    name='Trend',\n",
        "    line=dict(color='red', dash='dash')\n",
        "))\n",
        "\n",
        "fig_scatter.update_layout(\n",
        "    title=\"Sentence Length vs Tagging Accuracy<br><sub>Are Joyce's longer sentences harder to tag?</sub>\",\n",
        "    xaxis_title=\"Sentence Length (tokens)\",\n",
        "    yaxis_title=\"Accuracy\",\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig_scatter.show()\n",
        "\n",
        "# 4. DISTRIBUTION OF ACCURACIES - Shows the spread better than means\n",
        "fig_dist = go.Figure()\n",
        "\n",
        "for tool_name, accuracies in tool_performance.items():\n",
        "    fig_dist.add_trace(go.Box(\n",
        "        y=accuracies,\n",
        "        name=tool_name,\n",
        "        boxpoints='outliers'\n",
        "    ))\n",
        "\n",
        "fig_dist.update_layout(\n",
        "    title=\"Distribution of Sentence-Level Accuracies<br><sub>Box plots showing quartiles and outliers</sub>\",\n",
        "    xaxis_title=\"NLP Tools\",\n",
        "    yaxis_title=\"Sentence Accuracy\",\n",
        "    yaxis=dict(range=[0, 1], tickformat='.0%'),\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig_dist.show()\n",
        "\n",
        "print(\"Enhanced visualizations complete!\")"
      ],
      "metadata": {
        "id": "HAMJF0JcQUby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SAVE VISUALIZATIONS AS HTML FILES\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results directory\n",
        "results_dir = \"nlp_validation_results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Saving visualizations to {results_dir}/ directory...\")\n",
        "\n",
        "# 1. Save Confidence Interval Plot\n",
        "fig_ci.write_html(f\"{results_dir}/confidence_intervals.html\",\n",
        "                  config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# 2. Save Error Heatmap\n",
        "fig_heatmap.write_html(f\"{results_dir}/error_heatmap.html\",\n",
        "                       config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# 3. Save Scatter Plot\n",
        "fig_scatter.write_html(f\"{results_dir}/sentence_length_analysis.html\",\n",
        "                       config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# 4. Save Box Plot Distribution\n",
        "fig_dist.write_html(f\"{results_dir}/accuracy_distributions.html\",\n",
        "                    config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# 5. Save Original Accuracy Comparison (from earlier)\n",
        "fig.write_html(f\"{results_dir}/accuracy_comparison.html\",\n",
        "               config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# 6. Create a comprehensive dashboard HTML file\n",
        "dashboard_html = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>NLP Validation Results: Joyce's Dubliners</title>\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
        "        .header {{ background-color: #2c3e50; color: white; padding: 20px; margin-bottom: 20px; }}\n",
        "        .summary {{ background-color: white; padding: 20px; margin-bottom: 20px; border-radius: 5px; }}\n",
        "        .chart-container {{ background-color: white; margin-bottom: 20px; padding: 15px; border-radius: 5px; }}\n",
        "        .chart-link {{ display: inline-block; background-color: #3498db; color: white; padding: 10px 20px;\n",
        "                       text-decoration: none; border-radius: 5px; margin: 5px; }}\n",
        "        .chart-link:hover {{ background-color: #2980b9; }}\n",
        "        .stats-table {{ width: 100%; border-collapse: collapse; }}\n",
        "        .stats-table th, .stats-table td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "        .stats-table th {{ background-color: #f2f2f2; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\">\n",
        "        <h1>NLP Tagging Validation for Joyce's Dubliners</h1>\n",
        "        <p>Statistical Analysis of Modern NLP Tools vs Expert CLAWS7 Annotations</p>\n",
        "        <p>Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Executive Summary</h2>\n",
        "        <p><strong>Research Question:</strong> How accurately do modern NLP tools perform on James Joyce's syntactically complex literary prose?</p>\n",
        "        <p><strong>Methodology:</strong> Comparison of spaCy, Flair, and TextBlob against expert CLAWS7 annotations on {len(batch_results)} simile sentences from Dubliners.</p>\n",
        "\n",
        "        <h3>Key Findings:</h3>\n",
        "        <ul>\n",
        "            <li><strong>Low Overall Accuracy:</strong> Best tool (Flair) achieved only 63.0% token-level accuracy [61.7%, 64.3%]</li>\n",
        "            <li><strong>Rare Perfect Sentences:</strong> Only 1.1-1.6% of sentences tagged perfectly</li>\n",
        "            <li><strong>Systematic Underperformance:</strong> All tools significantly below claimed 95%+ accuracy on standard text</li>\n",
        "            <li><strong>Statistical Significance:</strong> Tool differences are significant but practically negligible (Cohen's h = 0.100)</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Statistical Results Summary</h2>\n",
        "        <table class=\"stats-table\">\n",
        "            <tr><th>Tool</th><th>Token Accuracy</th><th>95% CI Lower</th><th>95% CI Upper</th><th>Perfect Sentences</th></tr>\n",
        "\"\"\"\n",
        "\n",
        "for tool_name, stats in wilson_results.items():\n",
        "    dashboard_html += f\"\"\"\n",
        "            <tr>\n",
        "                <td><strong>{tool_name.upper()}</strong></td>\n",
        "                <td>{stats['token_accuracy']:.3f}</td>\n",
        "                <td>{stats['token_ci_lower']:.3f}</td>\n",
        "                <td>{stats['token_ci_upper']:.3f}</td>\n",
        "                <td>{stats['perfect_rate']:.1%}</td>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "\n",
        "dashboard_html += f\"\"\"\n",
        "        </table>\n",
        "        <p><strong>Sample Size:</strong> {wilson_results[list(wilson_results.keys())[0]]['total_tokens']:,} tokens across {len(batch_results)} sentences</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"chart-container\">\n",
        "        <h2>Interactive Visualizations</h2>\n",
        "        <p>Click on any chart below to open the full interactive version:</p>\n",
        "\n",
        "        <a href=\"confidence_intervals.html\" class=\"chart-link\"> Confidence Intervals</a>\n",
        "        <a href=\"error_heatmap.html\" class=\"chart-link\"> Error Heatmap</a>\n",
        "        <a href=\"sentence_length_analysis.html\" class=\"chart-link\"> Length vs Accuracy</a>\n",
        "        <a href=\"accuracy_distributions.html\" class=\"chart-link\"> Accuracy Distributions</a>\n",
        "        <a href=\"accuracy_comparison.html\" class=\"chart-link\"> Tool Comparison</a>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Critical Analysis</h2>\n",
        "        <p>These Wilson confidence interval results provide compelling empirical evidence for the methodological concerns raised in computational literary studies debates. With token-level accuracies ranging from 58.2% [56.8%, 59.5%] to 63.0% [61.7%, 64.3%] across 5,437 tokens, modern NLP tools demonstrate systematic underperformance on Joyce's literary prose compared to their claimed 95%+ accuracy on standard text. The extraordinarily low perfect sentence rates (1.1-1.6%) with wide confidence intervals [0.3%, 4.7%] reveal that flawless automated tagging of Joyce's syntactically complex sentences is statistically rare, occurring in fewer than 1 in 20 cases. While Flair's superiority over spaCy achieves statistical significance (p < 0.001), the negligible effect size (Cohen's h = 0.100) demonstrates that technological improvements yield practically minimal gains when confronting modernist literary language. This statistical validation supports Da's critique that computational literary analysis faces fundamental limitations with complex literary texts, while simultaneously validating Wallis's methodological framework for robust corpus linguistic research. The consistent underperformance across all tools—despite their neural architectures and contextual embeddings—suggests that Joyce's stylistic innovations create systematic challenges for automated linguistic analysis that transcend individual algorithmic approaches. These findings provide quantitative evidence that expert linguistic annotation remains essential for literary corpus analysis, particularly when dealing with texts that deliberately exploit syntactic ambiguity and narrative voice complexity as aesthetic strategies.</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Most Common Error Patterns</h2>\n",
        "        <ul>\n",
        "            <li><strong>PPHS1→PPIS1:</strong> 152-153 occurrences (3rd person pronouns misclassified as 1st person)</li>\n",
        "            <li><strong>IO→II:</strong> 105-119 occurrences (\"of\" preposition misclassified as general preposition)</li>\n",
        "            <li><strong>VVI→VV0:</strong> 95-102 occurrences (infinitive verbs misclassified as base form)</li>\n",
        "        </ul>\n",
        "        <p>These systematic errors across all tools suggest fundamental challenges with Joyce's indirect free discourse and syntactic complexity.</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Technical Details</h2>\n",
        "        <p><strong>Confidence Intervals:</strong> Wilson score intervals used for robust estimation with small samples</p>\n",
        "        <p><strong>Statistical Tests:</strong> Two-proportion z-tests for significance testing between tools</p>\n",
        "        <p><strong>Effect Size:</strong> Cohen's h for meaningful difference assessment</p>\n",
        "        <p><strong>Corpus:</strong> Expert CLAWS7 annotations from close reading analysis of Dubliners similes</p>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Save dashboard\n",
        "with open(f\"{results_dir}/index.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(dashboard_html)\n",
        "\n",
        "# Create a ZIP file for easy download\n",
        "import zipfile\n",
        "\n",
        "zip_filename = f\"nlp_validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    for root, dirs, files in os.walk(results_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, results_dir)\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\" All visualizations saved!\")\n",
        "print(f\" Files saved to: {results_dir}/\")\n",
        "print(f\" Open index.html in your browser for the complete dashboard\")\n",
        "print(f\" Download {zip_filename} for all files\")\n",
        "\n",
        "# Download the ZIP file in Colab\n",
        "from google.colab import files\n",
        "files.download(zip_filename)\n",
        "\n",
        "print(f\"\\\\n Browser Instructions:\")\n",
        "print(f\"1. Download and extract {zip_filename}\")\n",
        "print(f\"2. Open 'index.html' in any web browser\")\n",
        "print(f\"3. Click on individual chart links for interactive visualizations\")\n",
        "print(f\"4. All charts are fully interactive with zoom, pan, and hover features\")# ==============================================================================\n",
        "# SAVE VISUALIZATIONS AS HTML FILES\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results directory\n",
        "results_dir = \"nlp_validation_results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Saving visualizations to {results_dir}/ directory...\")\n",
        "\n",
        "# 1. Save Confidence Interval Plot\n",
        "fig_ci.write_html(f\"{results_dir}/confidence_intervals.html\",\n",
        "                  config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# 2. Save Error Heatmap\n",
        "fig_heatmap.write_html(f\"{results_dir}/error_heatmap.html\",\n",
        "                       config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# 3. Save Scatter Plot\n",
        "fig_scatter.write_html(f\"{results_dir}/sentence_length_analysis.html\",\n",
        "                       config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# 4. Save Box Plot Distribution\n",
        "fig_dist.write_html(f\"{results_dir}/accuracy_distributions.html\",\n",
        "                    config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# 5. Save Original Accuracy Comparison (from earlier)\n",
        "fig.write_html(f\"{results_dir}/accuracy_comparison.html\",\n",
        "               config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# 6. Create a comprehensive dashboard HTML file\n",
        "dashboard_html = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>NLP Validation Results: Joyce's Dubliners</title>\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
        "        .header {{ background-color: #2c3e50; color: white; padding: 20px; margin-bottom: 20px; }}\n",
        "        .summary {{ background-color: white; padding: 20px; margin-bottom: 20px; border-radius: 5px; }}\n",
        "        .chart-container {{ background-color: white; margin-bottom: 20px; padding: 15px; border-radius: 5px; }}\n",
        "        .chart-link {{ display: inline-block; background-color: #3498db; color: white; padding: 10px 20px;\n",
        "                       text-decoration: none; border-radius: 5px; margin: 5px; }}\n",
        "        .chart-link:hover {{ background-color: #2980b9; }}\n",
        "        .stats-table {{ width: 100%; border-collapse: collapse; }}\n",
        "        .stats-table th, .stats-table td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "        .stats-table th {{ background-color: #f2f2f2; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\">\n",
        "        <h1>NLP Tagging Validation for Joyce's Dubliners</h1>\n",
        "        <p>Statistical Analysis of Modern NLP Tools vs Expert CLAWS7 Annotations</p>\n",
        "        <p>Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Executive Summary</h2>\n",
        "        <p><strong>Research Question:</strong> How accurately do modern NLP tools perform on James Joyce's syntactically complex literary prose?</p>\n",
        "        <p><strong>Methodology:</strong> Comparison of spaCy, Flair, and TextBlob against expert CLAWS7 annotations on {len(batch_results)} simile sentences from Dubliners.</p>\n",
        "\n",
        "        <h3>Key Findings:</h3>\n",
        "        <ul>\n",
        "            <li><strong>Low Overall Accuracy:</strong> Best tool (Flair) achieved only 63.0% token-level accuracy [61.7%, 64.3%]</li>\n",
        "            <li><strong>Rare Perfect Sentences:</strong> Only 1.1-1.6% of sentences tagged perfectly</li>\n",
        "            <li><strong>Systematic Underperformance:</strong> All tools significantly below claimed 95%+ accuracy on standard text</li>\n",
        "            <li><strong>Statistical Significance:</strong> Tool differences are significant but practically negligible (Cohen's h = 0.100)</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Statistical Results Summary</h2>\n",
        "        <table class=\"stats-table\">\n",
        "            <tr><th>Tool</th><th>Token Accuracy</th><th>95% CI Lower</th><th>95% CI Upper</th><th>Perfect Sentences</th></tr>\n",
        "\"\"\"\n",
        "\n",
        "for tool_name, stats in wilson_results.items():\n",
        "    dashboard_html += f\"\"\"\n",
        "            <tr>\n",
        "                <td><strong>{tool_name.upper()}</strong></td>\n",
        "                <td>{stats['token_accuracy']:.3f}</td>\n",
        "                <td>{stats['token_ci_lower']:.3f}</td>\n",
        "                <td>{stats['token_ci_upper']:.3f}</td>\n",
        "                <td>{stats['perfect_rate']:.1%}</td>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "\n",
        "dashboard_html += f\"\"\"\n",
        "        </table>\n",
        "        <p><strong>Sample Size:</strong> {wilson_results[list(wilson_results.keys())[0]]['total_tokens']:,} tokens across {len(batch_results)} sentences</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"chart-container\">\n",
        "        <h2>Interactive Visualizations</h2>\n",
        "        <p>Click on any chart below to open the full interactive version:</p>\n",
        "\n",
        "        <a href=\"confidence_intervals.html\" class=\"chart-link\"> Confidence Intervals</a>\n",
        "        <a href=\"error_heatmap.html\" class=\"chart-link\"> Error Heatmap</a>\n",
        "        <a href=\"sentence_length_analysis.html\" class=\"chart-link\"> Length vs Accuracy</a>\n",
        "        <a href=\"accuracy_distributions.html\" class=\"chart-link\"> Accuracy Distributions</a>\n",
        "        <a href=\"accuracy_comparison.html\" class=\"chart-link\"> Tool Comparison</a>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Critical Analysis</h2>\n",
        "        <p>These Wilson confidence interval results provide compelling empirical evidence for the methodological concerns raised in computational literary studies debates. With token-level accuracies ranging from 58.2% [56.8%, 59.5%] to 63.0% [61.7%, 64.3%] across 5,437 tokens, modern NLP tools demonstrate systematic underperformance on Joyce's literary prose compared to their claimed 95%+ accuracy on standard text. The extraordinarily low perfect sentence rates (1.1-1.6%) with wide confidence intervals [0.3%, 4.7%] reveal that flawless automated tagging of Joyce's syntactically complex sentences is statistically rare, occurring in fewer than 1 in 20 cases. While Flair's superiority over spaCy achieves statistical significance (p < 0.001), the negligible effect size (Cohen's h = 0.100) demonstrates that technological improvements yield practically minimal gains when confronting modernist literary language. This statistical validation supports Da's critique that computational literary analysis faces fundamental limitations with complex literary texts, while simultaneously validating Wallis's methodological framework for robust corpus linguistic research. The consistent underperformance across all tools—despite their neural architectures and contextual embeddings—suggests that Joyce's stylistic innovations create systematic challenges for automated linguistic analysis that transcend individual algorithmic approaches. These findings provide quantitative evidence that expert linguistic annotation remains essential for literary corpus analysis, particularly when dealing with texts that deliberately exploit syntactic ambiguity and narrative voice complexity as aesthetic strategies.</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Most Common Error Patterns</h2>\n",
        "        <ul>\n",
        "            <li><strong>PPHS1→PPIS1:</strong> 152-153 occurrences (3rd person pronouns misclassified as 1st person)</li>\n",
        "            <li><strong>IO→II:</strong> 105-119 occurrences (\"of\" preposition misclassified as general preposition)</li>\n",
        "            <li><strong>VVI→VV0:</strong> 95-102 occurrences (infinitive verbs misclassified as base form)</li>\n",
        "        </ul>\n",
        "        <p>These systematic errors across all tools suggest fundamental challenges with Joyce's indirect free discourse and syntactic complexity.</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Technical Details</h2>\n",
        "        <p><strong>Confidence Intervals:</strong> Wilson score intervals used for robust estimation with small samples</p>\n",
        "        <p><strong>Statistical Tests:</strong> Two-proportion z-tests for significance testing between tools</p>\n",
        "        <p><strong>Effect Size:</strong> Cohen's h for meaningful difference assessment</p>\n",
        "        <p><strong>Corpus:</strong> Expert CLAWS7 annotations from close reading analysis of Dubliners similes</p>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Save dashboard\n",
        "with open(f\"{results_dir}/index.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(dashboard_html)\n",
        "\n",
        "# Create a ZIP file for easy download\n",
        "import zipfile\n",
        "\n",
        "zip_filename = f\"nlp_validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    for root, dirs, files in os.walk(results_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, results_dir)\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"All visualizations saved!\")\n",
        "print(f\"Files saved to: {results_dir}/\")\n",
        "print(f\"Open index.html in your browser for the complete dashboard\")\n",
        "print(f\"Download {zip_filename} for all files\")\n",
        "\n",
        "# Download the ZIP file in Colab\n",
        "from google.colab import files\n",
        "files.download(zip_filename)\n",
        "\n",
        "print(f\"\\\\n Browser Instructions:\")\n",
        "print(f\"1. Download and extract {zip_filename}\")\n",
        "print(f\"2. Open 'index.html' in any web browser\")\n",
        "print(f\"3. Click on individual chart links for interactive visualizations\")\n",
        "print(f\"4. All charts are fully interactive with zoom, pan, and hover features\")"
      ],
      "metadata": {
        "id": "5dOmii_1Q8E4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}