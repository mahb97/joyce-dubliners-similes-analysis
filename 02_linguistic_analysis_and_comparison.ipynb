{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/joyce-dubliners-similes-analysis/blob/main/02_linguistic_analysis_and_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw07RNuhGhxA"
      },
      "source": [
        "# Joyce Simile Research: Comprehensive Linguistic Analysis and Comparison Framework\n",
        "\n",
        "# Abstract\n",
        "\n",
        "This notebook presents a reproducible computational framework for analysing similes in James Joyce’s *Dubliners* and benchmarking extraction methods against a British National Corpus (BNC) baseline. Four datasets are integrated—manual close-reading (ground truth), a restrictive rule-based extractor, a less-restrictive NLP extractor, and BNC—under a harmonised taxonomy that merges *Joycean_Quasi* with the BNC’s *Quasi_Similes* into a single label **Quasi_Similes** to avoid double-counting. The pipeline performs sentence-level linguistic profiling (comparator-span detection, pre/post structure, POS distributions, syntactic complexity, exploratory sentiment) and evaluates extractors via **instance-aligned F1** (exact match then fuzzy ≥ 0.92) against the manual set.\n",
        "\n",
        "Statistical inference combines a 4-way χ² with standardized residuals and BH-FDR control, two-proportion tests (Newcombe CIs, Cohen’s *h*), binomial checks against BNC reference proportions, and continuous-feature comparisons (Welch *t*, Mann–Whitney *U*, Hedges’ *g*, Cliff’s δ). Topic modelling (LDA) summarises thematic variation per subset.\n",
        "\n",
        "Results show a strong categorical association across corpora (**χ² ≈ 281.88**, **df = 15**, **Cramér’s V ≈ 0.318**, *p* ≪ .001). **Quasi\\_Similes** are more prevalent in **BNC** (≈ 41 %) than in **Joyce-Manual** (≈ 29 %), while Joyce exhibits enrichments in **Joycean\\_Framed**, **Joycean\\_Quasi\\_Fuzzy**, and **Joycean\\_Silent** (≈ 20 % combined in the manual set). The less-restrictive NLP subset collapses to **Standard** (control), confirming distributional contrasts are not driven by it. Instance-aligned performance indicates partial recovery of expert labels (Rule-Based vs Manual: micro-F1 ≈ **0.343**, macro-F1 ≈ **0.178**; NLP vs Manual: micro-F1 ≈ **0.292**, macro-F1 ≈ **0.059**), motivating targeted pattern and dependency enhancements.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Comprehensive Linguistic Analysis Framework\n",
        "\n",
        "## 6.1 Multi-Dataset Integration\n",
        "The pipeline ingests four sources and standardises them to a common schema with stable IDs and explicit provenance:\n",
        "- **Manual_CloseReading** (expert annotations; ground truth)\n",
        "- **Restrictive_Dubliners** (rule-based extractor)\n",
        "- **NLP_LessRestrictive_PG** (broad pattern extractor; control—predominantly *Standard*)\n",
        "- **BNC_Baseline** (standard-English reference)\n",
        "\n",
        "Column names are normalised, missing IDs are repaired, and categories are **harmonised** so that *Joycean_Quasi* and the BNC’s *Quasi_Similes* are merged into a single label **Quasi_Similes**. This avoids double-counting and supports fair cross-corpus comparisons. All intermediate artefacts (CSV/JSON) are written with timestamps for reproducibility.\n",
        "\n",
        "## 6.2 Advanced Linguistic Feature Extraction\n",
        "Using spaCy (with a robust fallback), the framework derives sentence-level linguistic features tailored to simile analysis:\n",
        "- **Comparator span detection** for `like`, `as`, **as if/as though**, **as … as**, and lemma families (**seem\\***, **resembl\\***), plus punctuation-mediated comparators (**colon, semicolon, ellipsis, dash**).\n",
        "- **Pre/Post comparator structure**: token counts on each side and **Pre_Post_Ratio**.\n",
        "- **Syntactic complexity** via dependency depth; **POS distribution** and **lemmatised text**.\n",
        "- **Figurative density** from comparator/marker hits.\n",
        "- **Exploratory sentiment** (TextBlob polarity/subjectivity).\n",
        "If spaCy is unavailable, the fallback computes token counts, comparator positions, and sentiment only.\n",
        "\n",
        "## 6.3 Performance Validation\n",
        "Extractor performance is evaluated **instance-by-instance** against the manual set using **sentence alignment**:\n",
        "- Exact match first, then **fuzzy matching (≥ 0.92)** to pair sentences.\n",
        "- Per-category **TP/FP/FN** and **micro/macro F1** are computed from the aligned pairs—more faithful than bag-of-counts.\n",
        "- Statistical testing (reported in Section 8): **4-way χ²** with standardised residuals and BH-FDR; **two-proportion tests** (Newcombe CIs, Cohen’s *h*), **binomial checks** vs BNC, and **continuous** comparisons (Welch *t*, Mann–Whitney *U*, Hedges’ *g*, Cliff’s δ).  \n",
        "These procedures emphasise **effect sizes and error control**, and remain informative even when the less-restrictive NLP set collapses to *Standard*.\n",
        "\n"
      ],
      "metadata": {
        "id": "zXvVrXssd9qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS\n",
        "# (UPDATED: multi-comparator pre / between / post segmentation in Cell 1)\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from difflib import SequenceMatcher\n",
        "from datetime import datetime\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plot libs not used in this cell but kept for notebook continuity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional NLP libs\n",
        "try:\n",
        "    import spacy\n",
        "except Exception:\n",
        "    spacy = None\n",
        "\n",
        "try:\n",
        "    from textblob import TextBlob\n",
        "except Exception:\n",
        "    TextBlob = None\n",
        "\n",
        "print(\"COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS (FIXED + SEGMENTS)\")\n",
        "print(\"=\" * 75)\n",
        "print(\"Dataset 1: Manual Annotations (Ground Truth - Close Reading)\")\n",
        "print(\"Dataset 2: Rule-Based Extraction (Restrictive - Domain-Informed)\")\n",
        "print(\"Dataset 3: NLP Extraction (Less-Restrictive - PG Dubliners)\")\n",
        "print(\"Dataset 4: BNC Baseline Corpus (Standard English Reference)\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# Initialize spaCy if available\n",
        "nlp = None\n",
        "if spacy is not None:\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        print(\"spaCy pipeline loaded: en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"spaCy model not found; attempting to download…\")\n",
        "        os.system(\"python -m spacy download en_core_web_sm\")\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "            print(\"spaCy pipeline loaded after download: en_core_web_sm\")\n",
        "        except Exception:\n",
        "            print(\"spaCy unavailable; analysis will use simplified methods.\")\n",
        "\n",
        "class ComprehensiveLinguisticComparator:\n",
        "    \"\"\"\n",
        "    Full pipeline:\n",
        "      - robust loading & standardisation\n",
        "      - linguistic feature extraction (spaCy/TextBlob, simplified fallback)\n",
        "      - multi-comparator segmentation (Pre / Between / Post) + shares & flags\n",
        "      - category harmonisation (MERGES Joycean_Quasi into Quasi_Similes)\n",
        "      - instance-aligned F1 (exact + fuzzy sentence matching)\n",
        "      - reproducibility & environment stamping\n",
        "      - combined CSV export with stable ordering\n",
        "    \"\"\"\n",
        "\n",
        "    # ----------------- Constructor & environment -----------------\n",
        "    def __init__(self):\n",
        "        self.nlp = nlp\n",
        "        self.datasets = {}\n",
        "        self.linguistic_features = {}\n",
        "        self.comparison_results = {}\n",
        "\n",
        "        # Reproducibility\n",
        "        self.random_seed = 42\n",
        "        random.seed(self.random_seed)\n",
        "        np.random.seed(self.random_seed)\n",
        "\n",
        "        # Environment info for auditability\n",
        "        tb_ver = \"n/a\"\n",
        "        try:\n",
        "            import textblob as _tb\n",
        "            tb_ver = getattr(_tb, \"__version__\", \"n/a\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.env_info = {\n",
        "            \"python\": sys.version,\n",
        "            \"pandas\": pd.__version__,\n",
        "            \"numpy\": np.__version__,\n",
        "            \"spacy\": getattr(spacy, \"__version__\", \"n/a\") if spacy is not None else \"n/a\",\n",
        "            \"textblob\": tb_ver,\n",
        "        }\n",
        "        print(\"Environment:\", self.env_info)\n",
        "\n",
        "    # ----------------- ID / Loading / Standardisation -----------------\n",
        "    def _ensure_ids(self, df, dataset_name, prefix=None):\n",
        "        \"\"\"\n",
        "        Ensure a unique, non-null 'Instance_ID' string column exists.\n",
        "        If missing, non-unique, or contains NaNs, regenerate sequential IDs with a readable prefix.\n",
        "        \"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return pd.DataFrame(columns=['Instance_ID'])\n",
        "\n",
        "        short = (prefix or {\n",
        "            'manual': 'MAN',\n",
        "            'rule_based': 'RST',\n",
        "            'nlp': 'NLP',\n",
        "            'bnc': 'BNC'\n",
        "        }.get(dataset_name, dataset_name[:3].upper()))\n",
        "\n",
        "        candidates = ['Instance_ID', 'ID', 'id', 'sentence_id', 'Sentence_ID', 'Index', 'index']\n",
        "        chosen = next((c for c in candidates if c in df.columns), None)\n",
        "        if chosen and chosen != 'Instance_ID':\n",
        "            df = df.rename(columns={chosen: 'Instance_ID'})\n",
        "        elif not chosen:\n",
        "            df['Instance_ID'] = np.nan\n",
        "\n",
        "        # Normalizes and tests uniqueness\n",
        "        df['Instance_ID'] = df['Instance_ID'].astype(str).replace({'nan': np.nan, '': np.nan})\n",
        "        needs_regen = df['Instance_ID'].isna().any() or (not df['Instance_ID'].is_unique)\n",
        "        if needs_regen:\n",
        "            df['Instance_ID'] = [f\"{short}_{i+1:05d}\" for i in range(len(df))]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _load_manual_dataset_robust(self, file_content):\n",
        "        \"\"\"Robust loader for manual annotations with long quoted Joycean sentences.\"\"\"\n",
        "        import csv\n",
        "        try:\n",
        "            df = pd.read_csv(\n",
        "                file_content, encoding='cp1252', quotechar='\"',\n",
        "                quoting=csv.QUOTE_MINIMAL, skipinitialspace=True, engine='python'\n",
        "            )\n",
        "            if 'Sentence Context' in df.columns:\n",
        "                df = df[df['Sentence Context'].astype(str).str.lower() != 'sentence context'].copy()\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"  pandas (python engine) failed: {e}\")\n",
        "\n",
        "        # Fallback simpler read\n",
        "        try:\n",
        "            df = pd.read_csv(file_content, encoding='cp1252')\n",
        "            if 'Sentence Context' in df.columns:\n",
        "                df = df[df['Sentence Context'].astype(str).str.lower() != 'sentence context'].copy()\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"  pandas (default) failed: {e}\")\n",
        "\n",
        "        print(\"  Manual annotations not found or failed to load.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def load_datasets(self, manual_file=None, rule_based_file=None, nlp_file=None, bnc_file=None):\n",
        "        print(\"\\nLOADING DATASETS WITH FIXED ID HANDLING & EXPLICIT LABELS\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # Manual (close reading)\n",
        "        print(\"Loading manual annotations…\")\n",
        "        self.datasets['manual'] = self._load_manual_dataset_robust(manual_file) if manual_file else pd.DataFrame()\n",
        "        self.datasets['manual'] = self._ensure_ids(self.datasets['manual'], 'manual', prefix='MAN')\n",
        "        if not self.datasets['manual'].empty:\n",
        "            self.datasets['manual']['Original_Dataset'] = 'Manual_CloseReading'\n",
        "\n",
        "        # Rule-based (restrictive)\n",
        "        print(\"Loading rule-based (restrictive)…\")\n",
        "        self.datasets['rule_based'] = pd.read_csv(rule_based_file) if rule_based_file else pd.DataFrame()\n",
        "        self.datasets['rule_based'] = self._ensure_ids(self.datasets['rule_based'], 'rule_based', prefix='RST')\n",
        "        if not self.datasets['rule_based'].empty:\n",
        "            self.datasets['rule_based']['Original_Dataset'] = 'Restrictive_Dubliners'\n",
        "\n",
        "        # NLP (less-restrictive PG)\n",
        "        print(\"Loading NLP (less-restrictive PG)…\")\n",
        "        self.datasets['nlp'] = pd.read_csv(nlp_file) if nlp_file else pd.DataFrame()\n",
        "        self.datasets['nlp'] = self._ensure_ids(self.datasets['nlp'], 'nlp', prefix='NLP')\n",
        "        if not self.datasets['nlp'].empty:\n",
        "            self.datasets['nlp']['Original_Dataset'] = 'NLP_LessRestrictive_PG'\n",
        "\n",
        "        # BNC\n",
        "        print(\"Loading BNC baseline…\")\n",
        "        self.datasets['bnc'] = pd.read_csv(bnc_file, encoding='utf-8') if bnc_file else pd.DataFrame()\n",
        "        self.datasets['bnc'] = self._ensure_ids(self.datasets['bnc'], 'bnc', prefix='BNC')\n",
        "        if not self.datasets['bnc'].empty:\n",
        "            self.datasets['bnc']['Original_Dataset'] = 'BNC_Baseline'\n",
        "\n",
        "        self._standardize_datasets()\n",
        "        self._standardize_categories()\n",
        "\n",
        "        for name, df in self.datasets.items():\n",
        "            print(f\"{name:>12}: rows={len(df):4d}  \"\n",
        "                  f\"missing_IDs={df['Instance_ID'].isna().sum() if 'Instance_ID' in df else 'N/A'}  \"\n",
        "                  f\"missing_Original_Dataset={df['Original_Dataset'].isna().sum() if 'Original_Dataset' in df else 'N/A'}\")\n",
        "        print(f\"Total instances: {sum(len(df) for df in self.datasets.values())}\")\n",
        "\n",
        "    def _standardize_datasets(self):\n",
        "        print(\"Standardizing column names & adding Dataset_Source…\")\n",
        "\n",
        "        # Manual\n",
        "        df = self.datasets.get('manual', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            ren = {\n",
        "                'Category (Framwrok)': 'Category_Framework',\n",
        "                'Comparator Type ': 'Comparator_Type',\n",
        "                'Sentence Context': 'Sentence_Context',\n",
        "                'Page No.': 'Page_Number'\n",
        "            }\n",
        "            df = df.rename(columns={k: v for k, v in ren.items() if k in df.columns})\n",
        "            df['Dataset_Source'] = 'Manual_Expert_Annotation'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['manual'] = df\n",
        "        else:\n",
        "            self.datasets['manual'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context','Page_Number',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # Rule-based\n",
        "        df = self.datasets.get('rule_based', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            df = df.rename(columns={\n",
        "                'Sentence Context': 'Sentence_Context',\n",
        "                'Comparator Type ': 'Comparator_Type',\n",
        "                'Category (Framwrok)': 'Category_Framework'\n",
        "            })\n",
        "            df['Dataset_Source'] = 'Rule_Based_Domain_Informed'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['rule_based'] = df\n",
        "        else:\n",
        "            self.datasets['rule_based'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # NLP (less-restrictive)\n",
        "        df = self.datasets.get('nlp', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            if 'Sentence_Context' not in df.columns:\n",
        "                for c in ['Sentence Context','text','sentence','context','content']:\n",
        "                    if c in df.columns:\n",
        "                        df = df.rename(columns={c: 'Sentence_Context'})\n",
        "                        break\n",
        "            if 'Comparator Type ' in df.columns:\n",
        "                df = df.rename(columns={'Comparator Type ': 'Comparator_Type'})\n",
        "            if 'Category (Framwrok)' in df.columns and 'Category_Framework' not in df.columns:\n",
        "                df = df.rename(columns={'Category (Framwrok)': 'Category_Framework'})\n",
        "            if 'Category_Framework' not in df.columns:\n",
        "                df['Category_Framework'] = 'NLP_Basic_Pattern'\n",
        "            df['Dataset_Source'] = 'NLP_General_Pattern_Recognition'\n",
        "            df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['nlp'] = df\n",
        "        else:\n",
        "            self.datasets['nlp'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # BNC\n",
        "        df = self.datasets.get('bnc', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            if 'Category (Framework)' in df.columns and 'Category_Framework' not in df.columns:\n",
        "                df = df.rename(columns={'Category (Framework)':'Category_Framework'})\n",
        "            if 'Comparator Type' in df.columns and 'Comparator_Type' not in df.columns:\n",
        "                df = df.rename(columns={'Comparator Type':'Comparator_Type'})\n",
        "            if 'Sentence Context' in df.columns and 'Sentence_Context' not in df.columns:\n",
        "                df = df.rename(columns={'Sentence Context':'Sentence_Context'})\n",
        "            df['Dataset_Source'] = 'BNC_Standard_English_Baseline'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['bnc'] = df\n",
        "        else:\n",
        "            self.datasets['bnc'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Sentence_Context','Comparator_Type','Category_Framework',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        print(\"Standardization complete.\")\n",
        "\n",
        "    def _standardize_categories(self):\n",
        "        \"\"\"\n",
        "        Harmonize Category_Framework labels.\n",
        "        IMPORTANT: Merge Joycean_Quasi and its variants into Quasi_Similes (unified quasi-simile phenomenon).\n",
        "        \"\"\"\n",
        "        print(\"Harmonizing Category_Framework labels…\")\n",
        "        mapping = {\n",
        "            # Standard variants\n",
        "            'NLP_Basic': 'Standard',\n",
        "            'NLP_Basic_Pattern': 'Standard',\n",
        "            'Standard_English_Usage': 'Standard',\n",
        "            'Standard': 'Standard',\n",
        "\n",
        "            # Joycean subtypes\n",
        "            'Joycean_Framed': 'Joycean_Framed',\n",
        "            'Joycean_Silent': 'Joycean_Silent',\n",
        "            'Joycean_Quasi_Fuzzy': 'Joycean_Quasi_Fuzzy',\n",
        "            'Joycean-Quasi-Fuzzy': 'Joycean_Quasi_Fuzzy',\n",
        "\n",
        "            # >>> UNIFY all quasi-simile tags here <<<\n",
        "            'Quasi_Similes': 'Quasi_Similes',\n",
        "            'Quasi_Simile': 'Quasi_Similes',     # singular → plural canonical\n",
        "            'Joycean_Quasi': 'Quasi_Similes',    # merge with BNC tag\n",
        "            'Joycean-Quasi': 'Quasi_Similes',    # hyphen variant\n",
        "\n",
        "            # mislabels leaking from dataset names → map to Standard\n",
        "            'NLP_LessRestrictive': 'Standard',\n",
        "            'NLP_General_Pattern': 'Standard',\n",
        "            'Less-Restrictive': 'Standard',\n",
        "\n",
        "            # housekeeping\n",
        "            'Uncategorised': 'Uncategorized',\n",
        "            'nan': 'Uncategorized', 'NaN': 'Uncategorized', '': 'Uncategorized'\n",
        "        }\n",
        "        for name, df in self.datasets.items():\n",
        "            if df.empty or 'Category_Framework' not in df.columns:\n",
        "                continue\n",
        "            df['Category_Framework'] = df['Category_Framework'].astype(str).map(mapping).fillna(df['Category_Framework'])\n",
        "            self.datasets[name] = df\n",
        "        print(\"Category harmonization complete.\")\n",
        "\n",
        "    # ----------------- Utilities & segmentation helpers -----------------\n",
        "    _SUBORDINATORS = {\"that\",\"who\",\"which\",\"when\",\"because\",\"though\",\"although\",\"if\",\"as\",\n",
        "                      \"while\",\"since\",\"unless\",\"until\",\"where\",\"whereas\"}\n",
        "\n",
        "    def _fmt_between_list(self, lst):\n",
        "        \"\"\"Return '0' for empty; else 'a, b, c' with no brackets.\"\"\"\n",
        "        try:\n",
        "            if not lst:\n",
        "                return \"0\"\n",
        "            return \", \".join(str(int(x)) for x in lst)\n",
        "        except Exception:\n",
        "            return \"0\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_sentence_for_match(s: str) -> str:\n",
        "        s = (s or \"\")\n",
        "        s = s.replace(\"—\", \"-\").replace(\"–\", \"-\")\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
        "        table = str.maketrans(\"\", \"\", \"\\\"'“”‘’\")\n",
        "        s = s.translate(table)\n",
        "        return s\n",
        "\n",
        "    def _fuzzy_equal(self, a: str, b: str, threshold: float = 0.92) -> bool:\n",
        "        if not a or not b:\n",
        "            return False\n",
        "        ra = self._normalize_sentence_for_match(a)\n",
        "        rb = self._normalize_sentence_for_match(b)\n",
        "        if ra == rb:\n",
        "            return True\n",
        "        return SequenceMatcher(None, ra, rb).ratio() >= threshold\n",
        "\n",
        "    def _simple_tokens(self, s: str):\n",
        "        s = re.sub(r\"[“”‘’\\\"']\", \"\", str(s))\n",
        "        s = s.replace(\"—\",\"-\").replace(\"–\",\"-\")\n",
        "        return re.findall(r\"\\w+|[:;,\\.\\-\\(\\)…]\", s)\n",
        "\n",
        "    def _detect_comparator_spans(self, doc_tokens, use_spacy=False):\n",
        "        \"\"\"\n",
        "        Return list of comparator spans as (start_i, end_i, label).\n",
        "        - Single-token spans use start=end\n",
        "        - 'as … as' pairs become a single span covering both anchors: label='as_as'\n",
        "        \"\"\"\n",
        "        spans = []\n",
        "        n = len(doc_tokens)\n",
        "\n",
        "        def t_text(i): return (doc_tokens[i].text if use_spacy else doc_tokens[i])\n",
        "        def t_lemma(i):\n",
        "            if use_spacy:\n",
        "                return getattr(doc_tokens[i], \"lemma_\", t_text(i)).lower()\n",
        "            return t_text(i).lower()\n",
        "\n",
        "        lower = [t_text(i).lower() for i in range(n)]\n",
        "        lemmas = [t_lemma(i) for i in range(n)]\n",
        "        used = set()\n",
        "\n",
        "        # (1) multiword: 'as if' / 'as though'\n",
        "        i = 0\n",
        "        while i < n-1:\n",
        "            if lower[i] == \"as\" and lower[i+1] == \"if\":\n",
        "                spans.append((i, i+1, \"as_if\")); used.update((i,i+1)); i += 2; continue\n",
        "            if lower[i] == \"as\" and lower[i+1] == \"though\":\n",
        "                spans.append((i, i+1, \"as_though\")); used.update((i,i+1)); i += 2; continue\n",
        "            i += 1\n",
        "\n",
        "        # (2) 'as … as'\n",
        "        as_positions = [i for i,w in enumerate(lower) if w == \"as\" and i not in used]\n",
        "        consumed_as = set()\n",
        "        for i in as_positions:\n",
        "            if i in consumed_as:\n",
        "                continue\n",
        "            partner = None\n",
        "            for j in as_positions:\n",
        "                if j > i and (j - i) <= 8 and j not in consumed_as:\n",
        "                    partner = j; break\n",
        "            if partner is not None:\n",
        "                spans.append((i, partner, \"as_as\"))\n",
        "                consumed_as.add(i); consumed_as.add(partner)\n",
        "        used.update(consumed_as)\n",
        "\n",
        "        # (3) single 'like'\n",
        "        for i,w in enumerate(lower):\n",
        "            if w == \"like\":\n",
        "                spans.append((i,i,\"like\"))\n",
        "\n",
        "        # (4) lemma families: resemble / seem / appear\n",
        "        for i,lm in enumerate(lemmas):\n",
        "            if lm.startswith(\"resembl\") or lm.startswith(\"seem\") or lm.startswith(\"appear\"):\n",
        "                spans.append((i,i,lm.split(\"-\")[0] if \"-\" in lm else lm))\n",
        "\n",
        "        # (5) punctuation frames\n",
        "        for i,w in enumerate([t_text(k) for k in range(n)]):\n",
        "            if w in {\":\",\";\",\"—\",\"–\",\"-\",\"...\",\"…\"}:\n",
        "                spans.append((i,i,f\"punc_{w}\"))\n",
        "\n",
        "        # sort & coalesce overlaps\n",
        "        spans = sorted(spans, key=lambda x: (x[0], x[1]))\n",
        "        coalesced, last = [], None\n",
        "        for s in spans:\n",
        "            if last is None:\n",
        "                last = s\n",
        "            else:\n",
        "                if s[0] <= last[1] and s[1] >= last[0]:\n",
        "                    last = (min(last[0], s[0]), max(last[1], s[1]), last[2])\n",
        "                else:\n",
        "                    coalesced.append(last); last = s\n",
        "        if last is not None:\n",
        "            coalesced.append(last)\n",
        "        return coalesced\n",
        "\n",
        "    def _segments_from_spans(self, doc_tokens, spans, use_spacy=False):\n",
        "        \"\"\"\n",
        "        Compute token counts for pre / between / post w.r.t. comparator spans.\n",
        "        Excludes spaces/punct from counts. Returns dict with counts + flags.\n",
        "        \"\"\"\n",
        "        # Build list of content-token indices\n",
        "        content_idx = []\n",
        "        doc_to_content = {}\n",
        "        for i, tok in enumerate(doc_tokens):\n",
        "            if use_spacy:\n",
        "                if getattr(tok, \"is_space\", False) or getattr(tok, \"is_punct\", False):\n",
        "                    continue\n",
        "            else:\n",
        "                if re.fullmatch(r\"[:;,.\\-\\(\\)…]\", str(tok)):\n",
        "                    continue\n",
        "            doc_to_content[i] = len(content_idx)\n",
        "            content_idx.append(i)\n",
        "\n",
        "        def nearest_kept(i):\n",
        "            if i in doc_to_content:\n",
        "                return doc_to_content[i]\n",
        "            L, R = i-1, i+1\n",
        "            while L >= 0 or R < len(doc_tokens):\n",
        "                if L >= 0 and L in doc_to_content: return doc_to_content[L]\n",
        "                if R < len(doc_tokens) and R in doc_to_content: return doc_to_content[R]\n",
        "                L -= 1; R += 1\n",
        "            return 0\n",
        "\n",
        "        spans_np = []\n",
        "        for a,b,_label in spans:\n",
        "            a_np = nearest_kept(a); b_np = nearest_kept(b)\n",
        "            spans_np.append((min(a_np,b_np), max(a_np,b_np)))\n",
        "        spans_np = sorted(spans_np)\n",
        "\n",
        "        total = len(content_idx)\n",
        "        if total == 0:\n",
        "            return {\n",
        "                \"Pre_Tokens\": 0, \"Post_Tokens\": 0, \"Between_Tokens_List\": [],\n",
        "                \"Between_Tokens_Total\": 0, \"Between_Segments\": 0,\n",
        "                \"Between_Max\": 0, \"Between_Mean\": 0.0,\n",
        "                \"Between_Has_Verb\": False, \"Between_Has_Subordinator\": False\n",
        "            }\n",
        "\n",
        "        if not spans_np:\n",
        "            # No comparator detected → neutral split\n",
        "            pre = total // 2; post = total - pre\n",
        "            return {\n",
        "                \"Pre_Tokens\": pre, \"Post_Tokens\": post, \"Between_Tokens_List\": [],\n",
        "                \"Between_Tokens_Total\": 0, \"Between_Segments\": 0,\n",
        "                \"Between_Max\": 0, \"Between_Mean\": 0.0,\n",
        "                \"Between_Has_Verb\": False, \"Between_Has_Subordinator\": False\n",
        "            }\n",
        "\n",
        "        first_start = spans_np[0][0]\n",
        "        last_end   = spans_np[-1][1]\n",
        "\n",
        "        pre = first_start\n",
        "        post = max(total - (last_end + 1), 0)\n",
        "\n",
        "        between = []\n",
        "        for i in range(len(spans_np)-1):\n",
        "            gap = max(spans_np[i+1][0] - (spans_np[i][1] + 1), 0)\n",
        "            between.append(gap)\n",
        "\n",
        "        # clause-like diagnostics in raw token space\n",
        "        has_verb = False\n",
        "        has_sub  = False\n",
        "        if len(spans) >= 2:\n",
        "            for i in range(len(spans)-1):\n",
        "                a_end, b_start = spans[i][1], spans[i+1][0]\n",
        "                chunk = doc_tokens[a_end+1:b_start]\n",
        "                for tok in chunk:\n",
        "                    if use_spacy:\n",
        "                        if getattr(tok, \"pos_\", \"\") == \"VERB\":\n",
        "                            has_verb = True\n",
        "                        if str(getattr(tok, \"text\", tok)).lower() in self._SUBORDINATORS:\n",
        "                            has_sub = True\n",
        "                    else:\n",
        "                        st = str(tok).lower()\n",
        "                        if st in self._SUBORDINATORS:\n",
        "                            has_sub = True\n",
        "                        if re.fullmatch(r\".*(ed|ing)$\", st) or st in {\"be\",\"is\",\"are\",\"was\",\"were\",\"been\",\"am\",\"have\",\"has\",\"had\",\"do\",\"did\",\"does\"}:\n",
        "                            has_verb = True\n",
        "\n",
        "        bt_total = int(np.sum(between)) if between else 0\n",
        "        bt_max   = int(np.max(between)) if between else 0\n",
        "        bt_mean  = float(np.mean(between)) if between else 0.0\n",
        "\n",
        "        return {\n",
        "            \"Pre_Tokens\": int(pre),\n",
        "            \"Post_Tokens\": int(post),\n",
        "            \"Between_Tokens_List\": between,\n",
        "            \"Between_Tokens_Total\": bt_total,\n",
        "            \"Between_Segments\": int(len(between)),\n",
        "            \"Between_Max\": bt_max,\n",
        "            \"Between_Mean\": bt_mean,\n",
        "            \"Between_Has_Verb\": bool(has_verb),\n",
        "            \"Between_Has_Subordinator\": bool(has_sub)\n",
        "        }\n",
        "\n",
        "    def _pattern_label(self, span_labels):\n",
        "        if not span_labels:\n",
        "            return \"none\"\n",
        "        labs = [l for l in span_labels]\n",
        "        if labs == [\"as_as\"]:\n",
        "            return \"as_as\"\n",
        "        if labs.count(\"like\") >= 2:\n",
        "            return \"like_like\"\n",
        "        if any(l.startswith(\"punc_\") for l in labs):\n",
        "            return \"framed_multi\" if len(labs) >= 2 else \"framed_single\"\n",
        "        if len(labs) == 1:\n",
        "            return \"single\"\n",
        "        return \"multi\"\n",
        "\n",
        "    # ----------------- Linguistic analysis (spaCy / fallback) -----------------\n",
        "    def perform_comprehensive_linguistic_analysis(self):\n",
        "        print(\"\\nPERFORMING LINGUISTIC ANALYSIS (with pre/between/post segmentation)\")\n",
        "        print(\"-\" * 60)\n",
        "        if self.nlp is None:\n",
        "            print(\"spaCy unavailable → simplified analysis (segmentation + sentiment only).\")\n",
        "            return self._perform_simplified_analysis()\n",
        "\n",
        "        for name, df in list(self.datasets.items()):\n",
        "            if df.empty:\n",
        "                print(f\"Skipping empty dataset: {name}\")\n",
        "                continue\n",
        "\n",
        "            n = len(df)\n",
        "            feats = {\n",
        "                # --- segmentation block ---\n",
        "                'Pre_Tokens': [0]*n,\n",
        "                'Post_Tokens': [0]*n,\n",
        "                'Between_Tokens_List_Raw': [\"0\"]*n,  # bracketless string; '0' if empty\n",
        "                'Between_Tokens_List': [\"0\"]*n,      # user-facing same string\n",
        "                'Between_Tokens_Total': [0]*n,\n",
        "                'Between_Segments': [0]*n,\n",
        "                'Between_Max': [0]*n,\n",
        "                'Between_Mean': [0.0]*n,\n",
        "                'Between_Has_Verb': [False]*n,\n",
        "                'Between_Has_Subordinator': [False]*n,\n",
        "                'Pre_Share': [np.nan]*n,\n",
        "                'Between_Share': [np.nan]*n,\n",
        "                'Post_Share': [np.nan]*n,\n",
        "                'Comp_Count': [0]*n,\n",
        "                'Comp_Labels': [json.dumps([])]*n,\n",
        "                'Pattern_Label': [\"none\"]*n,\n",
        "\n",
        "                # --- legacy / other features still in use downstream ---\n",
        "                'Total_Tokens': [0]*n,\n",
        "                'Pre_Post_Ratio': [np.nan]*n,\n",
        "                'Lemmatized_Text': [None]*n,\n",
        "                'POS_Tags': [None]*n,\n",
        "                'POS_Distribution': [None]*n,\n",
        "                'Sentiment_Polarity': [np.nan]*n,\n",
        "                'Sentiment_Subjectivity': [np.nan]*n,\n",
        "                'Comparative_Structure': [None]*n,\n",
        "                'Syntactic_Complexity': [np.nan]*n,\n",
        "                'Sentence_Length': [0]*n,\n",
        "                'Adjective_Count': [0]*n,\n",
        "                'Verb_Count': [0]*n,\n",
        "                'Noun_Count': [0]*n,\n",
        "                'Figurative_Density': [0.0]*n\n",
        "            }\n",
        "\n",
        "            for ridx, row in df.iterrows():\n",
        "                sent = str(row.get('Sentence_Context', '') or '').strip()\n",
        "                comp = row.get('Comparator_Type', '')\n",
        "                if not sent:\n",
        "                    continue\n",
        "                try:\n",
        "                    doc = self.nlp(sent)\n",
        "\n",
        "                    # tokens for other features\n",
        "                    tokens_nopunct = [t for t in doc if not t.is_space and not t.is_punct]\n",
        "                    total = len(tokens_nopunct)\n",
        "\n",
        "                    # --- multi-comparator detection & segmentation ---\n",
        "                    spans = self._detect_comparator_spans(doc, use_spacy=True)\n",
        "                    comp_labels = [lab for *_ab, lab in spans]\n",
        "                    segs = self._segments_from_spans(doc, spans, use_spacy=True)\n",
        "\n",
        "                    pre, post = segs[\"Pre_Tokens\"], segs[\"Post_Tokens\"]\n",
        "                    bt_list = segs[\"Between_Tokens_List\"]\n",
        "                    bt_str = self._fmt_between_list(bt_list)\n",
        "                    denom = pre + post + segs[\"Between_Tokens_Total\"]\n",
        "                    pre_share = (pre / denom) if denom else np.nan\n",
        "                    between_share = (segs[\"Between_Tokens_Total\"] / denom) if denom else np.nan\n",
        "                    post_share = (post / denom) if denom else np.nan\n",
        "                    pre_post_ratio = (pre / post) if post > 0 else np.nan\n",
        "                    pattern = self._pattern_label(comp_labels)\n",
        "\n",
        "                    loc = df.index.get_loc(ridx)\n",
        "\n",
        "                    # Fills segmentation features\n",
        "                    feats['Pre_Tokens'][loc] = pre\n",
        "                    feats['Post_Tokens'][loc] = post\n",
        "                    feats['Between_Tokens_List_Raw'][loc] = bt_str      # bracketless or '0'\n",
        "                    feats['Between_Tokens_List'][loc] = bt_str\n",
        "                    feats['Between_Tokens_Total'][loc] = segs[\"Between_Tokens_Total\"]\n",
        "                    feats['Between_Segments'][loc] = segs[\"Between_Segments\"]\n",
        "                    feats['Between_Max'][loc] = segs[\"Between_Max\"]\n",
        "                    feats['Between_Mean'][loc] = segs[\"Between_Mean\"]\n",
        "                    feats['Between_Has_Verb'][loc] = segs[\"Between_Has_Verb\"]\n",
        "                    feats['Between_Has_Subordinator'][loc] = segs[\"Between_Has_Subordinator\"]\n",
        "                    feats['Pre_Share'][loc] = pre_share\n",
        "                    feats['Between_Share'][loc] = between_share\n",
        "                    feats['Post_Share'][loc] = post_share\n",
        "                    feats['Comp_Count'][loc] = len(spans)\n",
        "                    feats['Comp_Labels'][loc] = json.dumps(comp_labels)\n",
        "                    feats['Pattern_Label'][loc] = pattern\n",
        "\n",
        "                    # legacy/other features\n",
        "                    feats['Total_Tokens'][loc] = total\n",
        "                    feats['Pre_Post_Ratio'][loc] = pre_post_ratio\n",
        "\n",
        "                    lemmas = [t.lemma_.lower() for t in tokens_nopunct if not t.is_stop]\n",
        "                    pos_tags = [t.pos_ for t in tokens_nopunct]\n",
        "                    pos_dist = Counter(pos_tags)\n",
        "\n",
        "                    # Sentiment (exploratory)\n",
        "                    pol = subj = np.nan\n",
        "                    if TextBlob is not None:\n",
        "                        try:\n",
        "                            blob = TextBlob(sent)\n",
        "                            pol, subj = blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                    # shallow structure summary\n",
        "                    comp_struct = {\n",
        "                        'has_explicit_comparator': any(t.text.lower() in ['like','as','than'] for t in doc),\n",
        "                        'comparator_type': str(comp).strip() or \"Unknown\",\n",
        "                        'comparative_adjectives': [t.text for t in doc if t.tag_ in ['JJR','RBR']],\n",
        "                        'superlative_adjectives': [t.text for t in doc if t.tag_ in ['JJS','RBS']],\n",
        "                        'modal_verbs': [t.text for t in doc if t.pos_ == 'AUX' and t.text.lower() in ['might','could','would','should','may']],\n",
        "                        'epistemic_markers': [t.text for t in doc if t.text.lower() in ['perhaps','maybe','possibly','apparently','seemingly']]\n",
        "                    }\n",
        "\n",
        "                    def depth(tok, d=0):\n",
        "                        if not list(tok.children):\n",
        "                            return d\n",
        "                        return max(depth(ch, d+1) for ch in tok.children)\n",
        "                    roots = [t for t in doc if t.head == t]\n",
        "                    try:\n",
        "                        complexity = max(depth(r) for r in roots) if roots else 0\n",
        "                    except Exception:\n",
        "                        complexity = np.nan\n",
        "\n",
        "                    slen = total\n",
        "                    adj = sum(1 for t in tokens_nopunct if t.pos_ == 'ADJ')\n",
        "                    vrb = sum(1 for t in tokens_nopunct if t.pos_ == 'VERB')\n",
        "                    nou = sum(1 for t in tokens_nopunct if t.pos_ == 'NOUN')\n",
        "\n",
        "                    figurative_markers = {'like','as','such','seem','appear','resemble','as if','as though'}\n",
        "                    fdens = (sum(1 for t in tokens_nopunct if t.text.lower() in figurative_markers) / total) if total else 0\n",
        "\n",
        "                    feats['Lemmatized_Text'][loc] = ' '.join(lemmas)\n",
        "                    feats['POS_Tags'][loc] = '; '.join(pos_tags)\n",
        "                    feats['POS_Distribution'][loc] = json.dumps(dict(pos_dist))\n",
        "                    feats['Sentiment_Polarity'][loc] = pol\n",
        "                    feats['Sentiment_Subjectivity'][loc] = subj\n",
        "                    feats['Comparative_Structure'][loc] = json.dumps(comp_struct)\n",
        "                    feats['Syntactic_Complexity'][loc] = complexity\n",
        "                    feats['Sentence_Length'][loc] = slen\n",
        "                    feats['Adjective_Count'][loc] = adj\n",
        "                    feats['Verb_Count'][loc] = vrb\n",
        "                    feats['Noun_Count'][loc] = nou\n",
        "                    feats['Figurative_Density'][loc] = fdens\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error in {name} row {ridx}: {e}\")\n",
        "\n",
        "            # Attaches features to DF\n",
        "            for k, v in feats.items():\n",
        "                df[k] = v\n",
        "\n",
        "            self.linguistic_features[name] = feats\n",
        "            self.datasets[name] = df\n",
        "            print(f\"Finished linguistic analysis for {name}.\")\n",
        "\n",
        "        print(\"All datasets processed.\")\n",
        "\n",
        "    def _perform_simplified_analysis(self):\n",
        "        # Fallback when spaCy is unavailable: still do segmentation using simple tokens\n",
        "        for name, df in list(self.datasets.items()):\n",
        "            if df.empty or 'Sentence_Context' not in df.columns:\n",
        "                continue\n",
        "\n",
        "            n = len(df)\n",
        "            # init output columns (same schema)\n",
        "            df['Pre_Tokens'] = 0\n",
        "            df['Post_Tokens'] = 0\n",
        "            df['Between_Tokens_List_Raw'] = \"0\"\n",
        "            df['Between_Tokens_List'] = \"0\"\n",
        "            df['Between_Tokens_Total'] = 0\n",
        "            df['Between_Segments'] = 0\n",
        "            df['Between_Max'] = 0\n",
        "            df['Between_Mean'] = 0.0\n",
        "            df['Between_Has_Verb'] = False\n",
        "            df['Between_Has_Subordinator'] = False\n",
        "            df['Pre_Share'] = np.nan\n",
        "            df['Between_Share'] = np.nan\n",
        "            df['Post_Share'] = np.nan\n",
        "            df['Comp_Count'] = 0\n",
        "            df['Comp_Labels'] = json.dumps([])\n",
        "            df['Pattern_Label'] = \"none\"\n",
        "\n",
        "            df['Total_Tokens'] = 0\n",
        "            df['Pre_Post_Ratio'] = np.nan\n",
        "            df['Sentiment_Polarity'] = np.nan\n",
        "            df['Sentiment_Subjectivity'] = np.nan\n",
        "            df['Sentence_Length'] = 0\n",
        "            df['Lemmatized_Text'] = None\n",
        "            df['POS_Tags'] = None\n",
        "            df['POS_Distribution'] = None\n",
        "            df['Comparative_Structure'] = None\n",
        "            df['Syntactic_Complexity'] = np.nan\n",
        "            df['Adjective_Count'] = 0\n",
        "            df['Verb_Count'] = 0\n",
        "            df['Noun_Count'] = 0\n",
        "            df['Figurative_Density'] = 0.0\n",
        "\n",
        "            for ridx, row in df.iterrows():\n",
        "                sent = str(row.get('Sentence_Context','') or '').strip()\n",
        "                if not sent:\n",
        "                    continue\n",
        "                tokens = self._simple_tokens(sent)\n",
        "                content = [t for t in tokens if not re.fullmatch(r\"[:;,.\\-\\(\\)…]\", str(t))]\n",
        "                total = len(content)\n",
        "\n",
        "                # detect spans and segment\n",
        "                spans = self._detect_comparator_spans(tokens, use_spacy=False)\n",
        "                comp_labels = [lab for *_ab, lab in spans]\n",
        "                segs = self._segments_from_spans(tokens, spans, use_spacy=False)\n",
        "\n",
        "                pre, post = segs[\"Pre_Tokens\"], segs[\"Post_Tokens\"]\n",
        "                bt_list = segs[\"Between_Tokens_List\"]\n",
        "                bt_str = self._fmt_between_list(bt_list)\n",
        "                denom = pre + post + segs[\"Between_Tokens_Total\"]\n",
        "                pre_share = (pre / denom) if denom else np.nan\n",
        "                between_share = (segs[\"Between_Tokens_Total\"] / denom) if denom else np.nan\n",
        "                post_share = (post / denom) if denom else np.nan\n",
        "                pre_post_ratio = (pre / post) if post > 0 else np.nan\n",
        "                pattern = self._pattern_label(comp_labels)\n",
        "\n",
        "                df.at[ridx, 'Total_Tokens'] = total\n",
        "                df.at[ridx, 'Sentence_Length'] = total\n",
        "                df.at[ridx, 'Pre_Tokens'] = pre\n",
        "                df.at[ridx, 'Post_Tokens'] = post\n",
        "                df.at[ridx, 'Pre_Post_Ratio'] = pre_post_ratio\n",
        "                df.at[ridx, 'Between_Tokens_List_Raw'] = bt_str   # bracketless or '0'\n",
        "                df.at[ridx, 'Between_Tokens_List'] = bt_str\n",
        "                df.at[ridx, 'Between_Tokens_Total'] = segs[\"Between_Tokens_Total\"]\n",
        "                df.at[ridx, 'Between_Segments'] = segs[\"Between_Segments\"]\n",
        "                df.at[ridx, 'Between_Max'] = segs[\"Between_Max\"]\n",
        "                df.at[ridx, 'Between_Mean'] = segs[\"Between_Mean\"]\n",
        "                df.at[ridx, 'Between_Has_Verb'] = segs[\"Between_Has_Verb\"]\n",
        "                df.at[ridx, 'Between_Has_Subordinator'] = segs[\"Between_Has_Subordinator\"]\n",
        "                df.at[ridx, 'Pre_Share'] = pre_share\n",
        "                df.at[ridx, 'Between_Share'] = between_share\n",
        "                df.at[ridx, 'Post_Share'] = post_share\n",
        "                df.at[ridx, 'Comp_Count'] = len(spans)\n",
        "                df.at[ridx, 'Comp_Labels'] = json.dumps(comp_labels)\n",
        "                df.at[ridx, 'Pattern_Label'] = pattern\n",
        "\n",
        "                # Sentiment if available\n",
        "                if TextBlob is not None:\n",
        "                    try:\n",
        "                        blob = TextBlob(sent)\n",
        "                        df.at[ridx, 'Sentiment_Polarity'] = blob.sentiment.polarity\n",
        "                        df.at[ridx, 'Sentiment_Subjectivity'] = blob.sentiment.subjectivity\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "            self.datasets[name] = df\n",
        "\n",
        "        print(\"Simplified analysis complete.\")\n",
        "\n",
        "    # ----------------- Instance-aligned F1 metrics -----------------\n",
        "    def _pair_rows_by_sentence(self, gold_df, pred_df, fuzzy_threshold=0.92):\n",
        "        if gold_df.empty or pred_df.empty:\n",
        "            return [], gold_df, pred_df\n",
        "\n",
        "        gold_df = gold_df.copy()\n",
        "        pred_df = pred_df.copy()\n",
        "        gold_df[\"_norm_sent\"] = gold_df[\"Sentence_Context\"].map(self._normalize_sentence_for_match)\n",
        "        pred_df[\"_norm_sent\"] = pred_df[\"Sentence_Context\"].map(self._normalize_sentence_for_match)\n",
        "\n",
        "        exact_pairs, used_pred = [], set()\n",
        "        pred_lookup = {}\n",
        "        for j, s in pred_df[\"_norm_sent\"].items():\n",
        "            pred_lookup.setdefault(s, []).append(j)\n",
        "\n",
        "        for i, s in gold_df[\"_norm_sent\"].items():\n",
        "            if s in pred_lookup:\n",
        "                js = [jj for jj in pred_lookup[s] if jj not in used_pred]\n",
        "                if js:\n",
        "                    j = js[0]\n",
        "                    used_pred.add(j)\n",
        "                    exact_pairs.append((i, j))\n",
        "\n",
        "        unmatched_gold = [i for i in gold_df.index if i not in {gi for gi,_ in exact_pairs}]\n",
        "        unmatched_pred = [j for j in pred_df.index if j not in used_pred]\n",
        "\n",
        "        fuzzy_pairs = []\n",
        "        for i in unmatched_gold:\n",
        "            best_j, best_r = None, 0.0\n",
        "            gi = gold_df.at[i, \"_norm_sent\"]\n",
        "            for j in unmatched_pred:\n",
        "                r = SequenceMatcher(None, gi, pred_df.at[j, \"_norm_sent\"]).ratio()\n",
        "                if r > best_r:\n",
        "                    best_r = r; best_j = j\n",
        "            if best_j is not None and best_r >= fuzzy_threshold:\n",
        "                used_pred.add(best_j)\n",
        "                fuzzy_pairs.append((i, best_j))\n",
        "\n",
        "        pairs = exact_pairs + fuzzy_pairs\n",
        "        return pairs, gold_df, pred_df\n",
        "\n",
        "    def _compute_f1_from_pairs(self, gold_df, pred_df, pairs, category_col=\"Category_Framework\"):\n",
        "        cats = sorted(set(gold_df[category_col].astype(str)) | set(pred_df[category_col].astype(str)))\n",
        "        TP = {c:0 for c in cats}; FP = {c:0 for c in cats}; FN = {c:0 for c in cats}\n",
        "\n",
        "        matched_gold = set(i for i,_ in pairs)\n",
        "        matched_pred = set(j for _,j in pairs)\n",
        "\n",
        "        for i,j in pairs:\n",
        "            g = str(gold_df.at[i, category_col])\n",
        "            p = str(pred_df.at[j, category_col])\n",
        "            if p == g: TP[g] += 1\n",
        "            else: FP[p] += 1; FN[g] += 1\n",
        "\n",
        "        for i in gold_df.index:\n",
        "            if i not in matched_gold: FN[str(gold_df.at[i, category_col])] += 1\n",
        "        for j in pred_df.index:\n",
        "            if j not in matched_pred: FP[str(pred_df.at[j, category_col])] += 1\n",
        "\n",
        "        metrics = {}; micro_tp = micro_fp = micro_fn = 0\n",
        "        for c in cats:\n",
        "            tp, fp, fn = TP[c], FP[c], FN[c]\n",
        "            prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "            f1   = (2*prec*rec)/(prec+rec) if (prec+rec) > 0 else 0.0\n",
        "            metrics[c] = {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
        "            micro_tp += tp; micro_fp += fp; micro_fn += fn\n",
        "\n",
        "        micro_prec = micro_tp / (micro_tp + micro_fp) if (micro_tp + micro_fp) > 0 else 0.0\n",
        "        micro_rec  = micro_tp / (micro_tp + micro_fn) if (micro_tp + micro_fn) > 0 else 0.0\n",
        "        micro_f1   = (2*micro_prec*micro_rec)/(micro_prec+micro_rec) if (micro_prec+micro_rec) > 0 else 0.0\n",
        "        macro_f1   = np.mean([m[\"f1\"] for m in metrics.values()]) if metrics else 0.0\n",
        "        return metrics, {\"micro_precision\": micro_prec, \"micro_recall\": micro_rec, \"micro_f1\": micro_f1, \"macro_f1\": macro_f1}\n",
        "\n",
        "    def calculate_corrected_f1_scores(self, fuzzy_threshold=0.92):\n",
        "        print(\"\\nCALCULATING INSTANCE-ALIGNED F1 METRICS\")\n",
        "        print(\"-\" * 44)\n",
        "        print(f\"Assumptions: sentence-level alignment (exact, then fuzzy ≥ {fuzzy_threshold}); category = 'Category_Framework'.\")\n",
        "\n",
        "        manual_df = self.datasets.get('manual', pd.DataFrame())\n",
        "        if manual_df.empty or 'Sentence_Context' not in manual_df or 'Category_Framework' not in manual_df:\n",
        "            print(\"F1 unavailable: manual annotations missing/invalid.\")\n",
        "            self.comparison_results['f1_analysis'] = None\n",
        "            return None, None\n",
        "\n",
        "        out = {}\n",
        "\n",
        "        def eval_one(pred_df, pred_name):\n",
        "            if pred_df.empty or 'Sentence_Context' not in pred_df or 'Category_Framework' not in pred_df:\n",
        "                print(f\"{pred_name}: dataset missing required columns.\")\n",
        "                return None\n",
        "            pairs, gdf, pdf = self._pair_rows_by_sentence(manual_df, pred_df, fuzzy_threshold=fuzzy_threshold)\n",
        "            metrics, overall = self._compute_f1_from_pairs(gdf, pdf, pairs)\n",
        "            print(f\"{pred_name}: pairs={len(pairs)}  micro-F1={overall['micro_f1']:.3f}  macro-F1={overall['macro_f1']:.3f}\")\n",
        "            return {\"pairs\": len(pairs), \"category_metrics\": metrics, \"overall\": overall}\n",
        "\n",
        "        rb = self.datasets.get('rule_based', pd.DataFrame())\n",
        "        nl = self.datasets.get('nlp', pd.DataFrame())\n",
        "\n",
        "        out['rule_based_vs_manual'] = eval_one(rb, \"Rule-Based vs Manual\")\n",
        "        out['nlp_vs_manual'] = eval_one(nl, \"NLP vs Manual\")\n",
        "\n",
        "        self.comparison_results['f1_analysis'] = out\n",
        "        primary = out['rule_based_vs_manual']['overall']['micro_f1'] if out.get('rule_based_vs_manual') else None\n",
        "        return out, primary\n",
        "\n",
        "    # ----------------- Save / Export -----------------\n",
        "    def save_comprehensive_results(self, output_path=\"comprehensive_linguistic_analysis_corrected.csv\"):\n",
        "        print(\"\\nSAVING COMPREHENSIVE RESULTS …\")\n",
        "        frames = []\n",
        "        for name, df in self.datasets.items():\n",
        "            if df is None or df.empty:\n",
        "                continue\n",
        "            d = df.copy()\n",
        "            for col, default in [\n",
        "                ('Original_Dataset', name),\n",
        "                ('Instance_ID', None),\n",
        "                ('Sentence_Context', None),\n",
        "                ('Category_Framework', None),\n",
        "                ('Comparator_Type', None)\n",
        "            ]:\n",
        "                if col not in d.columns:\n",
        "                    d[col] = default\n",
        "\n",
        "            if d['Instance_ID'].isna().any() or (not d['Instance_ID'].astype(str).is_unique):\n",
        "                d = self._ensure_ids(d, name)\n",
        "\n",
        "            base = ['Instance_ID','Original_Dataset','Sentence_Context','Category_Framework','Comparator_Type']\n",
        "            others = [c for c in d.columns if c not in base]\n",
        "            d = d[base + others]\n",
        "            frames.append(d)\n",
        "\n",
        "        if not frames:\n",
        "            print(\"No data to save.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        combined = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "        # Stable sort: Manual → Restrictive → Less-Restrictive PG → BNC\n",
        "        order = {\n",
        "            'Manual_CloseReading': 1,\n",
        "            'Restrictive_Dubliners': 2,\n",
        "            'NLP_LessRestrictive_PG': 3,\n",
        "            'BNC_Baseline': 4\n",
        "        }\n",
        "        combined['__order__'] = combined['Original_Dataset'].map(order).fillna(99).astype(int)\n",
        "\n",
        "        def _id_numeric_tail(x):\n",
        "            m = re.search(r'(\\d+)$', str(x))\n",
        "            return int(m.group(1)) if m else 0\n",
        "\n",
        "        combined = combined.sort_values(\n",
        "            by=['__order__','Original_Dataset','Instance_ID'],\n",
        "            key=lambda s: s.map(_id_numeric_tail) if s.name == 'Instance_ID' else s\n",
        "        ).drop(columns='__order__')\n",
        "\n",
        "        combined.to_csv(output_path, index=False)\n",
        "        print(f\"Saved: {output_path}\")\n",
        "        print(\"Integrity:\",\n",
        "              \"missing Instance_ID =\", combined['Instance_ID'].isna().sum(),\n",
        "              \"| missing Original_Dataset =\", combined['Original_Dataset'].isna().sum(),\n",
        "              \"| rows =\", len(combined))\n",
        "        print(\"Environment (for reproducibility):\", self.env_info)\n",
        "        return combined\n",
        "\n",
        "\n",
        "# ========= RUN THE PIPELINE (sets paths) =========\n",
        "manual_file_content = \"/content/All Similes - Dubliners cont.csv\"\n",
        "rule_based_file_content = \"/content/dubliners_rulebased_extraction.csv\"\n",
        "nlp_file_content = \"/content/dubliners_nlp_less_restrictive_extraction.csv\"\n",
        "bnc_file_content = \"/content/bnc_processed_similes.csv\"\n",
        "\n",
        "comparator = ComprehensiveLinguisticComparator()\n",
        "comparator.load_datasets(manual_file_content, rule_based_file_content, nlp_file_content, bnc_file_content)\n",
        "comparator.perform_comprehensive_linguistic_analysis()\n",
        "f1_analysis, primary_f1 = comparator.calculate_corrected_f1_scores()  # instance-aligned F1\n",
        "results_df = comparator.save_comprehensive_results(\"comprehensive_linguistic_analysis.csv\")\n",
        "\n",
        "print(\"\\nPIPELINE COMPLETED.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAPbOHXnRx2M",
        "outputId": "c2fd33f5-e10a-4d6d-ae58-552200ad6a56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS (FIXED + SEGMENTS)\n",
            "===========================================================================\n",
            "Dataset 1: Manual Annotations (Ground Truth - Close Reading)\n",
            "Dataset 2: Rule-Based Extraction (Restrictive - Domain-Informed)\n",
            "Dataset 3: NLP Extraction (Less-Restrictive - PG Dubliners)\n",
            "Dataset 4: BNC Baseline Corpus (Standard English Reference)\n",
            "===========================================================================\n",
            "spaCy pipeline loaded: en_core_web_sm\n",
            "Environment: {'python': '3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]', 'pandas': '2.2.2', 'numpy': '2.0.2', 'spacy': '3.8.7', 'textblob': 'n/a'}\n",
            "\n",
            "LOADING DATASETS WITH FIXED ID HANDLING & EXPLICIT LABELS\n",
            "----------------------------------------------------------------------\n",
            "Loading manual annotations…\n",
            "Loading rule-based (restrictive)…\n",
            "Loading NLP (less-restrictive PG)…\n",
            "Loading BNC baseline…\n",
            "Standardizing column names & adding Dataset_Source…\n",
            "Standardization complete.\n",
            "Harmonizing Category_Framework labels…\n",
            "Category harmonization complete.\n",
            "      manual: rows= 184  missing_IDs=0  missing_Original_Dataset=0\n",
            "  rule_based: rows= 218  missing_IDs=0  missing_Original_Dataset=0\n",
            "         nlp: rows= 330  missing_IDs=0  missing_Original_Dataset=0\n",
            "         bnc: rows= 200  missing_IDs=0  missing_Original_Dataset=0\n",
            "Total instances: 932\n",
            "\n",
            "PERFORMING LINGUISTIC ANALYSIS (with pre/between/post segmentation)\n",
            "------------------------------------------------------------\n",
            "Finished linguistic analysis for manual.\n",
            "Finished linguistic analysis for rule_based.\n",
            "Finished linguistic analysis for nlp.\n",
            "Finished linguistic analysis for bnc.\n",
            "All datasets processed.\n",
            "\n",
            "CALCULATING INSTANCE-ALIGNED F1 METRICS\n",
            "--------------------------------------------\n",
            "Assumptions: sentence-level alignment (exact, then fuzzy ≥ 0.92); category = 'Category_Framework'.\n",
            "Rule-Based vs Manual: pairs=110  micro-F1=0.343  macro-F1=0.178\n",
            "NLP vs Manual: pairs=127  micro-F1=0.292  macro-F1=0.059\n",
            "\n",
            "SAVING COMPREHENSIVE RESULTS …\n",
            "Saved: comprehensive_linguistic_analysis.csv\n",
            "Integrity: missing Instance_ID = 0 | missing Original_Dataset = 0 | rows = 932\n",
            "Environment (for reproducibility): {'python': '3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]', 'pandas': '2.2.2', 'numpy': '2.0.2', 'spacy': '3.8.7', 'textblob': 'n/a'}\n",
            "\n",
            "PIPELINE COMPLETED.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Statistical Significance Testing\n",
        "# 7.1 Multi-Group Comparative Analysis\n",
        "The statistical analysis distinguishes between Joyce Manual, Joyce Restrictive, Joyce Less-Restrictive, and BNC subsets to provide granular assessment of methodological differences.\n",
        "\n",
        "# 7.2 Robust Statistical Framework\n",
        "Implementation includes:\n",
        "\n",
        "Four-way chi-square analysis for categorical distribution testing\n",
        "Newcombe-Wilson confidence intervals for two-proportion comparisons\n",
        "Binomial testing against BNC reference proportions\n",
        "Welch t-tests and Mann-Whitney U tests for continuous feature assessment\n",
        "\n",
        "# 7.3 Topic Modeling Integration\n",
        "Latent Dirichlet Allocation provides thematic analysis across all dataset subsets, revealing content-based distinctions complementing statistical findings."
      ],
      "metadata": {
        "id": "ft6Meu_eeMDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ROBUST STATISTICAL SIGNIFICANCE + TOPIC MODELLING (Joyce subsets vs BNC)\n",
        "# =============================================================================\n",
        "\n",
        "import os, json, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from math import asin, sqrt\n",
        "from scipy.stats import chi2_contingency, mannwhitneyu, ttest_ind, binomtest, norm, fisher_exact\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "try:\n",
        "    from statsmodels.stats.proportion import proportions_ztest, confint_proportions_2indep\n",
        "    _HAS_STATSMODELS = True\n",
        "except Exception:\n",
        "    _HAS_STATSMODELS = False\n",
        "\n",
        "# ---------- Setup ----------\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "out_dir = os.path.join(\"analysis_outputs\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "print(\"\\nROBUST STATISTICAL ANALYSIS (Joyce subsets vs BNC)\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# --- Sanity: results_df must exist from Cell 1 ---\n",
        "if 'results_df' not in globals() or results_df is None or results_df.empty:\n",
        "    raise RuntimeError(\"results_df not found or empty. Run Cell 1 first.\")\n",
        "\n",
        "# --- Group labels (from Cell 1 'Original_Dataset') ---\n",
        "LABELS = {\n",
        "    \"Manual_CloseReading\":      \"Joyce_Manual\",\n",
        "    \"Restrictive_Dubliners\":    \"Joyce_Restrictive\",\n",
        "    \"NLP_LessRestrictive_PG\":   \"Joyce_LessRestrictive\",\n",
        "    \"BNC_Baseline\":             \"BNC\"\n",
        "}\n",
        "\n",
        "df = results_df.copy()\n",
        "if \"Original_Dataset\" not in df.columns:\n",
        "    raise RuntimeError(\"results_df is missing 'Original_Dataset'.\")\n",
        "if \"Category_Framework\" not in df.columns:\n",
        "    raise RuntimeError(\"results_df is missing 'Category_Framework'.\")\n",
        "\n",
        "df[\"__Group__\"] = df[\"Original_Dataset\"].map(LABELS).fillna(df[\"Original_Dataset\"])\n",
        "\n",
        "# --- Category whitelist (Joycean_Quasi was merged → keep unified 'Quasi_Similes') ---\n",
        "KNOWN_CATEGORIES = {\n",
        "    'Standard',\n",
        "    'Quasi_Similes',          # unified quasi-simile label\n",
        "    'Joycean_Quasi_Fuzzy',\n",
        "    'Joycean_Framed',\n",
        "    'Joycean_Silent',\n",
        "    'Uncategorized'\n",
        "}\n",
        "\n",
        "bad_mask = ~df[\"Category_Framework\"].isin(KNOWN_CATEGORIES)\n",
        "if bad_mask.any():\n",
        "    dropped = int(bad_mask.sum())\n",
        "    print(f\"[WARN] Dropping {dropped} rows with unexpected Category_Framework values: \"\n",
        "          f\"{sorted(df.loc[bad_mask, 'Category_Framework'].astype(str).unique())}\")\n",
        "    df = df.loc[~bad_mask].copy()\n",
        "\n",
        "# --- Quick distribution sanity table (post-harmonisation) ---\n",
        "dist = (df.pivot_table(index=\"Category_Framework\",\n",
        "                       columns=\"Original_Dataset\",\n",
        "                       values=\"Instance_ID\",\n",
        "                       aggfunc=\"count\", fill_value=0)\n",
        "          .assign(Total=lambda x: x.sum(1))\n",
        "          .sort_values(\"Total\", ascending=False))\n",
        "print(\"\\nCategory distribution after harmonisation (counts):\")\n",
        "print(dist)\n",
        "\n",
        "# --- Split groups ---\n",
        "groups = {\n",
        "    \"Joyce_Manual\":          df[df[\"__Group__\"]==\"Joyce_Manual\"],\n",
        "    \"Joyce_Restrictive\":     df[df[\"__Group__\"]==\"Joyce_Restrictive\"],\n",
        "    \"Joyce_LessRestrictive\": df[df[\"__Group__\"]==\"Joyce_LessRestrictive\"],\n",
        "    \"BNC\":                   df[df[\"__Group__\"]==\"BNC\"]\n",
        "}\n",
        "for gname, gdf in groups.items():\n",
        "    print(f\"{gname:22s}: {len(gdf)} rows\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def p_adjust_bh(p):\n",
        "    \"\"\"Benjamini–Hochberg FDR for a 1D array-like of p-values.\"\"\"\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    n = p.size\n",
        "    order = np.argsort(p)\n",
        "    ranked = np.empty(n, dtype=float)\n",
        "    cummin = 1.0\n",
        "    for i, idx in enumerate(order[::-1], start=1):\n",
        "        rank = n - i + 1\n",
        "        val = p[idx] * n / rank\n",
        "        cummin = min(cummin, val)\n",
        "        ranked[idx] = cummin\n",
        "    return np.minimum(ranked, 1.0)\n",
        "\n",
        "def cramers_v(chi2, n, r, c):\n",
        "    \"\"\"Cramér's V for r x c table.\"\"\"\n",
        "    if n <= 0 or min(r, c) <= 1:\n",
        "        return np.nan\n",
        "    return sqrt(chi2 / (n * (min(r, c) - 1)))\n",
        "\n",
        "def cohens_h(p1, p2):\n",
        "    \"\"\"Cohen's h for proportions.\"\"\"\n",
        "    p1 = min(max(float(p1), 0.0), 1.0)\n",
        "    p2 = min(max(float(p2), 0.0), 1.0)\n",
        "    return 2 * (asin(sqrt(p1)) - asin(sqrt(p2)))\n",
        "\n",
        "def hedges_g(a, b):\n",
        "    \"\"\"Hedges' g (small-sample corrected Cohen's d).\"\"\"\n",
        "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
        "    na, nb = len(a), len(b)\n",
        "    if na < 2 or nb < 2:\n",
        "        return np.nan\n",
        "    s1 = np.var(a, ddof=1); s2 = np.var(b, ddof=1)\n",
        "    pooled = ((na-1)*s1 + (nb-1)*s2) / (na+nb-2) if (na+nb-2) > 0 else np.nan\n",
        "    if not np.isfinite(pooled) or pooled <= 0:\n",
        "        return np.nan\n",
        "    d = (np.mean(a) - np.mean(b)) / np.sqrt(pooled)\n",
        "    J = 1 - (3 / (4*(na+nb) - 9)) if (na+nb) > 2 else 1.0\n",
        "    return d * J\n",
        "\n",
        "def cliffs_delta_from_u(a, b):\n",
        "    \"\"\"Cliff's delta via Mann–Whitney U (orientation A > B).\"\"\"\n",
        "    a = pd.Series(a).dropna().to_numpy()\n",
        "    b = pd.Series(b).dropna().to_numpy()\n",
        "    if len(a) == 0 or len(b) == 0:\n",
        "        return np.nan\n",
        "    u_ab, _ = mannwhitneyu(a, b, alternative=\"greater\")\n",
        "    return (2 * u_ab) / (len(a)*len(b)) - 1\n",
        "\n",
        "# ---------- 1) 4-way Chi-square on Category_Framework ----------\n",
        "cats = sorted(KNOWN_CATEGORIES)\n",
        "contingency_4way = pd.DataFrame(\n",
        "    {\n",
        "        \"Joyce_Manual\":          [groups[\"Joyce_Manual\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "        \"Joyce_Restrictive\":     [groups[\"Joyce_Restrictive\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "        \"Joyce_LessRestrictive\": [groups[\"Joyce_LessRestrictive\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "        \"BNC\":                   [groups[\"BNC\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "    },\n",
        "    index=cats\n",
        ")\n",
        "\n",
        "sim_used = False\n",
        "try:\n",
        "    chi2_4, p_4, dof_4, exp_4 = chi2_contingency(contingency_4way, correction=False)\n",
        "    if (np.asarray(exp_4) < 5).any():\n",
        "        try:\n",
        "            chi2_4, p_4, dof_4, exp_4 = chi2_contingency(\n",
        "                contingency_4way, correction=False, simulate_pval=True, num_simulation=5000\n",
        "            )\n",
        "            sim_used = True\n",
        "        except TypeError:\n",
        "            pass\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"chi2_contingency failed: {e}\")\n",
        "\n",
        "N_total = contingency_4way.values.sum()\n",
        "V_4 = cramers_v(chi2_4, N_total, *contingency_4way.shape)\n",
        "\n",
        "print(\"\\n4-way Chi-square on Category_Framework (Joyce subsets vs BNC):\")\n",
        "print(f\"χ² = {chi2_4:.4f} | df = {dof_4} | p = {p_4:.6f} | Cramér’s V = {V_4:.3f} | Monte-Carlo={sim_used}\")\n",
        "\n",
        "# Save contingency + expected + standardized residuals + per-cell p-values (FDR)\n",
        "path_cont_4 = os.path.join(out_dir, f\"chi2_contingency_by_subset_{ts}.csv\")\n",
        "path_exp_4  = os.path.join(out_dir, f\"chi2_expected_by_subset_{ts}.csv\")\n",
        "contingency_4way.to_csv(path_cont_4)\n",
        "exp_df_4 = pd.DataFrame(exp_4, index=cats, columns=contingency_4way.columns)\n",
        "exp_df_4.to_csv(path_exp_4)\n",
        "\n",
        "pearson_z = (contingency_4way - exp_df_4) / np.sqrt(exp_df_4.replace(0, np.nan))\n",
        "cell_pvals = pearson_z.applymap(lambda z: 2*norm.sf(abs(z)) if pd.notnull(z) else np.nan)\n",
        "\n",
        "flat = cell_pvals.values.flatten()\n",
        "mask = np.isfinite(flat)\n",
        "adj = np.full_like(flat, np.nan, dtype=float)\n",
        "if mask.any():\n",
        "    adj[mask] = p_adjust_bh(flat[mask])\n",
        "cell_pvals_adj = pd.DataFrame(adj.reshape(cell_pvals.shape), index=cell_pvals.index, columns=cell_pvals.columns)\n",
        "\n",
        "path_resid_z = os.path.join(out_dir, f\"chi2_pearson_z_by_subset_{ts}.csv\")\n",
        "path_resid_p = os.path.join(out_dir, f\"chi2_cell_p_by_subset_{ts}.csv\")\n",
        "path_resid_padj = os.path.join(out_dir, f\"chi2_cell_padj_BH_by_subset_{ts}.csv\")\n",
        "pearson_z.to_csv(path_resid_z)\n",
        "cell_pvals.to_csv(path_resid_p)\n",
        "cell_pvals_adj.to_csv(path_resid_padj)\n",
        "\n",
        "# Print the strongest drivers (optional but helpful)\n",
        "absz = pearson_z.abs().stack().sort_values(ascending=False)\n",
        "print(\"\\nTop 10 standardized residuals (|z|):\")\n",
        "for (cat, grp), z in absz.head(10).items():\n",
        "    obs = contingency_4way.loc[cat, grp]\n",
        "    exp = exp_df_4.loc[cat, grp]\n",
        "    print(f\"  {grp:22s} | {cat:20s}  z={z:6.2f}  obs={obs} exp={exp:.1f}\")\n",
        "\n",
        "# ---------- 2) Two-proportion tests (each Joyce subset vs BNC) ----------\n",
        "print(\"\\nTwo-proportion tests (Newcombe–Wilson) for each Joyce subset vs BNC:\")\n",
        "two_prop_rows = []\n",
        "bnc_total = len(groups[\"BNC\"])\n",
        "bnc_counts = groups[\"BNC\"][\"Category_Framework\"].value_counts()\n",
        "\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "    subset_total = len(groups[subset])\n",
        "    subset_counts = groups[subset][\"Category_Framework\"].value_counts()\n",
        "    for cat in cats:\n",
        "        cA = subset_counts.get(cat,0); nA = subset_total\n",
        "        cB = bnc_counts.get(cat,0);    nB = bnc_total\n",
        "        pA = cA/nA if nA>0 else 0.0\n",
        "        pB = cB/nB if nB>0 else 0.0\n",
        "        row = {\"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset, \"Category\":cat,\n",
        "               \"count_A\":cA, \"n_A\":nA, \"prop_A\":pA, \"count_B\":cB, \"n_B\":nB, \"prop_B\":pB,\n",
        "               \"cohens_h\": float(cohens_h(pA, pB))}\n",
        "        # Skip boundary cases where both groups are 0 or 1 for this category\n",
        "        if (cA == 0 and cB == 0) or (cA == nA and cB == nB) or (nA == 0 or nB == 0):\n",
        "            row.update({\"z\": np.nan, \"p_value\": 1.0, \"CI_low\": np.nan, \"CI_up\": np.nan})\n",
        "            print(f\"  {subset:22s} | {cat:20s} (both groups at boundary → skip) h={row['cohens_h']:.3f}\")\n",
        "            two_prop_rows.append(row); continue\n",
        "        if _HAS_STATSMODELS:\n",
        "            try:\n",
        "                z, pz = proportions_ztest(np.array([cA,cB]), np.array([nA,nB]))\n",
        "            except Exception:\n",
        "                # Haldane–Anscombe continuity correction fallback\n",
        "                pA_ha = (cA + 0.5) / (nA + 1)\n",
        "                pB_ha = (cB + 0.5) / (nB + 1)\n",
        "                se = np.sqrt(pA_ha*(1-pA_ha)/(nA+1) + pB_ha*(1-pB_ha)/(nB+1))\n",
        "                z = (pA_ha - pB_ha) / se if se>0 else np.nan\n",
        "                pz = 2*norm.sf(abs(z)) if np.isfinite(z) else np.nan\n",
        "            ci_low, ci_up = (np.nan, np.nan)\n",
        "            try:\n",
        "                ci_low, ci_up = confint_proportions_2indep(cA, nA, cB, nB, method=\"newcombe\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            row.update({\"z\":float(z), \"p_value\":float(pz), \"CI_low\":float(ci_low), \"CI_up\":float(ci_up)})\n",
        "            print(f\"  {subset:22s} | {cat:20s} z={z:6.3f} p={pz:.6g} CI[{ci_low:.3f},{ci_up:.3f}] h={row['cohens_h']:.3f}\")\n",
        "        else:\n",
        "            row.update({\"z\":np.nan, \"p_value\":np.nan, \"CI_low\":np.nan, \"CI_up\":np.nan})\n",
        "            print(f\"  {subset:22s} | {cat:20s} (statsmodels unavailable → skipping z/CI) h={row['cohens_h']:.3f}\")\n",
        "        two_prop_rows.append(row)\n",
        "\n",
        "two_prop_df = pd.DataFrame(two_prop_rows)\n",
        "if \"p_value\" in two_prop_df.columns:\n",
        "    two_prop_df[\"p_adj_BH\"] = p_adjust_bh(two_prop_df[\"p_value\"].fillna(1.0).to_numpy())\n",
        "path_two_prop = os.path.join(out_dir, f\"two_prop_newcombe_by_subset_{ts}.csv\")\n",
        "two_prop_df.to_csv(path_two_prop, index=False)\n",
        "\n",
        "# ---------- 3) Binomial tests (subset vs BNC reference proportion) ----------\n",
        "print(\"\\nBinomial tests (each Joyce subset vs BNC category proportion):\")\n",
        "binom_rows = []\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "    nA = len(groups[subset])\n",
        "    subset_counts = groups[subset][\"Category_Framework\"].value_counts()\n",
        "    for cat in cats:\n",
        "        cA = subset_counts.get(cat,0)\n",
        "        cB = bnc_counts.get(cat,0); nB = bnc_total\n",
        "        p_ref = (cB/nB) if nB>0 else 0.0\n",
        "        if nA>0 and 0 < p_ref < 1:\n",
        "            bt = binomtest(cA, n=nA, p=p_ref)\n",
        "            pv = bt.pvalue\n",
        "            binom_rows.append({\"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset, \"Category\":cat,\n",
        "                               \"count_A\":cA, \"n_A\":nA, \"p_ref_BNC\":p_ref, \"p_value\":pv})\n",
        "            print(f\"  {subset:22s} | {cat:20s} {cA}/{nA} vs p_ref={p_ref:.4f} p={pv:.6g}\")\n",
        "        else:\n",
        "            binom_rows.append({\"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset, \"Category\":cat,\n",
        "                               \"count_A\":cA, \"n_A\":nA, \"p_ref_BNC\":p_ref, \"p_value\":np.nan})\n",
        "\n",
        "binom_df = pd.DataFrame(binom_rows)\n",
        "if \"p_value\" in binom_df.columns:\n",
        "    binom_df[\"p_adj_BH\"] = p_adjust_bh(binom_df[\"p_value\"].fillna(1.0).to_numpy())\n",
        "path_binom = os.path.join(out_dir, f\"binomial_tests_by_subset_{ts}.csv\")\n",
        "binom_df.to_csv(path_binom, index=False)\n",
        "\n",
        "# ---------- 4) Continuous features (subset vs BNC) ----------\n",
        "print(\"\\nContinuous features (Welch t + Mann–Whitney U) each Joyce subset vs BNC:\")\n",
        "continuous_feats = [\"Sentence_Length\",\"Pre_Post_Ratio\",\"Sentiment_Polarity\",\"Sentiment_Subjectivity\"]\n",
        "cont_rows = []\n",
        "\n",
        "for feat in continuous_feats:\n",
        "    for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "        A = pd.to_numeric(groups[subset].get(feat, pd.Series(dtype=float)), errors=\"coerce\").dropna()\n",
        "        B = pd.to_numeric(groups[\"BNC\"].get(feat, pd.Series(dtype=float)), errors=\"coerce\").dropna()\n",
        "        row = {\"Feature\":feat, \"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset,\n",
        "               \"A_n\":int(A.shape[0]), \"B_n\":int(B.shape[0])}\n",
        "        if len(A)>10 and len(B)>10:\n",
        "            t,p_t = ttest_ind(A,B,equal_var=False)\n",
        "            u,p_u = mannwhitneyu(A,B,alternative=\"two-sided\")\n",
        "            row.update({\n",
        "                \"A_mean\":float(A.mean()), \"A_median\":float(A.median()),\n",
        "                \"B_mean\":float(B.mean()), \"B_median\":float(B.median()),\n",
        "                \"t_stat\":float(t), \"t_pvalue\":float(p_t),\n",
        "                \"U_stat\":float(u), \"U_pvalue\":float(p_u),\n",
        "                \"hedges_g\": float(hedges_g(A, B)),\n",
        "                \"cliffs_delta\": float(cliffs_delta_from_u(A, B))\n",
        "            })\n",
        "            print(f\"  {feat:22s} | {subset:22s} t={t:7.3f} p={p_t:.6g} | U={u:9.1f} p={p_u:.6g} | g={row['hedges_g']:.3f} δ={row['cliffs_delta']:.3f}\")\n",
        "        cont_rows.append(row)\n",
        "\n",
        "cont_df = pd.DataFrame(cont_rows)\n",
        "if \"t_pvalue\" in cont_df.columns:\n",
        "    cont_df[\"t_padj_BH\"] = p_adjust_bh(cont_df[\"t_pvalue\"].fillna(1.0).to_numpy())\n",
        "if \"U_pvalue\" in cont_df.columns:\n",
        "    cont_df[\"U_padj_BH\"] = p_adjust_bh(cont_df[\"U_pvalue\"].fillna(1.0).to_numpy())\n",
        "\n",
        "path_cont = os.path.join(out_dir, f\"continuous_tests_by_subset_{ts}.csv\")\n",
        "cont_df.to_csv(path_cont, index=False)\n",
        "\n",
        "# ---------- 4B) Comparator-segment metrics (Pre / Between / Post) — integrated from results_df ----------\n",
        "print(\"\\nComparator-segment statistics (Pre / Between / Post) — integrated (no external CSV)\")\n",
        "seg_cols = [\n",
        "    \"Pre_Tokens\",\"Between_Tokens_Total\",\"Post_Tokens\",\"Total_Tokens\",\n",
        "    \"Pre_Share\",\"Between_Share\",\"Post_Share\",\n",
        "    \"Comp_Count\",\"Between_Segments\",\"Between_Max\",\"Between_Mean\",\"Between_Has_Verb\",\"Between_Has_Subordinator\"\n",
        "]\n",
        "\n",
        "seg_rows = []\n",
        "for feat in [c for c in seg_cols if c in df.columns]:\n",
        "    for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "        A = pd.to_numeric(groups[subset].get(feat, pd.Series(dtype=float)), errors=\"coerce\").dropna()\n",
        "        B = pd.to_numeric(groups[\"BNC\"].get(feat, pd.Series(dtype=float)), errors=\"coerce\").dropna()\n",
        "        row = {\"Feature\": feat, \"Comparison\": f\"{subset}_vs_BNC\", \"Subset\": subset,\n",
        "               \"A_n\": int(A.shape[0]), \"B_n\": int(B.shape[0])}\n",
        "        # For booleans (Has_Verb/Subordinator), treat as numeric 0/1\n",
        "        if feat in [\"Between_Has_Verb\",\"Between_Has_Subordinator\"]:\n",
        "            A = A.astype(float); B = B.astype(float)\n",
        "        if len(A) > 10 and len(B) > 10 and np.isfinite(A).any() and np.isfinite(B).any():\n",
        "            t, p_t = ttest_ind(A, B, equal_var=False)\n",
        "            u, p_u = mannwhitneyu(A, B, alternative=\"two-sided\")\n",
        "            row.update({\n",
        "                \"A_mean\": float(A.mean()), \"A_median\": float(A.median()),\n",
        "                \"B_mean\": float(B.mean()), \"B_median\": float(B.median()),\n",
        "                \"t_stat\": float(t), \"t_pvalue\": float(p_t),\n",
        "                \"U_stat\": float(u), \"U_pvalue\": float(p_u),\n",
        "                \"hedges_g\": float(hedges_g(A, B)),\n",
        "                \"cliffs_delta\": float(cliffs_delta_from_u(A, B))\n",
        "            })\n",
        "            print(f\"  {feat:20s} | {subset:22s} t={t:7.3f} p={p_t:.3g} | U={u:9.1f} p={p_u:.3g} | g={row['hedges_g']:.3f} δ={row['cliffs_delta']:.3f}\")\n",
        "        seg_rows.append(row)\n",
        "\n",
        "seg_cont_df = pd.DataFrame(seg_rows)\n",
        "if \"t_pvalue\" in seg_cont_df.columns:\n",
        "    seg_cont_df[\"t_padj_BH\"] = p_adjust_bh(seg_cont_df[\"t_pvalue\"].fillna(1.0).to_numpy())\n",
        "if \"U_pvalue\" in seg_cont_df.columns:\n",
        "    seg_cont_df[\"U_padj_BH\"] = p_adjust_bh(seg_cont_df[\"U_pvalue\"].fillna(1.0).to_numpy())\n",
        "\n",
        "path_seg_cont = os.path.join(out_dir, f\"continuous_segment_tests_by_subset_{ts}.csv\")\n",
        "seg_cont_df.to_csv(path_seg_cont, index=False)\n",
        "\n",
        "# ---- (ii) Does 'Between > 0' differ by group? (χ² with robust fallbacks) ----\n",
        "col_between = df.get(\"Between_Tokens_Total\", pd.Series(index=df.index)).fillna(0)\n",
        "tmp = df.assign(Seg_Between_gt0=(pd.to_numeric(col_between, errors=\"coerce\").fillna(0) > 0).astype(int))\n",
        "\n",
        "pres = tmp.groupby(\"__Group__\")[\"Seg_Between_gt0\"].agg([\"sum\",\"count\"])\n",
        "order = [g for g in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\",\"BNC\"] if g in pres.index]\n",
        "pres = pres.loc[order]\n",
        "\n",
        "presence_tbl = pd.DataFrame({\n",
        "    \"Between>0\": pres[\"sum\"].astype(int),\n",
        "    \"Between=0\": (pres[\"count\"] - pres[\"sum\"]).astype(int)\n",
        "})\n",
        "\n",
        "print(\"\\nBetween-clause presence × Group (χ²):\")\n",
        "print(presence_tbl if not presence_tbl.empty else \"(no usable data)\")\n",
        "\n",
        "if presence_tbl.shape[0] >= 2 and presence_tbl.shape[1] == 2:\n",
        "    exp_seg = None\n",
        "    try:\n",
        "        chi2_seg, p_seg, dof_seg, exp_seg = chi2_contingency(presence_tbl, correction=False)\n",
        "        V_seg = cramers_v(chi2_seg, presence_tbl.values.sum(), *presence_tbl.shape)\n",
        "        print(f\"  χ² = {chi2_seg:.3f} | df = {dof_seg} | p = {p_seg:.6f} | Cramér’s V = {V_seg:.3f}\")\n",
        "    except ValueError:\n",
        "        try:\n",
        "            chi2_seg, p_seg, dof_seg, exp_seg = chi2_contingency(\n",
        "                presence_tbl, correction=False, simulate_pval=True, num_simulation=5000\n",
        "            )\n",
        "            V_seg = cramers_v(chi2_seg, presence_tbl.values.sum(), *presence_tbl.shape)\n",
        "            print(f\"  Monte-Carlo χ² = {chi2_seg:.3f} | df = {dof_seg} | p = {p_seg:.6f} | V = {V_seg:.3f}\")\n",
        "        except TypeError:\n",
        "            print(\"  χ² not valid (zero expected). Pairwise Fisher exact p-values vs BNC:\")\n",
        "            for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "                if subset in presence_tbl.index and \"BNC\" in presence_tbl.index:\n",
        "                    tbl2 = presence_tbl.loc[[subset, \"BNC\"]].to_numpy()\n",
        "                    _, p_f = fisher_exact(tbl2)  # two-sided\n",
        "                    print(f\"   - {subset:22s}: p = {p_f:.6g}\")\n",
        "            p_seg = np.nan; chi2_seg = np.nan; dof_seg = 1\n",
        "    # Save presence table + expected + residuals if we have them\n",
        "    path_presence_csv = os.path.join(out_dir, f\"between_presence_by_group_{ts}.csv\")\n",
        "    presence_tbl.to_csv(path_presence_csv)\n",
        "    if 'exp_seg' in locals() and exp_seg is not None:\n",
        "        path_presence_exp = os.path.join(out_dir, f\"between_presence_expected_by_group_{ts}.csv\")\n",
        "        path_presence_resid = os.path.join(out_dir, f\"between_presence_residuals_by_group_{ts}.csv\")\n",
        "        exp_df = pd.DataFrame(exp_seg, index=presence_tbl.index, columns=presence_tbl.columns)\n",
        "        exp_df.to_csv(path_presence_exp)\n",
        "        resid = (presence_tbl - exp_df) / np.sqrt(exp_df.replace(0, np.nan))\n",
        "        resid.to_csv(path_presence_resid)\n",
        "    else:\n",
        "        path_presence_exp = path_presence_resid = None\n",
        "else:\n",
        "    print(\"  Not enough non-zero groups/columns for χ²; skipping.\")\n",
        "    path_presence_csv = path_presence_exp = path_presence_resid = None\n",
        "\n",
        "# ---------- 4C) Pattern/shaping statistics (anchors, patterns, between diagnostics, declared comparator) ----------\n",
        "print(\"\\nPattern/shaping statistics (anchors, patterns, between diagnostics, declared comparator)\")\n",
        "\n",
        "work = df.copy()\n",
        "\n",
        "# Parse Comparative_Structure JSON safely\n",
        "def _as_dict(x):\n",
        "    if isinstance(x, dict):\n",
        "        return x\n",
        "    if isinstance(x, str) and x.strip():\n",
        "        try:\n",
        "            return json.loads(x)\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "comp_struct = work.get(\"Comparative_Structure\", pd.Series(index=work.index)).apply(_as_dict)\n",
        "\n",
        "# Helper: safe Sentence_Context string\n",
        "def _sent(x):\n",
        "    return str(x) if pd.notnull(x) else \"\"\n",
        "\n",
        "# Flags from Comparative_Structure\n",
        "work[\"has_modal\"] = comp_struct.apply(lambda d: bool(d.get(\"modal_verbs\")))\n",
        "work[\"has_epistemic_marker\"] = comp_struct.apply(lambda d: bool(d.get(\"epistemic_markers\")))\n",
        "\n",
        "# Declared comparator (normalized)\n",
        "decl = work.get(\"Comparator_Type\", pd.Series(index=work.index)).astype(str).str.strip().str.lower()\n",
        "\n",
        "def starts(d, pfx): return d.fillna(\"\").str.startswith(pfx)\n",
        "def contains(d, sub): return d.fillna(\"\").str.contains(sub, regex=False)\n",
        "\n",
        "work[\"declared_comp_like\"]       = (decl == \"like\")\n",
        "work[\"declared_comp_as\"]         = (decl == \"as\")\n",
        "work[\"declared_comp_as_if\"]      = ((decl == \"as if\") | (decl == \"as-if\"))\n",
        "work[\"declared_comp_as_though\"]  = ((decl == \"as though\") | (decl == \"as-though\"))\n",
        "work[\"declared_comp_resemble\"]   = starts(decl, \"resembl\")\n",
        "work[\"declared_comp_seem\"]       = starts(decl, \"seem\")\n",
        "work[\"declared_comp_appear\"]     = starts(decl, \"appear\")\n",
        "work[\"declared_comp_colon\"]      = ((decl == \"colon\") | (decl == \":\"))\n",
        "work[\"declared_comp_semicolon\"]  = ((decl == \"semicolon\") | (decl == \";\"))\n",
        "work[\"declared_comp_dash\"]       = ((decl == \"en dash\") | (decl == \"–\") | (decl == \"—\") | (decl == \"-\"))\n",
        "work[\"declared_comp_ellipsis\"]   = ((decl == \"ellipsis\") | (decl == \"...\") | (decl == \"…\"))\n",
        "\n",
        "# Anchor detections from text (heuristic + declared comparator)\n",
        "sent_series = work.get(\"Sentence_Context\", pd.Series(index=work.index)).apply(_sent).str.lower().fillna(\"\")\n",
        "\n",
        "# as…as heuristic: at least two 'as' tokens separated within a small window OR explicit 'as ... as'\n",
        "def detect_as_as(s):\n",
        "    if not s: return False\n",
        "    if \"as ... as\" in s: return True\n",
        "    toks = [t for t in re.split(r\"\\W+\", s) if t]\n",
        "    positions = [i for i,t in enumerate(toks) if t == \"as\"]\n",
        "    for i in range(len(positions)):\n",
        "        for j in range(i+1, len(positions)):\n",
        "            if 1 <= (positions[j] - positions[i]) <= 8:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "import re\n",
        "work[\"anchor_like\"]          = work[\"declared_comp_like\"] | sent_series.str.contains(r\"\\blike\\b\", regex=True)\n",
        "work[\"anchor_as_if\"]         = work[\"declared_comp_as_if\"] | sent_series.str.contains(r\"\\bas if\\b\", regex=True)\n",
        "work[\"anchor_as_though\"]     = work[\"declared_comp_as_though\"] | sent_series.str.contains(r\"\\bas though\\b\", regex=True)\n",
        "work[\"anchor_as_as\"]         = work[\"declared_comp_as\"] | sent_series.apply(detect_as_as)\n",
        "work[\"anchor_resemble_family\"]= work[\"declared_comp_resemble\"] | sent_series.str.contains(r\"\\bresembl\\w*\\b\", regex=True)\n",
        "work[\"anchor_seem_family\"]    = work[\"declared_comp_seem\"] | sent_series.str.contains(r\"\\bseem\\w*\\b\", regex=True)\n",
        "work[\"anchor_appear_family\"]  = work[\"declared_comp_appear\"] | sent_series.str.contains(r\"\\bappear\\w*\\b\", regex=True)\n",
        "work[\"anchor_colon\"]          = work[\"declared_comp_colon\"] | sent_series.str.contains(r\":\", regex=False)\n",
        "work[\"anchor_semicolon\"]      = work[\"declared_comp_semicolon\"] | sent_series.str.contains(r\";\", regex=False)\n",
        "work[\"anchor_dash\"]           = work[\"declared_comp_dash\"] | sent_series.str.contains(r\"[–—-]\", regex=True)\n",
        "work[\"anchor_ellipsis\"]       = work[\"declared_comp_ellipsis\"] | sent_series.str.contains(r\"\\.\\.\\.|…\", regex=True)\n",
        "\n",
        "# Between diagnostics from integrated metrics\n",
        "work[\"between_has_verb\"]        = work.get(\"Between_Has_Verb\", pd.Series(index=work.index)).fillna(0).astype(bool)\n",
        "work[\"between_has_subordinator\"]= work.get(\"Between_Has_Subordinator\", pd.Series(index=work.index)).fillna(0).astype(bool)\n",
        "\n",
        "# Compact \"signature\" string per row: which anchors are present (sorted, joined)\n",
        "anchor_cols = [\n",
        "    \"anchor_like\",\"anchor_as_as\",\"anchor_as_if\",\"anchor_as_though\",\n",
        "    \"anchor_resemble_family\",\"anchor_seem_family\",\"anchor_appear_family\",\n",
        "    \"anchor_colon\",\"anchor_semicolon\",\"anchor_dash\",\"anchor_ellipsis\"\n",
        "]\n",
        "def signature(row):\n",
        "    on = [c.replace(\"anchor_\",\"\") for c in anchor_cols if bool(row.get(c, False))]\n",
        "    return \",\".join(sorted(on)) if on else \"none\"\n",
        "work[\"anchor_signature\"] = work[anchor_cols].apply(signature, axis=1)\n",
        "\n",
        "# ---- Pattern flag tests (each flag proportion vs BNC) ----\n",
        "flag_cols = [\n",
        "    # anchors\n",
        "    \"anchor_like\",\"anchor_as_as\",\"anchor_as_if\",\"anchor_as_though\",\n",
        "    \"anchor_resemble_family\",\"anchor_seem_family\",\"anchor_appear_family\",\n",
        "    \"anchor_colon\",\"anchor_semicolon\",\"anchor_dash\",\"anchor_ellipsis\",\n",
        "    # between diagnostics\n",
        "    \"between_has_verb\",\"between_has_subordinator\",\n",
        "    # declared comparator families (sanity)\n",
        "    \"declared_comp_like\",\"declared_comp_as\",\"declared_comp_as_if\",\"declared_comp_as_though\",\n",
        "    \"declared_comp_resemble\",\"declared_comp_seem\",\"declared_comp_appear\",\n",
        "    \"declared_comp_colon\",\"declared_comp_semicolon\",\"declared_comp_dash\",\"declared_comp_ellipsis\",\n",
        "]\n",
        "\n",
        "flag_rows = []\n",
        "for flag in [f for f in flag_cols if f in work.columns]:\n",
        "    bnc = work[work[\"__Group__\"]==\"BNC\"][flag].astype(bool)\n",
        "    nB = bnc.shape[0]; cB = int(bnc.sum())\n",
        "    for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "        sub = work[work[\"__Group__\"]==subset][flag].astype(bool)\n",
        "        nA = sub.shape[0]; cA = int(sub.sum())\n",
        "        pA = cA/nA if nA>0 else 0.0\n",
        "        pB = cB/nB if nB>0 else 0.0\n",
        "        row = {\"Flag\":flag, \"Subset\":subset, \"count_A\":cA, \"n_A\":nA, \"prop_A\":pA, \"count_B\":cB, \"n_B\":nB, \"prop_B\":pB}\n",
        "        if (nA>0 and nB>0) and not ((cA==0 and cB==0) or (cA==nA and cB==nB)):\n",
        "            if _HAS_STATSMODELS:\n",
        "                try:\n",
        "                    z, pz = proportions_ztest(np.array([cA,cB]), np.array([nA,nB]))\n",
        "                except Exception:\n",
        "                    pA_ha = (cA + 0.5) / (nA + 1)\n",
        "                    pB_ha = (cB + 0.5) / (nB + 1)\n",
        "                    se = np.sqrt(pA_ha*(1-pA_ha)/(nA+1) + pB_ha*(1-pB_ha)/(nB+1))\n",
        "                    z = (pA_ha - pB_ha) / se if se>0 else np.nan\n",
        "                    pz = 2*norm.sf(abs(z)) if np.isfinite(z) else np.nan\n",
        "                row.update({\"z\": float(z), \"p_value\": float(pz)})\n",
        "            else:\n",
        "                row.update({\"z\": np.nan, \"p_value\": np.nan})\n",
        "        else:\n",
        "            row.update({\"z\": np.nan, \"p_value\": 1.0})\n",
        "        flag_rows.append(row)\n",
        "\n",
        "flag_df = pd.DataFrame(flag_rows)\n",
        "if \"p_value\" in flag_df.columns:\n",
        "    flag_df[\"p_adj_BH\"] = p_adjust_bh(flag_df[\"p_value\"].fillna(1.0).to_numpy())\n",
        "\n",
        "path_flag = os.path.join(out_dir, f\"pattern_flag_tests_by_subset_{ts}.csv\")\n",
        "flag_df.to_csv(path_flag, index=False)\n",
        "\n",
        "# ---- Top anchor signatures per group ----\n",
        "sig_rows = []\n",
        "for g in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\",\"BNC\"]:\n",
        "    w = work[work[\"__Group__\"]==g]\n",
        "    vc = w[\"anchor_signature\"].value_counts()\n",
        "    total = int(vc.sum())\n",
        "    for s, c in vc.items():\n",
        "        sig_rows.append({\"Group\": g, \"Anchor_Signature\": s, \"Count\": int(c), \"Prop\": (c/total if total>0 else 0.0)})\n",
        "top_sigs_df = pd.DataFrame(sig_rows).sort_values([\"Group\",\"Count\"], ascending=[True,False])\n",
        "\n",
        "path_sigs = os.path.join(out_dir, f\"top_anchor_signatures_by_group_{ts}.csv\")\n",
        "top_sigs_df.to_csv(path_sigs, index=False)\n",
        "\n",
        "# ---- Console summary for pattern flags (BH<0.05) ----\n",
        "sig = flag_df.copy()\n",
        "if \"p_adj_BH\" in sig.columns:\n",
        "    sig = sig[sig[\"p_adj_BH\"] < 0.05].copy()\n",
        "    if not sig.empty:\n",
        "        sig[\"abs_diff\"] = (sig[\"prop_A\"] - sig[\"prop_B\"]).abs()\n",
        "        sig = sig.sort_values([\"p_adj_BH\",\"abs_diff\"], ascending=[True, False])\n",
        "        print(\"\\nSignificant pattern flags (BH<0.05):\")\n",
        "        for _, r in sig.iterrows():\n",
        "            d = (r[\"prop_A\"] - r[\"prop_B\"])\n",
        "            print(f\"  {r['Subset']:22s} | {r['Flag']:28s} Δ={d:+.3f} \"\n",
        "                  f\"(A={r['prop_A']:.3f}, B={r['prop_B']:.3f}) p_adj={r['p_adj_BH']:.3g}\")\n",
        "    else:\n",
        "        print(\"\\nSignificant pattern flags (BH<0.05): none\")\n",
        "\n",
        "# ---- Quick console view of top anchor signatures per group ----\n",
        "print(\"\\nTop anchor signatures per group (n≤10 each):\")\n",
        "for g in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\",\"BNC\"]:\n",
        "    sub = top_sigs_df[top_sigs_df[\"Group\"]==g].head(10)\n",
        "    if not sub.empty:\n",
        "        print(f\"\\n{g}\")\n",
        "        for _, row in sub.iterrows():\n",
        "            print(f\"  {row['Anchor_Signature']:<40s} count={row['Count']:>3d}  prop={row['Prop']:.3f}\")\n",
        "\n",
        "# ---- Anchor-signature divergence across groups (Jensen–Shannon) ----\n",
        "all_sigs = sorted(work[\"anchor_signature\"].dropna().unique())\n",
        "def dist_for(g):\n",
        "    vc = work[work[\"__Group__\"]==g][\"anchor_signature\"].value_counts()\n",
        "    p = np.array([vc.get(s,0) for s in all_sigs], dtype=float)\n",
        "    return p/p.sum() if p.sum()>0 else np.ones(len(all_sigs))/len(all_sigs)\n",
        "\n",
        "G = [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\",\"BNC\"]\n",
        "js_rows = []\n",
        "for i in range(len(G)):\n",
        "    for j in range(i+1,len(G)):\n",
        "        # squared Jensen–Shannon divergence for [0,1] interpretability\n",
        "        js = float(jensenshannon(dist_for(G[i]), dist_for(G[j]))**2)\n",
        "        js_rows.append({\"Group_A\":G[i], \"Group_B\":G[j], \"JSD\":js})\n",
        "js_df = pd.DataFrame(js_rows).sort_values(\"JSD\", ascending=False)\n",
        "js_path = os.path.join(out_dir, f\"anchor_signature_js_divergence_{ts}.csv\")\n",
        "js_df.to_csv(js_path, index=False)\n",
        "print(\"\\nAnchor-signature divergence (Jensen–Shannon, higher = more different):\")\n",
        "print(js_df.to_string(index=False))\n",
        "\n",
        "# ---------- 5) Topic modelling (per subset + BNC) ----------\n",
        "print(\"\\nTOPIC MODELLING (per subset + BNC)\")\n",
        "\n",
        "def prepare_corpus(gdf):\n",
        "    \"\"\"Prefer Lemmatized_Text from Cell 1; fallback to Sentence_Context.\"\"\"\n",
        "    if \"Lemmatized_Text\" in gdf.columns and gdf[\"Lemmatized_Text\"].notna().any():\n",
        "        texts = gdf[\"Lemmatized_Text\"].fillna(\"\").astype(str).tolist()\n",
        "    else:\n",
        "        texts = gdf[\"Sentence_Context\"].fillna(\"\").astype(str).tolist() if \"Sentence_Context\" in gdf.columns else []\n",
        "    return [t for t in texts if t.strip()]\n",
        "\n",
        "def lda_topics(corpus, n_topics=5, n_top_words=10, max_df=0.85, min_df=2, max_features=5000, random_state=RANDOM_STATE):\n",
        "    if not corpus:\n",
        "        return None, None, None\n",
        "    vectorizer = CountVectorizer(\n",
        "        max_df=max_df, min_df=min_df, max_features=max_features,\n",
        "        stop_words=\"english\",\n",
        "        token_pattern=r\"(?u)\\b[^\\W\\d_][^\\W\\d_]+\\b\"\n",
        "    )\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    lda = LatentDirichletAllocation(\n",
        "        n_components=n_topics, random_state=random_state, learning_method=\"batch\"\n",
        "    )\n",
        "    lda.fit(X)\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for comp in lda.components_:\n",
        "        top_idx = comp.argsort()[:-n_top_words-1:-1]\n",
        "        topics.append([terms[i] for i in top_idx])\n",
        "    doc_topic = lda.transform(X)  # rows sum ~1\n",
        "    return topics, doc_topic, terms\n",
        "\n",
        "topics_summary = {\"params\":{\"n_topics\":5,\"n_top_words\":10,\"vectorizer\":\"CountVectorizer\"}, \"groups\":{}}\n",
        "topic_rows = []\n",
        "topicmix_rows = []\n",
        "\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\",\"BNC\"]:\n",
        "    corpus = prepare_corpus(groups[subset])\n",
        "    if corpus:\n",
        "        tpcs, doc_topic, terms = lda_topics(corpus, n_topics=5, n_top_words=10)\n",
        "        topics_summary[\"groups\"][subset] = tpcs if tpcs is not None else []\n",
        "        if tpcs:\n",
        "            for i, words in enumerate(tpcs, 1):\n",
        "                topic_rows.append({\"Group\":subset, \"Topic\":i, \"Top_Words\":\", \".join(words)})\n",
        "            topic_means = doc_topic.mean(axis=0) if doc_topic is not None else None\n",
        "            if topic_means is not None:\n",
        "                for i, val in enumerate(topic_means, 1):\n",
        "                    topicmix_rows.append({\"Group\":subset, \"Topic\":i, \"Mean_Weight\":float(val)})\n",
        "        print(f\"  Topics generated for {subset}: {len(tpcs) if tpcs else 0}\")\n",
        "    else:\n",
        "        topics_summary[\"groups\"][subset] = []\n",
        "        print(f\"  Not enough text for {subset}\")\n",
        "\n",
        "topics_json_path = os.path.join(out_dir, f\"lda_topics_by_subset_{ts}.json\")\n",
        "with open(topics_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(topics_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "topics_csv = pd.DataFrame(topic_rows, columns=[\"Group\",\"Topic\",\"Top_Words\"])\n",
        "topics_csv_path = os.path.join(out_dir, f\"lda_topics_by_subset_{ts}.csv\")\n",
        "topics_csv.to_csv(topics_csv_path, index=False)\n",
        "\n",
        "topicmix_csv = pd.DataFrame(topicmix_rows, columns=[\"Group\",\"Topic\",\"Mean_Weight\"])\n",
        "topicmix_csv_path = os.path.join(out_dir, f\"lda_topic_mix_by_subset_{ts}.csv\")\n",
        "topicmix_csv.to_csv(topicmix_csv_path, index=False)\n",
        "\n",
        "# ---------- 6) Optional robustness: 3-way χ² ----------\n",
        "try:\n",
        "    cont_3 = contingency_4way.drop(columns=[\"Joyce_LessRestrictive\"])\n",
        "    chi2_3, p_3, dof_3, _ = chi2_contingency(cont_3, correction=False)\n",
        "    V_3 = cramers_v(chi2_3, cont_3.values.sum(), *cont_3.shape)\n",
        "    print(f\"\\n3-way χ² (Manual/Restrictive/BNC): χ²={chi2_3:.2f} df={dof_3} p={p_3:.3g} V={V_3:.3f}\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------- 7) Master summary JSON ----------\n",
        "master = {\n",
        "    \"generated_at\": ts,\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "    \"note\": \"By-subset outputs (Manual / Restrictive / Less-Restrictive vs BNC) with unified Quasi_Similes label and FDR corrections.\",\n",
        "    \"chi_square_4way\": {\n",
        "        \"chi2\": float(chi2_4), \"dof\": int(dof_4), \"p_value\": float(p_4),\n",
        "        \"cramers_v\": float(V_4), \"monte_carlo\": bool(sim_used),\n",
        "        \"N\": int(N_total)\n",
        "    },\n",
        "    \"files\": {\n",
        "        \"chi2_contingency_by_subset_csv\": path_cont_4,\n",
        "        \"chi2_expected_by_subset_csv\": path_exp_4,\n",
        "        \"chi2_pearson_z_by_subset_csv\": path_resid_z,\n",
        "        \"chi2_cell_p_by_subset_csv\": path_resid_p,\n",
        "        \"chi2_cell_padj_BH_by_subset_csv\": path_resid_padj,\n",
        "        \"two_prop_newcombe_by_subset_csv\": path_two_prop,\n",
        "        \"binomial_tests_by_subset_csv\": path_binom,\n",
        "        \"continuous_tests_by_subset_csv\": path_cont,\n",
        "        \"continuous_segment_tests_by_subset_csv\": path_seg_cont,\n",
        "        \"between_presence_by_group_csv\": path_presence_csv,\n",
        "        \"between_presence_expected_by_group_csv\": path_presence_exp,\n",
        "        \"between_presence_residuals_by_group_csv\": path_presence_resid,\n",
        "        \"pattern_flag_tests_by_subset_csv\": path_flag,\n",
        "        \"top_anchor_signatures_by_group_csv\": path_sigs,\n",
        "        \"anchor_signature_js_divergence_csv\": js_path,\n",
        "        \"lda_topics_by_subset_json\": topics_json_path,\n",
        "        \"lda_topics_by_subset_csv\": topics_csv_path,\n",
        "        \"lda_topic_mix_by_subset_csv\": topicmix_csv_path\n",
        "    }\n",
        "}\n",
        "master_path = os.path.join(out_dir, f\"stats_and_topics_summary_by_subset_{ts}.json\")\n",
        "with open(master_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(master, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nSAVED OUTPUTS (by-subset)\")\n",
        "print(\" - 4-way contingency:\", master[\"files\"][\"chi2_contingency_by_subset_csv\"])\n",
        "print(\" - 4-way expected:\", master[\"files\"][\"chi2_expected_by_subset_csv\"])\n",
        "print(\" - 4-way Pearson z:\", master[\"files\"][\"chi2_pearson_z_by_subset_csv\"])\n",
        "print(\" - 4-way cell p-values:\", master[\"files\"][\"chi2_cell_p_by_subset_csv\"])\n",
        "print(\" - 4-way cell p-values (BH):\", master[\"files\"][\"chi2_cell_padj_BH_by_subset_csv\"])\n",
        "print(\" - Two-proportion (subset vs BNC):\", master[\"files\"][\"two_prop_newcombe_by_subset_csv\"])\n",
        "print(\" - Binomial (subset vs BNC):\", master[\"files\"][\"binomial_tests_by_subset_csv\"])\n",
        "print(\" - Continuous tests (subset vs BNC):\", master[\"files\"][\"continuous_tests_by_subset_csv\"])\n",
        "print(\" - Segment tests (subset vs BNC):\", master[\"files\"][\"continuous_segment_tests_by_subset_csv\"])\n",
        "print(\" - Between presence by group:\", master[\"files\"][\"between_presence_by_group_csv\"])\n",
        "print(\" - Pattern flags (subset vs BNC):\", master[\"files\"][\"pattern_flag_tests_by_subset_csv\"])\n",
        "print(\" - Top anchor signatures:\", master[\"files\"][\"top_anchor_signatures_by_group_csv\"])\n",
        "print(\" - Anchor-signature divergence:\", master[\"files\"][\"anchor_signature_js_divergence_csv\"])\n",
        "print(\" - Topics JSON (per subset):\", master[\"files\"][\"lda_topics_by_subset_json\"])\n",
        "print(\" - Topics CSV (per subset):\", master[\"files\"][\"lda_topics_by_subset_csv\"])\n",
        "print(\" - Topic mix CSV (per subset):\", master[\"files\"][\"lda_topic_mix_by_subset_csv\"])\n",
        "print(\" - Master summary JSON:\", master_path)\n",
        "print(\"\\nDONE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hXOTUhplPqs",
        "outputId": "23964368-0d8c-4a54-b1d6-943767793610"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROBUST STATISTICAL ANALYSIS (Joyce subsets vs BNC)\n",
            "===========================================================================\n",
            "\n",
            "Category distribution after harmonisation (counts):\n",
            "Original_Dataset     BNC_Baseline  Manual_CloseReading  \\\n",
            "Category_Framework                                       \n",
            "Standard                      118                   93   \n",
            "Quasi_Similes                  82                   53   \n",
            "Joycean_Quasi_Fuzzy             0                   13   \n",
            "Joycean_Framed                  0                   18   \n",
            "Joycean_Silent                  0                    6   \n",
            "Uncategorized                   0                    1   \n",
            "\n",
            "Original_Dataset     NLP_LessRestrictive_PG  Restrictive_Dubliners  Total  \n",
            "Category_Framework                                                         \n",
            "Standard                                330                    150    691  \n",
            "Quasi_Similes                             0                     47    182  \n",
            "Joycean_Quasi_Fuzzy                       0                     14     27  \n",
            "Joycean_Framed                            0                      4     22  \n",
            "Joycean_Silent                            0                      3      9  \n",
            "Uncategorized                             0                      0      1  \n",
            "Joyce_Manual          : 184 rows\n",
            "Joyce_Restrictive     : 218 rows\n",
            "Joyce_LessRestrictive : 330 rows\n",
            "BNC                   : 200 rows\n",
            "\n",
            "4-way Chi-square on Category_Framework (Joyce subsets vs BNC):\n",
            "χ² = 281.8806 | df = 15 | p = 0.000000 | Cramér’s V = 0.318 | Monte-Carlo=False\n",
            "\n",
            "Top 10 standardized residuals (|z|):\n",
            "  Joyce_LessRestrictive  | Quasi_Similes         z=  8.03  obs=0 exp=64.4\n",
            "  BNC                    | Quasi_Similes         z=  6.87  obs=82 exp=39.1\n",
            "  Joyce_Manual           | Joycean_Framed        z=  6.55  obs=18 exp=4.3\n",
            "  Joyce_LessRestrictive  | Standard              z=  5.46  obs=330 exp=244.7\n",
            "  Joyce_Manual           | Standard              z=  3.72  obs=93 exp=136.4\n",
            "  Joyce_Manual           | Joycean_Quasi_Fuzzy   z=  3.32  obs=13 exp=5.3\n",
            "  Joyce_Manual           | Joycean_Silent        z=  3.17  obs=6 exp=1.8\n",
            "  Joyce_LessRestrictive  | Joycean_Quasi_Fuzzy   z=  3.09  obs=0 exp=9.6\n",
            "  Joyce_Restrictive      | Joycean_Quasi_Fuzzy   z=  3.06  obs=14 exp=6.3\n",
            "  Joyce_Manual           | Quasi_Similes         z=  2.85  obs=53 exp=35.9\n",
            "\n",
            "Two-proportion tests (Newcombe–Wilson) for each Joyce subset vs BNC:\n",
            "  Joyce_Manual           | Joycean_Framed       z= 4.531 p=5.87825e-06 CI[0.058,0.149] h=0.636\n",
            "  Joyce_Manual           | Joycean_Quasi_Fuzzy  z= 3.824 p=0.000131123 CI[0.036,0.117] h=0.538\n",
            "  Joyce_Manual           | Joycean_Silent       z= 2.574 p=0.0100543 CI[0.007,0.069] h=0.363\n",
            "  Joyce_Manual           | Quasi_Similes        z=-2.501 p=0.0124016 CI[-0.214,-0.026] h=-0.257\n",
            "  Joyce_Manual           | Standard             z=-1.664 p=0.0961402 CI[-0.182,0.015] h=-0.170\n",
            "  Joyce_Manual           | Uncategorized        z= 1.044 p=0.296517 CI[-0.014,0.030] h=0.148\n",
            "  Joyce_Restrictive      | Joycean_Framed       z= 1.925 p=0.0542438 CI[-0.004,0.046] h=0.272\n",
            "  Joyce_Restrictive      | Joycean_Quasi_Fuzzy  z= 3.645 p=0.00026695 CI[0.032,0.105] h=0.512\n",
            "  Joyce_Restrictive      | Joycean_Silent       z= 1.665 p=0.0959149 CI[-0.007,0.040] h=0.235\n",
            "  Joyce_Restrictive      | Quasi_Similes        z=-4.298 p=1.72149e-05 CI[-0.279,-0.106] h=-0.424\n",
            "  Joyce_Restrictive      | Standard             z= 2.088 p=0.0367809 CI[0.006,0.188] h=0.205\n",
            "  Joyce_Restrictive      | Uncategorized        (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Joycean_Framed       (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Joycean_Quasi_Fuzzy  (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Joycean_Silent       (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Quasi_Similes        z=-12.652 p=1.09523e-36 CI[-0.479,-0.343] h=-1.390\n",
            "  Joyce_LessRestrictive  | Standard             z=12.652 p=1.09523e-36 CI[0.343,0.479] h=1.390\n",
            "  Joyce_LessRestrictive  | Uncategorized        (both groups at boundary → skip) h=0.000\n",
            "\n",
            "Binomial tests (each Joyce subset vs BNC category proportion):\n",
            "  Joyce_Manual           | Quasi_Similes        53/184 vs p_ref=0.4100 p=0.00070983\n",
            "  Joyce_Manual           | Standard             93/184 vs p_ref=0.5900 p=0.0242733\n",
            "  Joyce_Restrictive      | Quasi_Similes        47/218 vs p_ref=0.4100 p=1.9121e-09\n",
            "  Joyce_Restrictive      | Standard             150/218 vs p_ref=0.5900 p=0.00303334\n",
            "  Joyce_LessRestrictive  | Quasi_Similes        0/330 vs p_ref=0.4100 p=3.46102e-76\n",
            "  Joyce_LessRestrictive  | Standard             330/330 vs p_ref=0.5900 p=3.46102e-76\n",
            "\n",
            "Continuous features (Welch t + Mann–Whitney U) each Joyce subset vs BNC:\n",
            "  Sentence_Length        | Joyce_Manual           t=  2.804 p=0.00541318 | U=  20123.5 p=0.112632 | g=0.293 δ=0.094\n",
            "  Sentence_Length        | Joyce_Restrictive      t=  1.806 p=0.0716264 | U=  23076.0 p=0.301001 | g=0.175 δ=0.059\n",
            "  Sentence_Length        | Joyce_LessRestrictive  t=  1.731 p=0.0841489 | U=  34228.5 p=0.472192 | g=0.145 δ=0.037\n",
            "  Pre_Post_Ratio         | Joyce_Manual           t=  0.168 p=0.86694 | U=  18455.0 p=0.155892 | g=0.017 δ=0.086\n",
            "  Pre_Post_Ratio         | Joyce_Restrictive      t= -1.431 p=0.153317 | U=  22077.5 p=0.696964 | g=-0.142 δ=0.022\n",
            "  Pre_Post_Ratio         | Joyce_LessRestrictive  t= -0.882 p=0.378168 | U=  32478.0 p=0.868394 | g=-0.083 δ=0.009\n",
            "  Sentiment_Polarity     | Joyce_Manual           t= -0.825 p=0.40996 | U=  17385.5 p=0.343227 | g=-0.084 δ=-0.055\n",
            "  Sentiment_Polarity     | Joyce_Restrictive      t= -0.634 p=0.526346 | U=  20806.5 p=0.414288 | g=-0.062 δ=-0.046\n",
            "  Sentiment_Polarity     | Joyce_LessRestrictive  t=  0.510 p=0.610387 | U=  33618.5 p=0.714242 | g=0.046 δ=0.019\n",
            "  Sentiment_Subjectivity | Joyce_Manual           t= -0.308 p=0.757975 | U=  18113.5 p=0.790903 | g=-0.031 δ=-0.016\n",
            "  Sentiment_Subjectivity | Joyce_Restrictive      t= -0.374 p=0.708807 | U=  21360.5 p=0.719938 | g=-0.036 δ=-0.020\n",
            "  Sentiment_Subjectivity | Joyce_LessRestrictive  t= -0.664 p=0.507057 | U=  31990.0 p=0.551874 | g=-0.059 δ=-0.031\n",
            "\n",
            "Comparator-segment statistics (Pre / Between / Post) — integrated (no external CSV)\n",
            "  Pre_Tokens           | Joyce_Manual           t=  1.498 p=0.135 | U=  21434.5 p=0.00515 | g=0.152 δ=0.165\n",
            "  Pre_Tokens           | Joyce_Restrictive      t= -0.205 p=0.837 | U=  23578.5 p=0.149 | g=-0.020 δ=0.082\n",
            "  Pre_Tokens           | Joyce_LessRestrictive  t=  0.150 p=0.881 | U=  35997.5 p=0.0789 | g=0.014 δ=0.091\n",
            "  Between_Tokens_Total | Joyce_Manual           t=  2.878 p=0.00438 | U=  20211.5 p=0.0436 | g=0.303 δ=0.098\n",
            "  Between_Tokens_Total | Joyce_Restrictive      t=  2.848 p=0.00464 | U=  23865.0 p=0.0429 | g=0.273 δ=0.095\n",
            "  Between_Tokens_Total | Joyce_LessRestrictive  t=  1.567 p=0.118 | U=  33470.5 p=0.729 | g=0.127 δ=0.014\n",
            "  Post_Tokens          | Joyce_Manual           t=  0.842 p=0.401 | U=  18016.5 p=0.724 | g=0.087 δ=-0.021\n",
            "  Post_Tokens          | Joyce_Restrictive      t=  1.210 p=0.227 | U=  22695.5 p=0.467 | g=0.117 δ=0.041\n",
            "  Post_Tokens          | Joyce_LessRestrictive  t=  1.993 p=0.0468 | U=  34641.5 p=0.336 | g=0.164 δ=0.050\n",
            "  Total_Tokens         | Joyce_Manual           t=  2.804 p=0.00541 | U=  20123.5 p=0.113 | g=0.293 δ=0.094\n",
            "  Total_Tokens         | Joyce_Restrictive      t=  1.806 p=0.0716 | U=  23076.0 p=0.301 | g=0.175 δ=0.059\n",
            "  Total_Tokens         | Joyce_LessRestrictive  t=  1.731 p=0.0841 | U=  34228.5 p=0.472 | g=0.145 δ=0.037\n",
            "  Pre_Share            | Joyce_Manual           t=  1.087 p=0.278 | U=  19571.5 p=0.281 | g=0.110 δ=0.064\n",
            "  Pre_Share            | Joyce_Restrictive      t= -0.349 p=0.727 | U=  21598.5 p=0.871 | g=-0.034 δ=-0.009\n",
            "  Pre_Share            | Joyce_LessRestrictive  t=  0.485 p=0.628 | U=  33952.5 p=0.577 | g=0.044 δ=0.029\n",
            "  Between_Share        | Joyce_Manual           t=  1.917 p=0.056 | U=  20016.0 p=0.0719 | g=0.197 δ=0.088\n",
            "  Between_Share        | Joyce_Restrictive      t=  1.696 p=0.0906 | U=  23569.5 p=0.0828 | g=0.165 δ=0.081\n",
            "  Between_Share        | Joyce_LessRestrictive  t=  0.017 p=0.987 | U=  33147.5 p=0.914 | g=0.002 δ=0.004\n",
            "  Post_Share           | Joyce_Manual           t= -2.810 p=0.00522 | U=  15664.0 p=0.0118 | g=-0.285 δ=-0.149\n",
            "  Post_Share           | Joyce_Restrictive      t= -1.092 p=0.275 | U=  20836.0 p=0.435 | g=-0.107 δ=-0.044\n",
            "  Post_Share           | Joyce_LessRestrictive  t= -0.498 p=0.619 | U=  32573.0 p=0.803 | g=-0.046 δ=-0.013\n",
            "  Comp_Count           | Joyce_Manual           t=  0.469 p=0.639 | U=  19021.0 p=0.504 | g=0.048 δ=0.034\n",
            "  Comp_Count           | Joyce_Restrictive      t= -0.019 p=0.985 | U=  22530.0 p=0.486 | g=-0.002 δ=0.033\n",
            "  Comp_Count           | Joyce_LessRestrictive  t= -1.793 p=0.0738 | U=  31041.0 p=0.171 | g=-0.167 δ=-0.059\n",
            "  Between_Segments     | Joyce_Manual           t=  1.122 p=0.263 | U=  19791.0 p=0.125 | g=0.114 δ=0.076\n",
            "  Between_Segments     | Joyce_Restrictive      t=  0.464 p=0.643 | U=  23160.0 p=0.186 | g=0.046 δ=0.062\n",
            "  Between_Segments     | Joyce_LessRestrictive  t= -0.971 p=0.332 | U=  32581.0 p=0.76 | g=-0.092 δ=-0.013\n",
            "  Between_Max          | Joyce_Manual           t=  2.795 p=0.00555 | U=  20227.5 p=0.0418 | g=0.292 δ=0.099\n",
            "  Between_Max          | Joyce_Restrictive      t=  2.715 p=0.00692 | U=  23898.5 p=0.0396 | g=0.261 δ=0.096\n",
            "  Between_Max          | Joyce_LessRestrictive  t=  1.648 p=0.1 | U=  33555.0 p=0.683 | g=0.134 δ=0.017\n",
            "  Between_Mean         | Joyce_Manual           t=  2.746 p=0.00637 | U=  20293.5 p=0.0349 | g=0.284 δ=0.103\n",
            "  Between_Mean         | Joyce_Restrictive      t=  2.898 p=0.00398 | U=  24014.5 p=0.0299 | g=0.278 δ=0.102\n",
            "  Between_Mean         | Joyce_LessRestrictive  t=  1.789 p=0.0742 | U=  33681.0 p=0.616 | g=0.147 δ=0.021\n",
            "  Between_Has_Verb     | Joyce_Manual           t=  2.653 p=0.00832 | U=  20464.0 p=0.008 | g=0.273 δ=0.112\n",
            "  Between_Has_Verb     | Joyce_Restrictive      t=  2.425 p=0.0157 | U=  23903.0 p=0.0167 | g=0.235 δ=0.096\n",
            "  Between_Has_Verb     | Joyce_LessRestrictive  t=  0.760 p=0.448 | U=  33855.0 p=0.454 | g=0.067 δ=0.026\n",
            "  Between_Has_Subordinator | Joyce_Manual           t=  2.151 p=0.0322 | U=  19604.0 p=0.0302 | g=0.222 δ=0.065\n",
            "  Between_Has_Subordinator | Joyce_Restrictive      t=  1.787 p=0.0748 | U=  22883.0 p=0.0781 | g=0.173 δ=0.050\n",
            "  Between_Has_Subordinator | Joyce_LessRestrictive  t=  1.218 p=0.224 | U=  33955.0 p=0.243 | g=0.105 δ=0.029\n",
            "\n",
            "Between-clause presence × Group (χ²):\n",
            "                       Between>0  Between=0\n",
            "__Group__                                  \n",
            "Joyce_Manual                  66        118\n",
            "Joyce_Restrictive             77        141\n",
            "Joyce_LessRestrictive         94        236\n",
            "BNC                           56        144\n",
            "  χ² = 5.633 | df = 3 | p = 0.130871 | Cramér’s V = 0.078\n",
            "\n",
            "Pattern/shaping statistics (anchors, patterns, between diagnostics, declared comparator)\n",
            "\n",
            "Significant pattern flags (BH<0.05):\n",
            "  Joyce_Restrictive      | anchor_as_as                 Δ=-0.175 (A=0.055, B=0.230) p_adj=1.7e-05\n",
            "  Joyce_Restrictive      | anchor_colon                 Δ=+0.121 (A=0.161, B=0.040) p_adj=0.00182\n",
            "  Joyce_LessRestrictive  | declared_comp_seem           Δ=+0.127 (A=0.242, B=0.115) p_adj=0.0047\n",
            "  Joyce_Manual           | anchor_colon                 Δ=+0.107 (A=0.147, B=0.040) p_adj=0.0047\n",
            "  Joyce_Restrictive      | anchor_semicolon             Δ=+0.086 (A=0.101, B=0.015) p_adj=0.0047\n",
            "  Joyce_LessRestrictive  | anchor_as_if                 Δ=-0.111 (A=0.109, B=0.220) p_adj=0.00651\n",
            "  Joyce_LessRestrictive  | anchor_colon                 Δ=+0.090 (A=0.130, B=0.040) p_adj=0.00651\n",
            "  Joyce_Manual           | anchor_semicolon             Δ=+0.067 (A=0.082, B=0.015) p_adj=0.0174\n",
            "  Joyce_Restrictive      | anchor_ellipsis              Δ=+0.046 (A=0.046, B=0.000) p_adj=0.0174\n",
            "  Joyce_Manual           | anchor_as_as                 Δ=-0.116 (A=0.114, B=0.230) p_adj=0.0175\n",
            "  Joyce_LessRestrictive  | anchor_as_as                 Δ=-0.100 (A=0.130, B=0.230) p_adj=0.0175\n",
            "  Joyce_Manual           | anchor_ellipsis              Δ=+0.043 (A=0.043, B=0.000) p_adj=0.0175\n",
            "  Joyce_LessRestrictive  | anchor_like                  Δ=-0.131 (A=0.394, B=0.525) p_adj=0.0179\n",
            "  Joyce_LessRestrictive  | anchor_seem_family           Δ=+0.107 (A=0.252, B=0.145) p_adj=0.0185\n",
            "  Joyce_LessRestrictive  | anchor_semicolon             Δ=+0.052 (A=0.067, B=0.015) p_adj=0.0314\n",
            "  Joyce_Manual           | between_has_verb             Δ=+0.112 (A=0.277, B=0.165) p_adj=0.0355\n",
            "  Joyce_Restrictive      | anchor_seem_family           Δ=+0.103 (A=0.248, B=0.145) p_adj=0.0362\n",
            "  Joyce_LessRestrictive  | declared_comp_appear         Δ=+0.033 (A=0.033, B=0.000) p_adj=0.0363\n",
            "  Joyce_LessRestrictive  | anchor_ellipsis              Δ=+0.030 (A=0.030, B=0.000) p_adj=0.049\n",
            "\n",
            "Top anchor signatures per group (n≤10 each):\n",
            "\n",
            "Joyce_Manual\n",
            "  like                                     count= 63  prop=0.342\n",
            "  as_if                                    count= 23  prop=0.125\n",
            "  seem_family                              count=  9  prop=0.049\n",
            "  as_as                                    count=  9  prop=0.049\n",
            "  like,semicolon                           count=  8  prop=0.043\n",
            "  dash,like                                count=  7  prop=0.038\n",
            "  as_if,colon                              count=  7  prop=0.038\n",
            "  none                                     count=  6  prop=0.033\n",
            "  colon,like                               count=  5  prop=0.027\n",
            "  like,seem_family                         count=  4  prop=0.022\n",
            "\n",
            "Joyce_Restrictive\n",
            "  like                                     count= 71  prop=0.326\n",
            "  as_if                                    count= 22  prop=0.101\n",
            "  seem_family                              count= 22  prop=0.101\n",
            "  like,semicolon                           count=  9  prop=0.041\n",
            "  colon,like                               count=  9  prop=0.041\n",
            "  as_as                                    count=  7  prop=0.032\n",
            "  none                                     count=  6  prop=0.028\n",
            "  as_if,colon                              count=  6  prop=0.028\n",
            "  seem_family,semicolon                    count=  6  prop=0.028\n",
            "  ellipsis,like                            count=  5  prop=0.023\n",
            "\n",
            "Joyce_LessRestrictive\n",
            "  like                                     count= 91  prop=0.276\n",
            "  seem_family                              count= 52  prop=0.158\n",
            "  as_as                                    count= 27  prop=0.082\n",
            "  none                                     count= 22  prop=0.067\n",
            "  as_if                                    count= 21  prop=0.064\n",
            "  colon,like                               count=  9  prop=0.027\n",
            "  as_as,dash                               count=  8  prop=0.024\n",
            "  dash                                     count=  8  prop=0.024\n",
            "  like,semicolon                           count=  8  prop=0.024\n",
            "  seem_family,semicolon                    count=  7  prop=0.021\n",
            "\n",
            "BNC\n",
            "  like                                     count= 72  prop=0.360\n",
            "  as_as                                    count= 35  prop=0.175\n",
            "  as_if                                    count= 29  prop=0.145\n",
            "  seem_family                              count= 15  prop=0.075\n",
            "  dash,like                                count=  9  prop=0.045\n",
            "  like,seem_family                         count=  5  prop=0.025\n",
            "  colon,like                               count=  5  prop=0.025\n",
            "  as_if,like                               count=  4  prop=0.020\n",
            "  as_as,dash                               count=  4  prop=0.020\n",
            "  as_if,seem_family                        count=  3  prop=0.015\n",
            "\n",
            "Anchor-signature divergence (Jensen–Shannon, higher = more different):\n",
            "              Group_A               Group_B      JSD\n",
            "Joyce_LessRestrictive                   BNC 0.162174\n",
            "    Joyce_Restrictive                   BNC 0.158379\n",
            "         Joyce_Manual                   BNC 0.142476\n",
            "         Joyce_Manual Joyce_LessRestrictive 0.086049\n",
            "    Joyce_Restrictive Joyce_LessRestrictive 0.064232\n",
            "         Joyce_Manual     Joyce_Restrictive 0.057966\n",
            "\n",
            "TOPIC MODELLING (per subset + BNC)\n",
            "  Topics generated for Joyce_Manual: 5\n",
            "  Topics generated for Joyce_Restrictive: 5\n",
            "  Topics generated for Joyce_LessRestrictive: 5\n",
            "  Topics generated for BNC: 5\n",
            "\n",
            "3-way χ² (Manual/Restrictive/BNC): χ²=69.65 df=10 p=5.18e-11 V=0.241\n",
            "\n",
            "SAVED OUTPUTS (by-subset)\n",
            " - 4-way contingency: analysis_outputs/chi2_contingency_by_subset_20250831_102743.csv\n",
            " - 4-way expected: analysis_outputs/chi2_expected_by_subset_20250831_102743.csv\n",
            " - 4-way Pearson z: analysis_outputs/chi2_pearson_z_by_subset_20250831_102743.csv\n",
            " - 4-way cell p-values: analysis_outputs/chi2_cell_p_by_subset_20250831_102743.csv\n",
            " - 4-way cell p-values (BH): analysis_outputs/chi2_cell_padj_BH_by_subset_20250831_102743.csv\n",
            " - Two-proportion (subset vs BNC): analysis_outputs/two_prop_newcombe_by_subset_20250831_102743.csv\n",
            " - Binomial (subset vs BNC): analysis_outputs/binomial_tests_by_subset_20250831_102743.csv\n",
            " - Continuous tests (subset vs BNC): analysis_outputs/continuous_tests_by_subset_20250831_102743.csv\n",
            " - Segment tests (subset vs BNC): analysis_outputs/continuous_segment_tests_by_subset_20250831_102743.csv\n",
            " - Between presence by group: analysis_outputs/between_presence_by_group_20250831_102743.csv\n",
            " - Pattern flags (subset vs BNC): analysis_outputs/pattern_flag_tests_by_subset_20250831_102743.csv\n",
            " - Top anchor signatures: analysis_outputs/top_anchor_signatures_by_group_20250831_102743.csv\n",
            " - Anchor-signature divergence: analysis_outputs/anchor_signature_js_divergence_20250831_102743.csv\n",
            " - Topics JSON (per subset): analysis_outputs/lda_topics_by_subset_20250831_102743.json\n",
            " - Topics CSV (per subset): analysis_outputs/lda_topics_by_subset_20250831_102743.csv\n",
            " - Topic mix CSV (per subset): analysis_outputs/lda_topic_mix_by_subset_20250831_102743.csv\n",
            " - Master summary JSON: analysis_outputs/stats_and_topics_summary_by_subset_20250831_102743.json\n",
            "\n",
            "DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Academic Reporting and Documentation\n",
        "\n",
        "## 8.1 Professional Report Generation\n",
        "The HTML report generator produces a polished, self-contained document suitable for:\n",
        "- **Peer review** and journal **supplementary materials**\n",
        "- Internal/external **research documentation** and **reproducibility** records\n",
        "- **Academic presentations** (slides/posters) and dissemination to non-specialists\n",
        "\n",
        "It compiles results directly from the timestamped CSV/JSON artifacts written to `analysis_outputs/`, ensuring the report is traceable to concrete files.\n",
        "\n",
        "## 8.2 Results Integration\n",
        "The report synthesizes all major components of the analysis, including:\n",
        "- **Instance-aligned F1** summaries (exact + fuzzy sentence matching) for extractor vs manual annotations\n",
        "- **4-way χ²** contingency results with **expected counts**, **standardized residuals (z)**, and **BH-FDR** per-cell p-values\n",
        "- **Two-proportion tests** (Newcombe CIs) with **Cohen’s h** effect sizes, BH-FDR corrected\n",
        "- **Binomial tests** vs BNC reference proportions, BH-FDR corrected\n",
        "- **Continuous feature** comparisons (Welch *t*, Mann–Whitney *U*) with **Hedges’ g**, **Cliff’s δ**, and optional **Hodges–Lehmann** shifts (bootstrap CIs)\n",
        "- **Topic modelling** per subset (LDA top words) and **mean topic-mix** weights\n",
        "- **Dataset and category overviews**, reflecting the **harmonised taxonomy** with **Quasi_Similes** as the unified label\n",
        "\n",
        "## 8.3 Academic Standards and Transparency\n",
        "The report follows scholarly communication norms:\n",
        "- Clear sectioning, professional typography, descriptive figure/table captions\n",
        "- Explicit **statistical choices** (e.g., BH-FDR; Monte-Carlo χ² fallback when expected counts are low)\n",
        "- **Environment stamping** (Python/library versions), deterministic seeds, and a **file manifest** with timestamps for reproducibility\n",
        "- **Taxonomy harmonisation** note (Joycean_Quasi → **Quasi_Similes**) to prevent label inflation\n",
        "- Methodological caveats (e.g., **TextBlob sentiment** reported as exploratory; the **less-restrictive NLP** subset being **all Standard** is treated as a control and excluded in robustness checks when appropriate)\n",
        "\n",
        "## 8.4 Reproducibility & Data Availability (recommended)\n",
        "- Archive the full `analysis_outputs/` directory, `comprehensive_linguistic_analysis_corrected.csv`, and this notebook.\n",
        "- Include licensing/usage notes for external corpora (e.g., **BNC** access terms) and cite the **Project Gutenberg** source for *Dubliners*.\n",
        "- Provide a short **README** describing how to re-run Cells 1–3 and regenerate the identical HTML using the saved artifacts.\n"
      ],
      "metadata": {
        "id": "RwoglydbeaCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ACADEMIC HTML REPORT GENERATOR — UPDATED (data-driven, harmonised labels)\n",
        "# Generates a comprehensive, reproducible HTML report from Cells 1–2 outputs\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "print(\"GENERATING ACADEMIC HTML REPORT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Timestamp for report\n",
        "now = datetime.now()\n",
        "report_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "report_date = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def latest_file(pattern, base=\"analysis_outputs\"):\n",
        "    files = glob.glob(os.path.join(base, pattern))\n",
        "    return max(files, key=os.path.getctime) if files else None\n",
        "\n",
        "def safe_read_csv(path):\n",
        "    if not path or not os.path.exists(path):\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load CSV {path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def safe_read_json(path):\n",
        "    if not path or not os.path.exists(path):\n",
        "        return {}\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load JSON {path}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def df_for_html(df, index=False, max_rows=None):\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "    d = df.copy()\n",
        "    if max_rows is not None and len(d) > max_rows:\n",
        "        d = d.head(max_rows)\n",
        "        d.__truncated__ = True\n",
        "    return d\n",
        "\n",
        "def create_table_html(df, title=\"\", max_rows=20, index=False):\n",
        "    \"\"\"Create HTML table with styling; auto-handle empty and truncation note.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return f\"<p><em>No data available for {title}</em></p>\"\n",
        "    d = df.copy()\n",
        "    trunc = False\n",
        "    if max_rows and len(d) > max_rows:\n",
        "        d = d.head(max_rows)\n",
        "        trunc = True\n",
        "    if index:\n",
        "        d = d.reset_index()\n",
        "    table_html = d.to_html(classes='analysis-table', escape=False, index=False)\n",
        "    note = f\"<p class='truncated-note'><em>Showing first {max_rows} of {len(df)} rows</em></p>\" if trunc else \"\"\n",
        "    return f\"\"\"\n",
        "    <div class=\"table-container\">\n",
        "        <h4>{title}</h4>\n",
        "        <div class=\"table-wrapper\">{table_html}</div>\n",
        "        {note}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# ---------- Pull in analysis outputs from Cell 2 ----------\n",
        "out_dir = \"analysis_outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "paths = {\n",
        "    # Chi-square + per-cell\n",
        "    \"chi2_cont\": latest_file(\"chi2_contingency_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_exp\": latest_file(\"chi2_expected_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_z\": latest_file(\"chi2_pearson_z_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_p\": latest_file(\"chi2_cell_p_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_padj\": latest_file(\"chi2_cell_padj_BH_by_subset_*.csv\", out_dir),\n",
        "    # Proportions / binomial\n",
        "    \"two_prop\": latest_file(\"two_prop_newcombe_by_subset_*.csv\", out_dir),\n",
        "    \"binom\": latest_file(\"binomial_tests_by_subset_*.csv\", out_dir),\n",
        "    # Continuous (legacy)\n",
        "    \"cont\": latest_file(\"continuous_tests_by_subset_*.csv\", out_dir),\n",
        "    # NEW: Comparator-segment metrics & presence\n",
        "    \"seg_cont\": latest_file(\"continuous_segment_tests_by_subset_*.csv\", out_dir),\n",
        "    \"presence\": latest_file(\"between_presence_by_group_*.csv\", out_dir),\n",
        "    # NEW: Pattern flags & anchor signatures\n",
        "    \"pattern_flags\": latest_file(\"pattern_flag_tests_by_subset_*.csv\", out_dir),\n",
        "    \"top_sigs\": latest_file(\"top_anchor_signatures_by_group_*.csv\", out_dir),\n",
        "    \"js_div\": latest_file(\"anchor_signature_js_divergence_*.csv\", out_dir),\n",
        "    # Topics\n",
        "    \"topics\": latest_file(\"lda_topics_by_subset_*.csv\", out_dir),\n",
        "    \"topicmix\": latest_file(\"lda_topic_mix_by_subset_*.csv\", out_dir),\n",
        "    # HL shifts if present\n",
        "    \"hl\": latest_file(\"continuous_HL_shifts_*.csv\", out_dir),\n",
        "    # Master summary\n",
        "    \"master\": latest_file(\"stats_and_topics_summary_by_subset_*.json\", out_dir),\n",
        "}\n",
        "\n",
        "chi2_cont_df = safe_read_csv(paths[\"chi2_cont\"])\n",
        "chi2_exp_df  = safe_read_csv(paths[\"chi2_exp\"])\n",
        "chi2_z_df    = safe_read_csv(paths[\"chi2_z\"])\n",
        "chi2_p_df    = safe_read_csv(paths[\"chi2_p\"])\n",
        "chi2_padj_df = safe_read_csv(paths[\"chi2_padj\"])\n",
        "two_prop_df  = safe_read_csv(paths[\"two_prop\"])\n",
        "binom_df     = safe_read_csv(paths[\"binom\"])\n",
        "cont_df      = safe_read_csv(paths[\"cont\"])\n",
        "seg_cont_df  = safe_read_csv(paths[\"seg_cont\"])\n",
        "presence_df  = safe_read_csv(paths[\"presence\"])\n",
        "pattern_df   = safe_read_csv(paths[\"pattern_flags\"])\n",
        "top_sigs_df  = safe_read_csv(paths[\"top_sigs\"])\n",
        "js_div_df    = safe_read_csv(paths[\"js_div\"])\n",
        "topics_df    = safe_read_csv(paths[\"topics\"])\n",
        "topicmix_df  = safe_read_csv(paths[\"topicmix\"])\n",
        "hl_df        = safe_read_csv(paths[\"hl\"])\n",
        "master_json  = safe_read_json(paths[\"master\"])\n",
        "\n",
        "# ---------- Summaries from Cell 1 (results_df, f1_analysis, comparator.env_info) ----------\n",
        "def create_summary_stats_html():\n",
        "    if 'results_df' not in globals() or results_df is None or results_df.empty:\n",
        "        return \"<p><em>No results data available</em></p>\"\n",
        "    dcounts = results_df['Original_Dataset'].value_counts()\n",
        "    ccounts = results_df['Category_Framework'].value_counts()\n",
        "\n",
        "    items_d = \"\".join([f\"<li><strong>{k}:</strong> {int(v):,} instances</li>\" for k,v in dcounts.items()])\n",
        "    items_c = \"\".join([f\"<li><strong>{k}:</strong> {int(v):,} ({(v/len(results_df))*100:.1f}%)</li>\" for k,v in ccounts.items()])\n",
        "\n",
        "    return f\"\"\"\n",
        "    <div class=\"summary-stats\">\n",
        "        <div class=\"stat-group\">\n",
        "            <h4>Dataset Distribution</h4>\n",
        "            <ul>{items_d}</ul>\n",
        "            <p><strong>Total Instances:</strong> {len(results_df):,}</p>\n",
        "        </div>\n",
        "        <div class=\"stat-group\">\n",
        "            <h4>Category Distribution</h4>\n",
        "            <ul>{items_c}</ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def f1_summary_html():\n",
        "    micro_rb = macro_rb = micro_nlp = macro_nlp = None\n",
        "    pairs_rb = pairs_nlp = None\n",
        "    if 'f1_analysis' in globals() and isinstance(f1_analysis, dict):\n",
        "        rb = f1_analysis.get('rule_based_vs_manual') or f1_analysis.get('Rule-Based vs Manual')\n",
        "        nl = f1_analysis.get('nlp_vs_manual') or f1_analysis.get('NLP vs Manual')\n",
        "        if rb and rb.get(\"overall\"):\n",
        "            micro_rb = rb[\"overall\"].get(\"micro_f1\")\n",
        "            macro_rb = rb[\"overall\"].get(\"macro_f1\")\n",
        "            pairs_rb = rb.get(\"pairs\")\n",
        "        if nl and nl.get(\"overall\"):\n",
        "            micro_nlp = nl[\"overall\"].get(\"micro_f1\")\n",
        "            macro_nlp = nl[\"overall\"].get(\"macro_f1\")\n",
        "            pairs_nlp = nl.get(\"pairs\")\n",
        "    elif 'comparator' in globals():\n",
        "        try:\n",
        "            fa = comparator.comparison_results.get('f1_analysis', {})\n",
        "            rb = fa.get('rule_based_vs_manual')\n",
        "            nl = fa.get('nlp_vs_manual')\n",
        "            if rb and rb.get(\"overall\"):\n",
        "                micro_rb = rb[\"overall\"].get(\"micro_f1\")\n",
        "                macro_rb = rb[\"overall\"].get(\"macro_f1\")\n",
        "                pairs_rb = rb.get(\"pairs\")\n",
        "            if nl and nl.get(\"overall\"):\n",
        "                micro_nlp = nl[\"overall\"].get(\"micro_f1\")\n",
        "                macro_nlp = nl[\"overall\"].get(\"macro_f1\")\n",
        "                pairs_nlp = nl.get(\"pairs\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if micro_rb is None and micro_nlp is None:\n",
        "        return \"<p><em>F1 metrics unavailable (run Cell 1 in this session).</em></p>\"\n",
        "\n",
        "    lines = []\n",
        "    if micro_rb is not None:\n",
        "        lines.append(f\"• Rule-Based vs Manual: micro-F1 = {micro_rb:.3f}, macro-F1 = {macro_rb:.3f}{' (pairs=' + str(pairs_rb) + ')' if pairs_rb else ''}\")\n",
        "    if micro_nlp is not None:\n",
        "        lines.append(f\"• NLP vs Manual: micro-F1 = {micro_nlp:.3f}, macro-F1 = {macro_nlp:.3f}{' (pairs=' + str(pairs_nlp) + ')' if pairs_nlp else ''}\")\n",
        "\n",
        "    return f\"\"\"\n",
        "    <div class=\"highlight\">\n",
        "      <strong>F1 Summary:</strong><br>\n",
        "      {'<br>'.join(lines)}\n",
        "      <br>• Total instances processed: {len(results_df):,} (across all datasets)\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def env_html():\n",
        "    info = {}\n",
        "    if 'comparator' in globals():\n",
        "        try:\n",
        "            info = comparator.env_info\n",
        "        except Exception:\n",
        "            info = {}\n",
        "    if not info:\n",
        "        return \"\"\n",
        "    items = \"\".join([f\"<li><strong>{k}:</strong> {v}</li>\" for k,v in info.items()])\n",
        "    return f\"\"\"\n",
        "    <div class=\"methodology\">\n",
        "      <strong>Environment:</strong>\n",
        "      <ul>{items}</ul>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# ---------- Build analytic highlight snippets ----------\n",
        "def chi2_summary_html():\n",
        "    if not master_json:\n",
        "        return \"\"\n",
        "    chi = master_json.get(\"chi_square_4way\", {})\n",
        "    chi_line = (f\"χ² = {chi.get('chi2', float('nan')):.3f} | df = {chi.get('dof', 0)} | \"\n",
        "                f\"p = {chi.get('p_value', float('nan')):.3g} | Cramér’s V = {chi.get('cramers_v', float('nan')):.3f} \"\n",
        "                f\"| Monte-Carlo={chi.get('monte_carlo', False)} | N={chi.get('N', 0)}\")\n",
        "    return f\"<p>{chi_line}</p>\"\n",
        "\n",
        "def top_residuals_html(k=10):\n",
        "    if chi2_z_df.empty or chi2_cont_df.empty or chi2_exp_df.empty:\n",
        "        return \"\"\n",
        "    z_long = chi2_z_df.copy()\n",
        "    z_long = z_long.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in z_long.columns else z_long\n",
        "    z_long = z_long.melt(id_vars=[c for c in [\"Category_Framework\"] if c in z_long.columns],\n",
        "                         var_name=\"Group\", value_name=\"z\")\n",
        "    z_long[\"abs_z\"] = z_long[\"z\"].abs()\n",
        "    cont = chi2_cont_df.copy()\n",
        "    cont = cont.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in cont.columns else cont\n",
        "    exp = chi2_exp_df.copy()\n",
        "    exp = exp.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in exp.columns else exp\n",
        "    obs_long = cont.melt(id_vars=[c for c in [\"Category_Framework\"] if c in cont.columns],\n",
        "                         var_name=\"Group\", value_name=\"obs\")\n",
        "    exp_long = exp.melt(id_vars=[c for c in [\"Category_Framework\"] if c in exp.columns],\n",
        "                        var_name=\"Group\", value_name=\"exp\")\n",
        "    m = (z_long.merge(obs_long, on=[\"Category_Framework\",\"Group\"], how=\"left\")\n",
        "               .merge(exp_long, on=[\"Category_Framework\",\"Group\"], how=\"left\"))\n",
        "    m = m.sort_values(\"abs_z\", ascending=False).head(k)\n",
        "    return create_table_html(m, f\"Top {k} standardized residuals (|z|)\", max_rows=k)\n",
        "\n",
        "def sig_two_prop_html(alpha=0.05, top=15):\n",
        "    if two_prop_df.empty or \"p_adj_BH\" not in two_prop_df.columns:\n",
        "        return \"\"\n",
        "    df = two_prop_df.copy()\n",
        "    df[\"|h|\"] = df.get(\"cohens_h\", 0).abs()\n",
        "    sig = df[df[\"p_adj_BH\"] < alpha].sort_values([\"p_adj_BH\",\"|h|\"])\n",
        "    cols = [c for c in [\"Subset\",\"Category\",\"prop_A\",\"prop_B\",\"z\",\"p_value\",\"p_adj_BH\",\"cohens_h\"] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Two-proportion (BH<{alpha}) — top {top}\", max_rows=top)\n",
        "\n",
        "def sig_binom_html(alpha=0.05, top=15):\n",
        "    if binom_df.empty or \"p_adj_BH\" not in binom_df.columns:\n",
        "        return \"\"\n",
        "    df = binom_df.copy()\n",
        "    sig = df[df[\"p_adj_BH\"] < alpha].sort_values(\"p_adj_BH\")\n",
        "    cols = [c for c in [\"Subset\",\"Category\",\"count_A\",\"n_A\",\"p_ref_BNC\",\"p_value\",\"p_adj_BH\"] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Binomial vs BNC (BH<{alpha}) — top {top}\", max_rows=top)\n",
        "\n",
        "def sig_continuous_html(alpha=0.05):\n",
        "    if cont_df.empty:\n",
        "        return \"\"\n",
        "    d = cont_df.copy()\n",
        "    if \"U_padj_BH\" in d.columns:\n",
        "        sig = d[d[\"U_padj_BH\"] < alpha].sort_values(\"U_padj_BH\")\n",
        "        label = \"Mann–Whitney U (BH)\"\n",
        "        padj_col = \"U_padj_BH\"\n",
        "    elif \"t_padj_BH\" in d.columns:\n",
        "        sig = d[d[\"t_padj_BH\"] < alpha].sort_values(\"t_padj_BH\")\n",
        "        label = \"Welch t (BH)\"\n",
        "        padj_col = \"t_padj_BH\"\n",
        "    else:\n",
        "        return \"\"\n",
        "    if sig.empty:\n",
        "        return \"<p><em>No continuous feature differences were significant after FDR correction.</em></p>\"\n",
        "    cols = [c for c in [\"Feature\",\"Subset\",\"A_n\",\"B_n\",\"A_mean\",\"B_mean\",\"A_median\",\"B_median\",\n",
        "                        \"t_stat\",\"t_pvalue\",\"U_stat\",\"U_pvalue\",\"hedges_g\",\"cliffs_delta\", padj_col] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Continuous features — significant by {label}\", max_rows=15)\n",
        "\n",
        "# NEW: significant segment metrics (Pre/Between/Post family)\n",
        "def sig_seg_continuous_html(alpha=0.05):\n",
        "    if seg_cont_df.empty:\n",
        "        return \"\"\n",
        "    d = seg_cont_df.copy()\n",
        "    padj_col = \"U_padj_BH\" if \"U_padj_BH\" in d.columns else (\"t_padj_BH\" if \"t_padj_BH\" in d.columns else None)\n",
        "    if not padj_col:\n",
        "        return \"\"\n",
        "    sig = d[d[padj_col] < alpha].copy()\n",
        "    if sig.empty:\n",
        "        return \"<p><em>No segment metric differences were significant after FDR correction.</em></p>\"\n",
        "    # Order: strongest adjusted p then effect size magnitude if available\n",
        "    if \"hedges_g\" in sig.columns:\n",
        "        sig[\"|g|\"] = sig[\"hedges_g\"].abs()\n",
        "        sig = sig.sort_values([padj_col,\"|g|\"])\n",
        "    else:\n",
        "        sig = sig.sort_values(padj_col)\n",
        "    keep = [c for c in [\"Feature\",\"Subset\",\"A_n\",\"B_n\",\"A_mean\",\"B_mean\",\"A_median\",\"B_median\",\n",
        "                        \"t_stat\",\"t_pvalue\",\"U_stat\",\"U_pvalue\",\"hedges_g\",\"cliffs_delta\", padj_col] if c in sig.columns]\n",
        "    return create_table_html(sig[keep], f\"Comparator-segment metrics — significant (BH<{alpha})\", max_rows=20)\n",
        "\n",
        "# NEW: pattern flags (BH<0.05) table\n",
        "def sig_pattern_flags_html(alpha=0.05, top=25):\n",
        "    if pattern_df.empty or \"p_adj_BH\" not in pattern_df.columns:\n",
        "        return \"\"\n",
        "    d = pattern_df.copy()\n",
        "    d[\"abs_diff\"] = (d[\"prop_A\"] - d[\"prop_B\"]).abs()\n",
        "    sig = d[d[\"p_adj_BH\"] < alpha].sort_values([\"p_adj_BH\",\"abs_diff\"])\n",
        "    keep = [c for c in [\"Subset\",\"Flag\",\"prop_A\",\"prop_B\",\"z\",\"p_value\",\"p_adj_BH\"] if c in sig.columns]\n",
        "    return create_table_html(sig[keep], f\"Pattern/anchor flags — significant (BH<{alpha}) — top {top}\", max_rows=top)\n",
        "\n",
        "# NEW: top anchor signatures per group (show top 10 each)\n",
        "def top_anchor_signatures_html():\n",
        "    if top_sigs_df.empty:\n",
        "        return \"\"\n",
        "    # For compactness, keep top 10 per group\n",
        "    parts = []\n",
        "    for g in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\",\"BNC\"]:\n",
        "        sub = top_sigs_df[top_sigs_df[\"Group\"]==g]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        sub = sub.sort_values(\"Count\", ascending=False).head(10)\n",
        "        parts.append(create_table_html(sub, f\"Top anchor signatures — {g}\", max_rows=10))\n",
        "    return \"\".join(parts) if parts else \"\"\n",
        "\n",
        "# NEW: JS divergence (anchor-signature distributions)\n",
        "def anchor_js_divergence_html():\n",
        "    if js_div_df.empty:\n",
        "        return \"\"\n",
        "    d = js_div_df.copy().sort_values(\"JSD\", ascending=False)\n",
        "    return create_table_html(d, \"Anchor-signature divergence (Jensen–Shannon; higher = more different)\", max_rows=12)\n",
        "\n",
        "# ---------- Build the HTML ----------\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/>\n",
        "<title>Joyce Simile Research: Comprehensive Linguistic Analysis Report</title>\n",
        "<style>\n",
        "    body {{\n",
        "        font-family: 'Times New Roman', serif;\n",
        "        line-height: 1.6;\n",
        "        margin: 0;\n",
        "        padding: 20px;\n",
        "        background-color: #f9f9f9;\n",
        "        color: #333;\n",
        "    }}\n",
        "    .container {{\n",
        "        max-width: 1200px;\n",
        "        margin: 0 auto;\n",
        "        background: white;\n",
        "        padding: 30px;\n",
        "        border-radius: 8px;\n",
        "        box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
        "    }}\n",
        "    .header {{\n",
        "        text-align: center;\n",
        "        border-bottom: 3px solid #2c3e50;\n",
        "        padding-bottom: 20px;\n",
        "        margin-bottom: 30px;\n",
        "    }}\n",
        "    .header h1 {{ color: #2c3e50; margin: 0; font-size: 2.2em; font-weight: bold; }}\n",
        "    .header .subtitle {{ color: #7f8c8d; font-size: 1.1em; margin: 10px 0 5px 0; font-style: italic; }}\n",
        "    .header .timestamp {{ color: #95a5a6; font-size: 0.9em; }}\n",
        "    .section {{\n",
        "        margin: 30px 0;\n",
        "        padding: 20px;\n",
        "        border-left: 4px solid #3498db;\n",
        "        background-color: #f8f9fa;\n",
        "    }}\n",
        "    .section h2 {{\n",
        "        color: #2c3e50;\n",
        "        margin-top: 0; border-bottom: 2px solid #ecf0f1; padding-bottom: 10px;\n",
        "    }}\n",
        "    .section h3 {{ color: #34495e; margin-top: 25px; }}\n",
        "    .section h4 {{ color: #5d6d7e; margin-top: 20px; margin-bottom: 10px; }}\n",
        "    .analysis-table {{ width: 100%; border-collapse: collapse; margin: 15px 0; font-size: 0.9em; }}\n",
        "    .analysis-table th {{ background:#34495e; color:#fff; padding:12px 8px; text-align:left; font-weight:bold; }}\n",
        "    .analysis-table td {{ padding:10px 8px; border-bottom:1px solid #ddd; }}\n",
        "    .analysis-table tr:nth-child(even) {{ background-color:#f2f2f2; }}\n",
        "    .analysis-table tr:hover {{ background-color:#e8f4fd; }}\n",
        "    .summary-stats {{ display:grid; grid-template-columns:1fr 1fr; gap:30px; margin:20px 0; }}\n",
        "    .stat-group {{ background:#fff; padding:20px; border-radius:6px; border:1px solid #e1e8ed; }}\n",
        "    .stat-group h4 {{ margin-top:0; color:#2c3e50; border-bottom:1px solid #ecf0f1; padding-bottom:8px; }}\n",
        "    .stat-group ul {{ list-style-type:none; padding:0; }}\n",
        "    .stat-group li {{ padding:5px 0; border-bottom:1px solid #f8f9fa; }}\n",
        "    .highlight {{ background:#d1ecf1; padding:15px; border-left:4px solid #17a2b8; margin:15px 0; }}\n",
        "    .methodology {{ background:#f8f9fa; padding:15px; border-radius:5px; margin:15px 0; font-style:italic; }}\n",
        "    .table-container {{ margin:20px 0; }}\n",
        "    .table-wrapper {{ overflow-x:auto; }}\n",
        "    .truncated-note {{ color:#6c757d; font-size:0.9em; margin-top:5px; }}\n",
        "    .footer {{ text-align:center; margin-top:40px; padding-top:20px; border-top:2px solid #ecf0f1; color:#7f8c8d; font-size:0.9em; }}\n",
        "    @media (max-width: 768px) {{\n",
        "        .summary-stats {{ grid-template-columns:1fr; }}\n",
        "        .container {{ padding:15px; }}\n",
        "        .analysis-table {{ font-size:0.85em; }}\n",
        "    }}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <div class=\"header\">\n",
        "        <h1>Joyce Simile Research</h1>\n",
        "        <div class=\"subtitle\">Comprehensive Linguistic Analysis Report</div>\n",
        "        <div class=\"subtitle\">Manual Annotations vs Extraction Methods vs BNC Baseline</div>\n",
        "        <div class=\"timestamp\">Generated on {report_timestamp}</div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Executive Summary</h2>\n",
        "        <p>This report compiles the results of a comparative analysis of simile usage in <em>Dubliners</em>, spanning manual expert annotations, a restrictive rule-based extractor, a less-restrictive NLP extractor, and a BNC baseline. Categories were harmonised with <strong>Quasi_Similes</strong> as the unified label for the Joyce/BNC quasi-simile phenomenon.</p>\n",
        "        {chi2_summary_html()}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Dataset Overview</h2>\n",
        "        <p>Four datasets were analysed, representing complementary identification approaches and a baseline:</p>\n",
        "        {create_summary_stats_html()}\n",
        "        {env_html()}\n",
        "        <div class=\"methodology\">\n",
        "            <strong>Methodology:</strong> Features include token counts, comparator-segment metrics (Pre / Between / Post), pre/post comparator ratio, POS distribution, syntactic complexity, TextBlob sentiment (exploratory), and comparative structure flags. Between spans are computed across detected comparator anchors (e.g., <em>as … as</em>, <em>like</em>, framing punctuation); we report totals and ratios, assess group differences with non-parametric tests, and a χ² test on presence (Between &gt; 0).\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Performance Metrics</h2>\n",
        "        <h3>Instance-Aligned F1 Scores</h3>\n",
        "        {f1_summary_html()}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Statistical Results</h2>\n",
        "        <h3>4-way Category Distribution</h3>\n",
        "\"\"\"\n",
        "\n",
        "# 4-way chi-square tables\n",
        "if not chi2_cont_df.empty:\n",
        "    html_content += create_table_html(chi2_cont_df, \"Observed counts by group × category\", max_rows=10, index=True)\n",
        "if not chi2_exp_df.empty:\n",
        "    html_content += create_table_html(chi2_exp_df, \"Expected counts under independence\", max_rows=10, index=True)\n",
        "if not chi2_z_df.empty:\n",
        "    html_content += create_table_html(chi2_z_df, \"Pearson standardized residuals (z)\", max_rows=10, index=True)\n",
        "    html_content += top_residuals_html(k=10)\n",
        "if not chi2_padj_df.empty:\n",
        "    html_content += create_table_html(chi2_padj_df, \"Per-cell p-values (BH-adjusted)\", max_rows=10, index=True)\n",
        "\n",
        "# Two-proportion\n",
        "if not two_prop_df.empty:\n",
        "    html_content += \"\"\"\n",
        "        <h3>Two-Proportion Tests (Joyce subsets vs BNC)</h3>\n",
        "        <p>Newcombe confidence intervals and Cohen’s h. BH correction applied across tests.</p>\n",
        "    \"\"\"\n",
        "    if \"p_adj_BH\" not in two_prop_df.columns and \"p_value\" in two_prop_df.columns:\n",
        "        pv = two_prop_df[\"p_value\"].fillna(1.0).to_numpy()\n",
        "        order = np.argsort(pv)\n",
        "        ranked = np.empty_like(pv, dtype=float)\n",
        "        cummin = 1.0\n",
        "        n = pv.size\n",
        "        for i, idx in enumerate(order[::-1], start=1):\n",
        "            rank = n - i + 1\n",
        "            val = pv[idx] * n / rank\n",
        "            cummin = min(cummin, val)\n",
        "            ranked[idx] = min(cummin, 1.0)\n",
        "        two_prop_df[\"p_adj_BH\"] = ranked\n",
        "    html_content += create_table_html(two_prop_df, \"All two-proportion results\", max_rows=15)\n",
        "    html_content += sig_two_prop_html(alpha=0.05, top=15)\n",
        "\n",
        "# Binomial\n",
        "if not binom_df.empty:\n",
        "    if \"p_adj_BH\" not in binom_df.columns and \"p_value\" in binom_df.columns:\n",
        "        pv = binom_df[\"p_value\"].fillna(1.0).to_numpy()\n",
        "        order = np.argsort(pv)\n",
        "        ranked = np.empty_like(pv, dtype=float)\n",
        "        cummin = 1.0\n",
        "        n = pv.size\n",
        "        for i, idx in enumerate(order[::-1], start=1):\n",
        "            rank = n - i + 1\n",
        "            val = pv[idx] * n / rank\n",
        "            cummin = min(cummin, val)\n",
        "            ranked[idx] = min(cummin, 1.0)\n",
        "        binom_df[\"p_adj_BH\"] = ranked\n",
        "    html_content += \"\"\"\n",
        "        <h3>Binomial Tests (Joyce subsets vs BNC reference proportion)</h3>\n",
        "        <p>One-sample tests per category per subset; BH correction across tests.</p>\n",
        "    \"\"\"\n",
        "    html_content += create_table_html(binom_df, \"All binomial results\", max_rows=12)\n",
        "    html_content += sig_binom_html(alpha=0.05, top=12)\n",
        "\n",
        "# Continuous (legacy features)\n",
        "if not cont_df.empty:\n",
        "    html_content += \"\"\"\n",
        "        <h3>Continuous Features</h3>\n",
        "        <p>Welch t and Mann–Whitney U with effect sizes (Hedges’ g, Cliff’s δ). BH correction applied.</p>\n",
        "    \"\"\"\n",
        "    html_content += create_table_html(cont_df, \"Continuous feature comparisons\", max_rows=12)\n",
        "    html_content += sig_continuous_html(alpha=0.05)\n",
        "if not hl_df.empty:\n",
        "    html_content += create_table_html(hl_df, \"Hodges–Lehmann location shifts (A − B) with bootstrap CIs\", max_rows=12)\n",
        "\n",
        "# NEW: Comparator-segment metrics & presence\n",
        "html_content += \"\"\"\n",
        "    <h3>Comparator-Segment Metrics (Pre / Between / Post)</h3>\n",
        "    <p>Integrated from Cell 1; totals and shares per segment, plus between-span diagnostics.</p>\n",
        "\"\"\"\n",
        "if not seg_cont_df.empty:\n",
        "    html_content += create_table_html(seg_cont_df, \"Segment metrics — subset vs BNC (Welch t / U, with BH)\", max_rows=20)\n",
        "    html_content += sig_seg_continuous_html(alpha=0.05)\n",
        "\n",
        "if not presence_df.empty:\n",
        "    html_content += create_table_html(presence_df, \"Between-clause presence × Group (counts)\", max_rows=10)\n",
        "\n",
        "# NEW: Pattern/anchor flags & signatures\n",
        "html_content += \"\"\"\n",
        "    <h3>Comparator Anchors & Pattern Flags</h3>\n",
        "    <p>Binary flags for anchors (e.g., <em>like</em>, <em>as…as</em>, punctuation) and between-span diagnostics; two-proportion tests vs BNC with BH correction.</p>\n",
        "\"\"\"\n",
        "if not pattern_df.empty:\n",
        "    html_content += create_table_html(pattern_df, \"All pattern/anchor flag tests\", max_rows=20)\n",
        "    html_content += sig_pattern_flags_html(alpha=0.05, top=25)\n",
        "\n",
        "if not top_sigs_df.empty:\n",
        "    html_content += top_anchor_signatures_html()\n",
        "if not js_div_df.empty:\n",
        "    html_content += anchor_js_divergence_html()\n",
        "\n",
        "# Topics\n",
        "html_content += \"\"\"\n",
        "    </div>\n",
        "    <div class=\"section\">\n",
        "        <h2>Topic Modelling</h2>\n",
        "        <p>Per-subset LDA topics (CountVectorizer; 5 topics × 10 words). Topic mixture weights summarised by mean.</p>\n",
        "\"\"\"\n",
        "if not topics_df.empty:\n",
        "    html_content += create_table_html(topics_df, \"Top words per topic × group\", max_rows=20)\n",
        "if not topicmix_df.empty:\n",
        "    html_content += create_table_html(topicmix_df, \"Mean topic weights per group\", max_rows=20)\n",
        "\n",
        "# Sample of comprehensive results\n",
        "if 'results_df' in globals() and isinstance(results_df, pd.DataFrame) and not results_df.empty:\n",
        "    sample_cols = [c for c in ['Instance_ID','Original_Dataset','Category_Framework','Comparator_Type',\n",
        "                               'Sentence_Length','Sentiment_Polarity','Pre_Share','Between_Share','Post_Share',\n",
        "                               'Comp_Count','Between_Segments','Between_Max','Between_Mean'] if c in results_df.columns]\n",
        "    sample_results = results_df.head(25)[sample_cols].round(3)\n",
        "    html_content += f\"\"\"\n",
        "    </div>\n",
        "    <div class=\"section\">\n",
        "        <h2>Comprehensive Results — Sample</h2>\n",
        "        <p>Representative snippet of the full dataset (first 25 rows):</p>\n",
        "        {create_table_html(sample_results, \"Sample of comprehensive analysis\", max_rows=25)}\n",
        "        <div class=\"methodology\">\n",
        "            The full dataset includes lemmatisation, POS distributions, syntactic complexity, comparator anchors, and comparator-segment diagnostics.\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# Files list\n",
        "files_list = []\n",
        "files_list.append(\"comprehensive_linguistic_analysis_corrected.csv\")\n",
        "for k, p in paths.items():\n",
        "    if p:\n",
        "        files_list.append(os.path.relpath(p))\n",
        "files_items = \"\".join([f\"<li><code>{f}</code></li>\" for f in sorted(set(files_list))])\n",
        "\n",
        "html_content += f\"\"\"\n",
        "    <div class=\"section\">\n",
        "        <h2>Files Generated</h2>\n",
        "        <ul>{files_items}</ul>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"footer\">\n",
        "        <p>Generated by Comprehensive Linguistic Analysis Pipeline</p>\n",
        "        <p>Joyce Simile Research Project • {report_timestamp}</p>\n",
        "        <p><em>Categories harmonised with unified <strong>Quasi_Similes</strong>; results computed using FDR corrections where applicable.</em></p>\n",
        "    </div>\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Save HTML report\n",
        "report_filename = f\"joyce_simile_analysis_report_{report_date}.html\"\n",
        "with open(report_filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(f\"✓ Academic HTML report generated: {report_filename}\")\n",
        "print(f\"✓ File size: {os.path.getsize(report_filename):,} bytes\")\n",
        "print(f\"✓ Report length: {len(html_content):,} characters\")\n",
        "print(\"\\nREPORT READY — open in a browser to view.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLQyH9qIoIps",
        "outputId": "581b7d11-3cdb-4c8b-8da5-60fd3b6a1e80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATING ACADEMIC HTML REPORT\n",
            "==================================================\n",
            "✓ Academic HTML report generated: joyce_simile_analysis_report_20250831_105405.html\n",
            "✓ File size: 78,719 bytes\n",
            "✓ Report length: 78,665 characters\n",
            "\n",
            "REPORT READY — open in a browser to view.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Research Implications and Future Directions\n",
        "\n",
        "## 9.1 Computational Literary Analysis\n",
        "Using sentence-level **instance-aligned evaluation** (exact match, then fuzzy ≥ 0.92), the extractors partially recover the manual categories:\n",
        "- **Rule-Based vs Manual:** micro-F1 ≈ **0.343**, macro-F1 ≈ **0.178** (pairs matched: see Cell 1 output)\n",
        "- **NLP (Less-Restrictive) vs Manual:** micro-F1 ≈ **0.292**, macro-F1 ≈ **0.059**\n",
        "\n",
        "These scores indicate **meaningful but incomplete replication** of expert judgments—appropriate for unsupervised/heuristic extractors—and motivate targeted improvements (Section 9.4). Importantly, categorical distributions still differ strongly across corpora (**χ²≈281.88**, **Cramér’s V≈0.318**, *p*≪.001), so the **signal is primarily categorical**, not scalar.\n",
        "\n",
        "**Key distributional contrasts (harmonised labels):**\n",
        "- **BNC**: *Quasi_Similes* ≈ **41%** (82/200) vs **Joyce-Manual** ≈ **29%** (53/184).  \n",
        "- **Joyce-Manual** shows enrichments in **Joycean_Framed**, **Joycean_Quasi_Fuzzy**, and **Joycean_Silent**.\n",
        "- **NLP (Less-Restrictive)** is **all Standard** (330/330), by design—useful as a control but excluded in robustness checks where appropriate.\n",
        "\n",
        "## 9.2 Innovation Quantification\n",
        "Under the unified scheme:\n",
        "- **Joycean-specific categories** (Framed + Quasi_Fuzzy + Silent) account for **~20%** of **Joyce-Manual** (37/184).\n",
        "- **Quasi_Similes** appear **less** in **Joyce-Manual** (~29%) relative to **BNC** (~41%), with significant two-proportion differences (BH-corrected; moderate effects, e.g., |*h*|≈0.26–0.42 for Manual/Restrictive vs BNC).\n",
        "\n",
        "This suggests Joyce’s corpus **rebalances** the space of similes: fewer conventional quasi-similes than BNC and **more Joycean-specific forms**, evidencing stylistic innovation distinct from baseline English.\n",
        "\n",
        "## 9.3 Methodological Contributions\n",
        "- **Label harmonisation**: *Joycean_Quasi* merged into **Quasi_Similes** to align BNC and Joyce phenomena, avoiding double-counting and artificial χ² inflation.\n",
        "- **Robust inference**: 4-way χ² with per-cell residuals and **BH-FDR**; two-proportion tests with **Newcombe CIs** + **Cohen’s h**; binomial tests vs BNC reference; Welch-*t* & Mann–Whitney-*U* with **Hedges’ g** and **Cliff’s δ**; optional 3-way χ² excluding the degenerate NLP group.\n",
        "- **Instance-aligned evaluation**: sentence-pairing (exact+fuzzy) for extractor vs manual categories, a more faithful alternative to bag-of-counts F1.\n",
        "- **Reproducibility**: environment stamping; deterministic random seeds; saved CSV/JSON artifacts.\n",
        "\n",
        "## 9.4 Future Directions\n",
        "1. **Improve extractor recall** for Joycean categories  \n",
        "   - Add patterns for **punctuation-mediated** comparators (colon/semicolon/ellipsis) and **‘as … as’** spans.  \n",
        "   - Incorporate dependency features (governor-comparator-target) and discourse cues.\n",
        "\n",
        "2. **Supervised modelling**  \n",
        "   - Train a classifier on paired manual/extractor sentences (use current pairs as silver labels), with features from syntax, lemmata, and comparator spans.\n",
        "\n",
        "3. **Error analysis & ablations**  \n",
        "   - Per-category confusion and residuals to identify systematic misses; ablate comparator detection vs category mapping.\n",
        "\n",
        "4. **Topic–category linkage**  \n",
        "   - Relate LDA topic mixtures to categories (e.g., Framed vs Standard) and test with permutation or regression.\n",
        "\n",
        "5. **Generalisation**  \n",
        "   - Validate on other Joyce texts or contemporaries; check stability of the Joycean category enrichments.\n",
        "\n",
        "---\n",
        "\n",
        "## References and Data Sources\n",
        "\n",
        "**Primary Text**  \n",
        "- Joyce, James. *Dubliners*. Project Gutenberg.\n",
        "\n",
        "**Baseline Corpus**  \n",
        "- British National Corpus (BNC), used as a **standard English** reference. *Ensure appropriate licensing/access for the specific BNC source employed.*\n",
        "\n",
        "---\n",
        "\n",
        "## Computational Tools\n",
        "- **spaCy** (`en_core_web_sm`): tokenisation, POS, dependencies  \n",
        "- **scikit-learn**: CountVectorizer, LDA, utilities  \n",
        "- **SciPy**: χ², *t*-tests, Mann–Whitney U, binomial  \n",
        "- **statsmodels** (if available): two-proportion z-tests, Newcombe CIs  \n",
        "- **TextBlob**: exploratory sentiment  \n",
        "- **pandas / numpy**: data handling and numerics\n",
        "\n",
        "---\n",
        "\n",
        "## Research Framework (Summary)\n",
        "- **Evaluation**: instance-aligned F1 (micro/macro) via sentence pairing (exact + fuzzy ≥ 0.92).  \n",
        "- **Categorical inference**: 4-way χ² with per-cell residual z and BH-FDR; two-proportion tests (Newcombe CIs, Cohen’s h); binomial tests vs BNC proportions.  \n",
        "- **Continuous inference**: Welch-*t* and Mann–Whitney-*U* with **Hedges’ g**, **Cliff’s δ**, and **Hodges–Lehmann** shifts (bootstrap CIs).  \n",
        "- **Topics**: LDA per subset using **CountVectorizer** (5 topics × 10 words), with mean topic-mix summaries.  \n",
        "- **Harmonised taxonomy**: **Quasi_Similes** (unified), **Joycean_Framed**, **Joycean_Quasi_Fuzzy**, **Joycean_Silent**, **Standard**, **Uncategorized**.\n",
        "\n",
        "> **Terminology note:** *Quasi_Similes* is the unified tag covering both the BNC’s “Quasi_Similes” and Joyce’s former “Joycean_Quasi,” representing the same phenomenon.\n"
      ],
      "metadata": {
        "id": "C71bNmH3ejeg"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}