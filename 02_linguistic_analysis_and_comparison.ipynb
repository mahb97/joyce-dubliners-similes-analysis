{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/joyce-dubliners-similes-analysis/blob/main/02_linguistic_analysis_and_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw07RNuhGhxA"
      },
      "source": [
        "# Joyce Simile Research: Comprehensive Linguistic Analysis and Comparison Framework\n",
        "\n",
        "# Abstract\n",
        "\n",
        "This notebook presents a reproducible computational framework for analysing similes in James Joyce’s *Dubliners* and benchmarking extraction methods against a British National Corpus (BNC) baseline. Four datasets are integrated—manual close-reading (ground truth), a restrictive rule-based extractor, a less-restrictive NLP extractor, and BNC—under a harmonised taxonomy that merges *Joycean_Quasi* with the BNC’s *Quasi_Similes* into a single label **Quasi_Similes** to avoid double-counting. The pipeline performs sentence-level linguistic profiling (comparator-span detection, pre/post structure, POS distributions, syntactic complexity, exploratory sentiment) and evaluates extractors via **instance-aligned F1** (exact match then fuzzy ≥ 0.92) against the manual set.\n",
        "\n",
        "Statistical inference combines a 4-way χ² with standardized residuals and BH-FDR control, two-proportion tests (Newcombe CIs, Cohen’s *h*), binomial checks against BNC reference proportions, and continuous-feature comparisons (Welch *t*, Mann–Whitney *U*, Hedges’ *g*, Cliff’s δ). Topic modelling (LDA) summarises thematic variation per subset.\n",
        "\n",
        "Results show a strong categorical association across corpora (**χ² ≈ 281.88**, **df = 15**, **Cramér’s V ≈ 0.318**, *p* ≪ .001). **Quasi\\_Similes** are more prevalent in **BNC** (≈ 41 %) than in **Joyce-Manual** (≈ 29 %), while Joyce exhibits enrichments in **Joycean\\_Framed**, **Joycean\\_Quasi\\_Fuzzy**, and **Joycean\\_Silent** (≈ 20 % combined in the manual set). The less-restrictive NLP subset collapses to **Standard** (control), confirming distributional contrasts are not driven by it. Instance-aligned performance indicates partial recovery of expert labels (Rule-Based vs Manual: micro-F1 ≈ **0.343**, macro-F1 ≈ **0.178**; NLP vs Manual: micro-F1 ≈ **0.292**, macro-F1 ≈ **0.059**), motivating targeted pattern and dependency enhancements.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Comprehensive Linguistic Analysis Framework\n",
        "\n",
        "## 6.1 Multi-Dataset Integration\n",
        "The pipeline ingests four sources and standardises them to a common schema with stable IDs and explicit provenance:\n",
        "- **Manual_CloseReading** (expert annotations; ground truth)\n",
        "- **Restrictive_Dubliners** (rule-based extractor)\n",
        "- **NLP_LessRestrictive_PG** (broad pattern extractor; control—predominantly *Standard*)\n",
        "- **BNC_Baseline** (standard-English reference)\n",
        "\n",
        "Column names are normalised, missing IDs are repaired, and categories are **harmonised** so that *Joycean_Quasi* and the BNC’s *Quasi_Similes* are merged into a single label **Quasi_Similes**. This avoids double-counting and supports fair cross-corpus comparisons. All intermediate artefacts (CSV/JSON) are written with timestamps for reproducibility.\n",
        "\n",
        "## 6.2 Advanced Linguistic Feature Extraction\n",
        "Using spaCy (with a robust fallback), the framework derives sentence-level linguistic features tailored to simile analysis:\n",
        "- **Comparator span detection** for `like`, `as`, **as if/as though**, **as … as**, and lemma families (**seem\\***, **resembl\\***), plus punctuation-mediated comparators (**colon, semicolon, ellipsis, dash**).\n",
        "- **Pre/Post comparator structure**: token counts on each side and **Pre_Post_Ratio**.\n",
        "- **Syntactic complexity** via dependency depth; **POS distribution** and **lemmatised text**.\n",
        "- **Figurative density** from comparator/marker hits.\n",
        "- **Exploratory sentiment** (TextBlob polarity/subjectivity).\n",
        "If spaCy is unavailable, the fallback computes token counts, comparator positions, and sentiment only.\n",
        "\n",
        "## 6.3 Performance Validation\n",
        "Extractor performance is evaluated **instance-by-instance** against the manual set using **sentence alignment**:\n",
        "- Exact match first, then **fuzzy matching (≥ 0.92)** to pair sentences.\n",
        "- Per-category **TP/FP/FN** and **micro/macro F1** are computed from the aligned pairs—more faithful than bag-of-counts.\n",
        "- Statistical testing (reported in Section 8): **4-way χ²** with standardised residuals and BH-FDR; **two-proportion tests** (Newcombe CIs, Cohen’s *h*), **binomial checks** vs BNC, and **continuous** comparisons (Welch *t*, Mann–Whitney *U*, Hedges’ *g*, Cliff’s δ).  \n",
        "These procedures emphasise **effect sizes and error control**, and remain informative even when the less-restrictive NLP set collapses to *Standard*.\n",
        "\n"
      ],
      "metadata": {
        "id": "zXvVrXssd9qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from difflib import SequenceMatcher\n",
        "from datetime import datetime\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plot libs not used in this cell but kept for notebook continuity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional NLP libs\n",
        "try:\n",
        "    import spacy\n",
        "except Exception:\n",
        "    spacy = None\n",
        "\n",
        "try:\n",
        "    from textblob import TextBlob\n",
        "except Exception:\n",
        "    TextBlob = None\n",
        "\n",
        "print(\"COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS (FIXED)\")\n",
        "print(\"=\" * 75)\n",
        "print(\"Dataset 1: Manual Annotations (Ground Truth - Close Reading)\")\n",
        "print(\"Dataset 2: Rule-Based Extraction (Restrictive - Domain-Informed)\")\n",
        "print(\"Dataset 3: NLP Extraction (Less-Restrictive - PG Dubliners)\")\n",
        "print(\"Dataset 4: BNC Baseline Corpus (Standard English Reference)\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# Initialize spaCy if available\n",
        "nlp = None\n",
        "if spacy is not None:\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        print(\"spaCy pipeline loaded: en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"spaCy model not found; attempting to download…\")\n",
        "        os.system(\"python -m spacy download en_core_web_sm\")\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "            print(\"spaCy pipeline loaded after download: en_core_web_sm\")\n",
        "        except Exception:\n",
        "            print(\"spaCy unavailable; analysis will use simplified methods.\")\n",
        "\n",
        "class ComprehensiveLinguisticComparator:\n",
        "    \"\"\"\n",
        "    Full pipeline:\n",
        "      - robust loading & standardisation\n",
        "      - linguistic feature extraction (spaCy/TextBlob, simplified fallback)\n",
        "      - category harmonisation (MERGES Joycean_Quasi into Quasi_Similes)\n",
        "      - instance-aligned F1 (exact + fuzzy sentence matching)\n",
        "      - reproducibility & environment stamping\n",
        "      - combined CSV export with stable ordering\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = nlp\n",
        "        self.datasets = {}\n",
        "        self.linguistic_features = {}\n",
        "        self.comparison_results = {}\n",
        "\n",
        "        # Reproducibility\n",
        "        self.random_seed = 42\n",
        "        random.seed(self.random_seed)\n",
        "        np.random.seed(self.random_seed)\n",
        "\n",
        "        # Environment info for auditability\n",
        "        tb_ver = \"n/a\"\n",
        "        try:\n",
        "            import textblob as _tb\n",
        "            tb_ver = getattr(_tb, \"__version__\", \"n/a\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.env_info = {\n",
        "            \"python\": sys.version,\n",
        "            \"pandas\": pd.__version__,\n",
        "            \"numpy\": np.__version__,\n",
        "            \"spacy\": getattr(spacy, \"__version__\", \"n/a\") if spacy is not None else \"n/a\",\n",
        "            \"textblob\": tb_ver,\n",
        "        }\n",
        "        print(\"Environment:\", self.env_info)\n",
        "\n",
        "    # ---------- ID / Loading / Standardisation ----------\n",
        "\n",
        "    def _ensure_ids(self, df, dataset_name, prefix=None):\n",
        "        \"\"\"\n",
        "        Ensure a unique, non-null 'Instance_ID' string column exists.\n",
        "        If missing, non-unique, or contains NaNs, regenerate sequential IDs with a readable prefix.\n",
        "        \"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return pd.DataFrame(columns=['Instance_ID'])\n",
        "\n",
        "        short = (prefix or {\n",
        "            'manual': 'MAN',\n",
        "            'rule_based': 'RST',\n",
        "            'nlp': 'NLP',\n",
        "            'bnc': 'BNC'\n",
        "        }.get(dataset_name, dataset_name[:3].upper()))\n",
        "\n",
        "        candidates = ['Instance_ID', 'ID', 'id', 'sentence_id', 'Sentence_ID', 'Index', 'index']\n",
        "        chosen = next((c for c in candidates if c in df.columns), None)\n",
        "        if chosen and chosen != 'Instance_ID':\n",
        "            df = df.rename(columns={chosen: 'Instance_ID'})\n",
        "        elif not chosen:\n",
        "            df['Instance_ID'] = np.nan\n",
        "\n",
        "        # Normalize and test uniqueness\n",
        "        df['Instance_ID'] = df['Instance_ID'].astype(str).replace({'nan': np.nan, '': np.nan})\n",
        "        needs_regen = df['Instance_ID'].isna().any() or (not df['Instance_ID'].is_unique)\n",
        "        if needs_regen:\n",
        "            df['Instance_ID'] = [f\"{short}_{i+1:05d}\" for i in range(len(df))]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _load_manual_dataset_robust(self, file_content):\n",
        "        \"\"\"Robust loader for manual annotations with long quoted Joycean sentences.\"\"\"\n",
        "        import csv\n",
        "        try:\n",
        "            df = pd.read_csv(\n",
        "                file_content, encoding='cp1252', quotechar='\"',\n",
        "                quoting=csv.QUOTE_MINIMAL, skipinitialspace=True, engine='python'\n",
        "            )\n",
        "            if 'Sentence Context' in df.columns:\n",
        "                df = df[df['Sentence Context'].astype(str).str.lower() != 'sentence context'].copy()\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"  pandas (python engine) failed: {e}\")\n",
        "\n",
        "        # Fallback simpler read\n",
        "        try:\n",
        "            df = pd.read_csv(file_content, encoding='cp1252')\n",
        "            if 'Sentence Context' in df.columns:\n",
        "                df = df[df['Sentence Context'].astype(str).str.lower() != 'sentence context'].copy()\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"  pandas (default) failed: {e}\")\n",
        "\n",
        "        print(\"  Manual annotations not found or failed to load.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def load_datasets(self, manual_file=None, rule_based_file=None, nlp_file=None, bnc_file=None):\n",
        "        print(\"\\nLOADING DATASETS WITH FIXED ID HANDLING & EXPLICIT LABELS\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # Manual (close reading)\n",
        "        print(\"Loading manual annotations…\")\n",
        "        self.datasets['manual'] = self._load_manual_dataset_robust(manual_file) if manual_file else pd.DataFrame()\n",
        "        self.datasets['manual'] = self._ensure_ids(self.datasets['manual'], 'manual', prefix='MAN')\n",
        "        if not self.datasets['manual'].empty:\n",
        "            self.datasets['manual']['Original_Dataset'] = 'Manual_CloseReading'\n",
        "\n",
        "        # Rule-based (restrictive)\n",
        "        print(\"Loading rule-based (restrictive)…\")\n",
        "        self.datasets['rule_based'] = pd.read_csv(rule_based_file) if rule_based_file else pd.DataFrame()\n",
        "        self.datasets['rule_based'] = self._ensure_ids(self.datasets['rule_based'], 'rule_based', prefix='RST')\n",
        "        if not self.datasets['rule_based'].empty:\n",
        "            self.datasets['rule_based']['Original_Dataset'] = 'Restrictive_Dubliners'\n",
        "\n",
        "        # NLP (less-restrictive PG)\n",
        "        print(\"Loading NLP (less-restrictive PG)…\")\n",
        "        self.datasets['nlp'] = pd.read_csv(nlp_file) if nlp_file else pd.DataFrame()\n",
        "        self.datasets['nlp'] = self._ensure_ids(self.datasets['nlp'], 'nlp', prefix='NLP')\n",
        "        if not self.datasets['nlp'].empty:\n",
        "            self.datasets['nlp']['Original_Dataset'] = 'NLP_LessRestrictive_PG'\n",
        "\n",
        "        # BNC\n",
        "        print(\"Loading BNC baseline…\")\n",
        "        self.datasets['bnc'] = pd.read_csv(bnc_file, encoding='utf-8') if bnc_file else pd.DataFrame()\n",
        "        self.datasets['bnc'] = self._ensure_ids(self.datasets['bnc'], 'bnc', prefix='BNC')\n",
        "        if not self.datasets['bnc'].empty:\n",
        "            self.datasets['bnc']['Original_Dataset'] = 'BNC_Baseline'\n",
        "\n",
        "        self._standardize_datasets()\n",
        "        self._standardize_categories()\n",
        "\n",
        "        for name, df in self.datasets.items():\n",
        "            print(f\"{name:>12}: rows={len(df):4d}  \"\n",
        "                  f\"missing_IDs={df['Instance_ID'].isna().sum() if 'Instance_ID' in df else 'N/A'}  \"\n",
        "                  f\"missing_Original_Dataset={df['Original_Dataset'].isna().sum() if 'Original_Dataset' in df else 'N/A'}\")\n",
        "        print(f\"Total instances: {sum(len(df) for df in self.datasets.values())}\")\n",
        "\n",
        "    def _standardize_datasets(self):\n",
        "        print(\"Standardizing column names & adding Dataset_Source…\")\n",
        "\n",
        "        # Manual\n",
        "        df = self.datasets.get('manual', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            ren = {\n",
        "                'Category (Framwrok)': 'Category_Framework',\n",
        "                'Comparator Type ': 'Comparator_Type',\n",
        "                'Sentence Context': 'Sentence_Context',\n",
        "                'Page No.': 'Page_Number'\n",
        "            }\n",
        "            df = df.rename(columns={k: v for k, v in ren.items() if k in df.columns})\n",
        "            df['Dataset_Source'] = 'Manual_Expert_Annotation'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['manual'] = df\n",
        "        else:\n",
        "            self.datasets['manual'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context','Page_Number',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # Rule-based\n",
        "        df = self.datasets.get('rule_based', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            df = df.rename(columns={\n",
        "                'Sentence Context': 'Sentence_Context',\n",
        "                'Comparator Type ': 'Comparator_Type',\n",
        "                'Category (Framwrok)': 'Category_Framework'\n",
        "            })\n",
        "            df['Dataset_Source'] = 'Rule_Based_Domain_Informed'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['rule_based'] = df\n",
        "        else:\n",
        "            self.datasets['rule_based'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # NLP (less-restrictive)\n",
        "        df = self.datasets.get('nlp', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            if 'Sentence_Context' not in df.columns:\n",
        "                for c in ['Sentence Context','text','sentence','context','content']:\n",
        "                    if c in df.columns:\n",
        "                        df = df.rename(columns={c: 'Sentence_Context'})\n",
        "                        break\n",
        "            if 'Comparator Type ' in df.columns:\n",
        "                df = df.rename(columns={'Comparator Type ': 'Comparator_Type'})\n",
        "            if 'Category (Framwrok)' in df.columns and 'Category_Framework' not in df.columns:\n",
        "                df = df.rename(columns={'Category (Framwrok)': 'Category_Framework'})\n",
        "            if 'Category_Framework' not in df.columns:\n",
        "                df['Category_Framework'] = 'NLP_Basic_Pattern'\n",
        "            df['Dataset_Source'] = 'NLP_General_Pattern_Recognition'\n",
        "            df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['nlp'] = df\n",
        "        else:\n",
        "            self.datasets['nlp'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # BNC\n",
        "        df = self.datasets.get('bnc', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            if 'Category (Framework)' in df.columns and 'Category_Framework' not in df.columns:\n",
        "                df = df.rename(columns={'Category (Framework)':'Category_Framework'})\n",
        "            if 'Comparator Type' in df.columns and 'Comparator_Type' not in df.columns:\n",
        "                df = df.rename(columns={'Comparator Type':'Comparator_Type'})\n",
        "            if 'Sentence Context' in df.columns and 'Sentence_Context' not in df.columns:\n",
        "                df = df.rename(columns={'Sentence Context':'Sentence_Context'})\n",
        "            df['Dataset_Source'] = 'BNC_Standard_English_Baseline'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['bnc'] = df\n",
        "        else:\n",
        "            self.datasets['bnc'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Sentence_Context','Comparator_Type','Category_Framework',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        print(\"Standardization complete.\")\n",
        "\n",
        "    def _standardize_categories(self):\n",
        "        \"\"\"\n",
        "        Harmonize Category_Framework labels.\n",
        "        IMPORTANT: Merge Joycean_Quasi and its variants into Quasi_Similes (unified quasi-simile phenomenon).\n",
        "        \"\"\"\n",
        "        print(\"Harmonizing Category_Framework labels…\")\n",
        "        mapping = {\n",
        "            # Standard variants\n",
        "            'NLP_Basic': 'Standard',\n",
        "            'NLP_Basic_Pattern': 'Standard',\n",
        "            'Standard_English_Usage': 'Standard',\n",
        "            'Standard': 'Standard',\n",
        "\n",
        "            # Joycean subtypes\n",
        "            'Joycean_Framed': 'Joycean_Framed',\n",
        "            'Joycean_Silent': 'Joycean_Silent',\n",
        "            'Joycean_Quasi_Fuzzy': 'Joycean_Quasi_Fuzzy',\n",
        "            'Joycean-Quasi-Fuzzy': 'Joycean_Quasi_Fuzzy',\n",
        "\n",
        "            # >>> UNIFY all quasi-simile tags here <<<\n",
        "            'Quasi_Similes': 'Quasi_Similes',\n",
        "            'Quasi_Simile': 'Quasi_Similes',     # singular → plural canonical\n",
        "            'Joycean_Quasi': 'Quasi_Similes',    # merge with BNC tag\n",
        "            'Joycean-Quasi': 'Quasi_Similes',    # hyphen variant\n",
        "\n",
        "            # mislabels leaking from dataset names → map to Standard\n",
        "            'NLP_LessRestrictive': 'Standard',\n",
        "            'NLP_General_Pattern': 'Standard',\n",
        "            'Less-Restrictive': 'Standard',\n",
        "\n",
        "            # housekeeping\n",
        "            'Uncategorised': 'Uncategorized',\n",
        "            'nan': 'Uncategorized', 'NaN': 'Uncategorized', '': 'Uncategorized'\n",
        "        }\n",
        "        for name, df in self.datasets.items():\n",
        "            if df.empty or 'Category_Framework' not in df.columns:\n",
        "                continue\n",
        "            df['Category_Framework'] = df['Category_Framework'].astype(str).map(mapping).fillna(df['Category_Framework'])\n",
        "            self.datasets[name] = df\n",
        "        print(\"Category harmonization complete.\")\n",
        "\n",
        "    # ---------- Utilities for matching & normalization ----------\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_sentence_for_match(s: str) -> str:\n",
        "        s = (s or \"\")\n",
        "        s = s.replace(\"—\", \"-\").replace(\"–\", \"-\")\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
        "        # strip quotes; keep colon/semicolon because they matter in Joyce\n",
        "        table = str.maketrans(\"\", \"\", \"\\\"'“”‘’\")\n",
        "        s = s.translate(table)\n",
        "        return s\n",
        "\n",
        "    def _fuzzy_equal(self, a: str, b: str, threshold: float = 0.92) -> bool:\n",
        "        if not a or not b:\n",
        "            return False\n",
        "        ra = self._normalize_sentence_for_match(a)\n",
        "        rb = self._normalize_sentence_for_match(b)\n",
        "        if ra == rb:\n",
        "            return True\n",
        "        return SequenceMatcher(None, ra, rb).ratio() >= threshold\n",
        "\n",
        "    # ---------- Linguistic analysis (spaCy/TextBlob; corrected comparator handling) ----------\n",
        "\n",
        "    def _find_comparator_span(self, doc, comparator_type):\n",
        "        \"\"\"\n",
        "        Return (start_i, end_i) in doc-token indices for the comparator span, inclusive.\n",
        "        Handles 'like', 'as if', 'as though', 'as … as', 'seem*', 'resembl*', punctuation comparators.\n",
        "        Returns None if not found.\n",
        "        \"\"\"\n",
        "        comp = (str(comparator_type) or \"\").strip().lower()\n",
        "\n",
        "        def idx_seq_match(tokens, start, seq):\n",
        "            n = len(seq)\n",
        "            if start + n > len(tokens):\n",
        "                return False\n",
        "            for k in range(n):\n",
        "                if tokens[start + k].text.lower() != seq[k]:\n",
        "                    return False\n",
        "            return True\n",
        "\n",
        "        tokens = list(doc)\n",
        "\n",
        "        # Multiword comparators\n",
        "        if comp in {\"as if\", \"as-if\"}:\n",
        "            seq = [\"as\", \"if\"]\n",
        "            for i in range(len(tokens)-1):\n",
        "                if idx_seq_match(tokens, i, seq):\n",
        "                    return (i, i+1)\n",
        "\n",
        "        if comp in {\"as though\", \"as-though\"}:\n",
        "            seq = [\"as\", \"though\"]\n",
        "            for i in range(len(tokens)-1):\n",
        "                if idx_seq_match(tokens, i, seq):\n",
        "                    return (i, i+1)\n",
        "\n",
        "        # 'as … as' construction (if comparator supplied as 'as')\n",
        "        if comp == \"as\":\n",
        "            as_positions = [i for i,t in enumerate(tokens) if t.text.lower() == \"as\"]\n",
        "            for i in as_positions:\n",
        "                for j in as_positions:\n",
        "                    if j > i and (j - i) <= 8:  # window limit\n",
        "                        return (i, j)\n",
        "            if as_positions:\n",
        "                return (as_positions[0], as_positions[0])\n",
        "\n",
        "        # Single-word comparators\n",
        "        if comp == \"like\":\n",
        "            for i,t in enumerate(tokens):\n",
        "                if t.text.lower() == \"like\":\n",
        "                    return (i, i)\n",
        "\n",
        "        # Lemma families\n",
        "        if comp.startswith(\"resembl\"):\n",
        "            for i,t in enumerate(tokens):\n",
        "                if getattr(t, \"lemma_\", t.text).lower().startswith(\"resembl\"):\n",
        "                    return (i, i)\n",
        "\n",
        "        if comp.startswith(\"seem\"):\n",
        "            for i,t in enumerate(tokens):\n",
        "                if getattr(t, \"lemma_\", t.text).lower().startswith(\"seem\"):\n",
        "                    return (i, i)\n",
        "\n",
        "        # punctuation comparators\n",
        "        if comp in {\"colon\", \":\"}:\n",
        "            for i,t in enumerate(tokens):\n",
        "                if t.text == \":\":\n",
        "                    return (i, i)\n",
        "        if comp in {\"semicolon\", \";\"}:\n",
        "            for i,t in enumerate(tokens):\n",
        "                if t.text == \";\":\n",
        "                    return (i, i)\n",
        "        if comp in {\"ellipsis\", \"...\", \"…\"}:\n",
        "            for i,t in enumerate(tokens):\n",
        "                if t.text in {\"...\", \"…\"}:\n",
        "                    return (i, i)\n",
        "        if comp in {\"en dash\", \"–\", \"—\", \"-\"}:\n",
        "            for i,t in enumerate(tokens):\n",
        "                if t.text in {\"—\", \"–\", \"-\"}:\n",
        "                    return (i, i)\n",
        "\n",
        "        # last resort: exact token string match\n",
        "        for i,t in enumerate(tokens):\n",
        "            if t.text.lower() == comp:\n",
        "                return (i, i)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _pre_post_counts_from_span(self, doc, span):\n",
        "        \"\"\"\n",
        "        Compute pre/post token counts using the same token filtering used for 'total'.\n",
        "        Ensures indices are comparable.\n",
        "        \"\"\"\n",
        "        if span is None:\n",
        "            nonpun = [t for t in doc if not t.is_space and not t.is_punct]\n",
        "            total = len(nonpun)\n",
        "            pre = total // 2\n",
        "            post = total - pre\n",
        "            ratio = (pre / post) if post > 0 else np.nan\n",
        "            return total, pre, post, ratio\n",
        "\n",
        "        start_i, end_i = span\n",
        "        nonpun = []\n",
        "        doc_to_nonpun_idx = {}\n",
        "        for idx, t in enumerate(doc):\n",
        "            if not t.is_space and not t.is_punct:\n",
        "                doc_to_nonpun_idx[idx] = len(nonpun)\n",
        "                nonpun.append(t)\n",
        "\n",
        "        total = len(nonpun)\n",
        "\n",
        "        def nearest_kept(i):\n",
        "            if i in doc_to_nonpun_idx:\n",
        "                return doc_to_nonpun_idx[i]\n",
        "            L = i - 1\n",
        "            R = i + 1\n",
        "            while L >= 0 or R < len(doc):\n",
        "                if L >= 0 and L in doc_to_nonpun_idx:\n",
        "                    return doc_to_nonpun_idx[L]\n",
        "                if R < len(doc) and R in doc_to_nonpun_idx:\n",
        "                    return doc_to_nonpun_idx[R]\n",
        "                L -= 1\n",
        "                R += 1\n",
        "            return 0\n",
        "\n",
        "        start_np = nearest_kept(start_i)\n",
        "        end_np = nearest_kept(end_i)\n",
        "\n",
        "        pre = start_np\n",
        "        post = max(total - (end_np + 1), 0)\n",
        "        ratio = (pre / post) if post > 0 else np.nan\n",
        "        return total, pre, post, ratio\n",
        "\n",
        "    def _analyze_comparative_structure(self, doc, comparator_type):\n",
        "        structure = {\n",
        "            'has_explicit_comparator': False,\n",
        "            'comparator_type': str(comparator_type).strip() or \"Unknown\",\n",
        "            'comparative_adjectives': [],\n",
        "            'superlative_adjectives': [],\n",
        "            'modal_verbs': [],\n",
        "            'epistemic_markers': []\n",
        "        }\n",
        "        for token in doc:\n",
        "            if token.text.lower() in ['like','as','than']:\n",
        "                structure['has_explicit_comparator'] = True\n",
        "            if token.tag_ in ['JJR','RBR']:\n",
        "                structure['comparative_adjectives'].append(token.text)\n",
        "            elif token.tag_ in ['JJS','RBS']:\n",
        "                structure['superlative_adjectives'].append(token.text)\n",
        "            if token.pos_ == 'AUX' and token.text.lower() in ['might','could','would','should','may']:\n",
        "                structure['modal_verbs'].append(token.text)\n",
        "            if token.text.lower() in ['perhaps','maybe','possibly','apparently','seemingly']:\n",
        "                structure['epistemic_markers'].append(token.text)\n",
        "        return structure\n",
        "\n",
        "    def _calculate_syntactic_complexity(self, doc):\n",
        "        def depth(tok, d=0):\n",
        "            if not list(tok.children):\n",
        "                return d\n",
        "            return max(depth(ch, d+1) for ch in tok.children)\n",
        "        roots = [t for t in doc if t.head == t]\n",
        "        if not roots:\n",
        "            return 0\n",
        "        try:\n",
        "            return max(depth(r) for r in roots)\n",
        "        except Exception:\n",
        "            return np.nan\n",
        "\n",
        "    def perform_comprehensive_linguistic_analysis(self):\n",
        "        print(\"\\nPERFORMING LINGUISTIC ANALYSIS\")\n",
        "        print(\"-\" * 35)\n",
        "        if self.nlp is None:\n",
        "            print(\"spaCy unavailable → simplified analysis (token counts + TextBlob sentiment).\")\n",
        "            return self._perform_simplified_analysis()\n",
        "\n",
        "        for name, df in list(self.datasets.items()):\n",
        "            if df.empty:\n",
        "                print(f\"Skipping empty dataset: {name}\")\n",
        "                continue\n",
        "\n",
        "            # Initialize feature containers\n",
        "            n = len(df)\n",
        "            feats = {\n",
        "                'Total_Tokens': [None]*n,\n",
        "                'Pre_Comparator_Tokens': [None]*n,\n",
        "                'Post_Comparator_Tokens': [None]*n,\n",
        "                'Pre_Post_Ratio': [None]*n,\n",
        "                'Lemmatized_Text': [None]*n,\n",
        "                'POS_Tags': [None]*n,\n",
        "                'POS_Distribution': [None]*n,\n",
        "                'Sentiment_Polarity': [None]*n,\n",
        "                'Sentiment_Subjectivity': [None]*n,\n",
        "                'Comparative_Structure': [None]*n,\n",
        "                'Syntactic_Complexity': [None]*n,\n",
        "                'Sentence_Length': [None]*n,\n",
        "                'Adjective_Count': [None]*n,\n",
        "                'Verb_Count': [None]*n,\n",
        "                'Noun_Count': [None]*n,\n",
        "                'Figurative_Density': [None]*n\n",
        "            }\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                sent = str(row.get('Sentence_Context', '') or '').strip()\n",
        "                comp = row.get('Comparator_Type', '')\n",
        "                if not sent:\n",
        "                    continue\n",
        "                try:\n",
        "                    doc = self.nlp(sent)\n",
        "\n",
        "                    # Comparator span & pre/post\n",
        "                    span = self._find_comparator_span(doc, comp)\n",
        "                    total, pre, post, ratio = self._pre_post_counts_from_span(doc, span)\n",
        "\n",
        "                    # Lemmas & POS (exclude spaces/punct for most features)\n",
        "                    tokens_nopunct = [t for t in doc if not t.is_space and not t.is_punct]\n",
        "                    lemmas = [t.lemma_.lower() for t in tokens_nopunct if not t.is_stop]\n",
        "                    pos_tags = [t.pos_ for t in tokens_nopunct]  # no punctuation\n",
        "                    pos_dist = Counter(pos_tags)\n",
        "\n",
        "                    # Sentiment (exploratory only)\n",
        "                    pol = subj = np.nan\n",
        "                    if TextBlob is not None:\n",
        "                        try:\n",
        "                            blob = TextBlob(sent)\n",
        "                            pol, subj = blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                    comp_struct = self._analyze_comparative_structure(doc, comp)\n",
        "                    complexity = self._calculate_syntactic_complexity(doc)\n",
        "                    slen = len(tokens_nopunct)\n",
        "                    adj = sum(1 for t in tokens_nopunct if t.pos_ == 'ADJ')\n",
        "                    vrb = sum(1 for t in tokens_nopunct if t.pos_ == 'VERB')\n",
        "                    nou = sum(1 for t in tokens_nopunct if t.pos_ == 'NOUN')\n",
        "\n",
        "                    figurative_markers = {'like','as','such','seem','appear','resemble','as if','as though'}\n",
        "                    # token-level density (multiword markers counted by token hits)\n",
        "                    fdens = (sum(1 for t in tokens_nopunct if t.text.lower() in figurative_markers) / total) if total else 0\n",
        "\n",
        "                    loc = df.index.get_loc(idx)\n",
        "                    feats['Total_Tokens'][loc] = total\n",
        "                    feats['Pre_Comparator_Tokens'][loc] = pre\n",
        "                    feats['Post_Comparator_Tokens'][loc] = post\n",
        "                    feats['Pre_Post_Ratio'][loc] = ratio\n",
        "                    feats['Lemmatized_Text'][loc] = ' '.join(lemmas)\n",
        "                    feats['POS_Tags'][loc] = '; '.join(pos_tags)\n",
        "                    feats['POS_Distribution'][loc] = dict(pos_dist)\n",
        "                    feats['Sentiment_Polarity'][loc] = pol\n",
        "                    feats['Sentiment_Subjectivity'][loc] = subj\n",
        "                    feats['Comparative_Structure'][loc] = comp_struct\n",
        "                    feats['Syntactic_Complexity'][loc] = complexity\n",
        "                    feats['Sentence_Length'][loc] = slen\n",
        "                    feats['Adjective_Count'][loc] = adj\n",
        "                    feats['Verb_Count'][loc] = vrb\n",
        "                    feats['Noun_Count'][loc] = nou\n",
        "                    feats['Figurative_Density'][loc] = fdens\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error in {name} row {idx}: {e}\")\n",
        "\n",
        "            # Serialize complex columns for CSV\n",
        "            df['POS_Distribution'] = [json.dumps(x) if isinstance(x, dict) else None for x in feats['POS_Distribution']]\n",
        "            df['Comparative_Structure'] = [json.dumps(x) if isinstance(x, dict) else None for x in feats['Comparative_Structure']]\n",
        "            for k, v in feats.items():\n",
        "                if k in ['POS_Distribution','Comparative_Structure']:\n",
        "                    continue\n",
        "                df[k] = v\n",
        "\n",
        "            self.linguistic_features[name] = feats\n",
        "            self.datasets[name] = df\n",
        "            print(f\"Finished linguistic analysis for {name}.\")\n",
        "\n",
        "        print(\"All datasets processed.\")\n",
        "\n",
        "    def _perform_simplified_analysis(self):\n",
        "        for name, df in list(self.datasets.items()):\n",
        "            if df.empty or 'Sentence_Context' not in df.columns:\n",
        "                continue\n",
        "            n = len(df)\n",
        "            df['Total_Tokens'] = [None]*n\n",
        "            df['Pre_Comparator_Tokens'] = [None]*n\n",
        "            df['Post_Comparator_Tokens'] = [None]*n\n",
        "            df['Pre_Post_Ratio'] = [np.nan]*n\n",
        "            df['Sentiment_Polarity'] = [np.nan]*n\n",
        "            df['Sentiment_Subjectivity'] = [np.nan]*n\n",
        "            df['Sentence_Length'] = [None]*n\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                sent = str(row.get('Sentence_Context','') or '').strip()\n",
        "                if not sent:\n",
        "                    continue\n",
        "                tokens = [t for t in sent.split(\" \") if t]\n",
        "                total = len(tokens)\n",
        "                df.loc[idx, 'Total_Tokens'] = total\n",
        "                df.loc[idx, 'Sentence_Length'] = total\n",
        "                if TextBlob is not None:\n",
        "                    try:\n",
        "                        blob = TextBlob(sent)\n",
        "                        df.loc[idx, 'Sentiment_Polarity'] = blob.sentiment.polarity\n",
        "                        df.loc[idx, 'Sentiment_Subjectivity'] = blob.sentiment.subjectivity\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                comp = row.get('Comparator_Type','')\n",
        "                pos = -1\n",
        "                if str(comp).strip():\n",
        "                    try:\n",
        "                        m = re.search(r'\\b' + re.escape(str(comp).strip()) + r'\\b', sent, re.IGNORECASE)\n",
        "                        if m:\n",
        "                            pre_text = sent[:m.start()]\n",
        "                            pos = len([t for t in pre_text.split(\" \") if t])\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                if total > 0 and pos != -1:\n",
        "                    pre, post = pos, total - pos - 1\n",
        "                    df.loc[idx, 'Pre_Comparator_Tokens'] = pre\n",
        "                    df.loc[idx, 'Post_Comparator_Tokens'] = post\n",
        "                    df.loc[idx, 'Pre_Post_Ratio'] = (pre / post) if post > 0 else np.nan\n",
        "            self.datasets[name] = df\n",
        "        print(\"Simplified analysis complete.\")\n",
        "\n",
        "    # ---------- Instance-aligned F1 metrics ----------\n",
        "\n",
        "    def _pair_rows_by_sentence(self, gold_df, pred_df, fuzzy_threshold=0.92):\n",
        "        \"\"\"\n",
        "        Returns list of matched pairs [(gold_idx, pred_idx)] using exact match first,\n",
        "        then fuzzy matching without reuse of already matched rows.\n",
        "        \"\"\"\n",
        "        if gold_df.empty or pred_df.empty:\n",
        "            return [], gold_df, pred_df\n",
        "\n",
        "        gold_df = gold_df.copy()\n",
        "        pred_df = pred_df.copy()\n",
        "        gold_df[\"_norm_sent\"] = gold_df[\"Sentence_Context\"].map(self._normalize_sentence_for_match)\n",
        "        pred_df[\"_norm_sent\"] = pred_df[\"Sentence_Context\"].map(self._normalize_sentence_for_match)\n",
        "\n",
        "        exact_pairs = []\n",
        "        used_pred = set()\n",
        "        pred_lookup = {}\n",
        "        for j, s in pred_df[\"_norm_sent\"].items():\n",
        "            pred_lookup.setdefault(s, []).append(j)\n",
        "\n",
        "        for i, s in gold_df[\"_norm_sent\"].items():\n",
        "            if s in pred_lookup:\n",
        "                js = [jj for jj in pred_lookup[s] if jj not in used_pred]\n",
        "                if js:\n",
        "                    j = js[0]\n",
        "                    used_pred.add(j)\n",
        "                    exact_pairs.append((i, j))\n",
        "\n",
        "        unmatched_gold = [i for i in gold_df.index if i not in {gi for gi,_ in exact_pairs}]\n",
        "        unmatched_pred = [j for j in pred_df.index if j not in used_pred]\n",
        "\n",
        "        fuzzy_pairs = []\n",
        "        for i in unmatched_gold:\n",
        "            best_j = None\n",
        "            best_r = 0.0\n",
        "            gi = gold_df.at[i, \"_norm_sent\"]\n",
        "            for j in unmatched_pred:\n",
        "                r = SequenceMatcher(None, gi, pred_df.at[j, \"_norm_sent\"]).ratio()\n",
        "                if r > best_r:\n",
        "                    best_r = r\n",
        "                    best_j = j\n",
        "            if best_j is not None and best_r >= fuzzy_threshold:\n",
        "                used_pred.add(best_j)\n",
        "                fuzzy_pairs.append((i, best_j))\n",
        "\n",
        "        pairs = exact_pairs + fuzzy_pairs\n",
        "        return pairs, gold_df, pred_df\n",
        "\n",
        "    def _compute_f1_from_pairs(self, gold_df, pred_df, pairs, category_col=\"Category_Framework\"):\n",
        "        \"\"\"\n",
        "        Build TP/FP/FN per category from aligned pairs.\n",
        "        A predicted row is a TP for category c if both gold and pred label == c.\n",
        "        If labels differ, count FP for pred's category and FN for gold's category.\n",
        "        Unmatched gold rows are FNs; unmatched pred rows are FPs.\n",
        "        \"\"\"\n",
        "        cats = sorted(set(gold_df[category_col].astype(str)) | set(pred_df[category_col].astype(str)))\n",
        "        TP = {c:0 for c in cats}\n",
        "        FP = {c:0 for c in cats}\n",
        "        FN = {c:0 for c in cats}\n",
        "\n",
        "        matched_gold = set(i for i,_ in pairs)\n",
        "        matched_pred = set(j for _,j in pairs)\n",
        "\n",
        "        for i,j in pairs:\n",
        "            g = str(gold_df.at[i, category_col])\n",
        "            p = str(pred_df.at[j, category_col])\n",
        "            if p == g:\n",
        "                TP[g] += 1\n",
        "            else:\n",
        "                FP[p] += 1\n",
        "                FN[g] += 1\n",
        "\n",
        "        for i in gold_df.index:\n",
        "            if i not in matched_gold:\n",
        "                g = str(gold_df.at[i, category_col])\n",
        "                FN[g] += 1\n",
        "\n",
        "        for j in pred_df.index:\n",
        "            if j not in matched_pred:\n",
        "                p = str(pred_df.at[j, category_col])\n",
        "                FP[p] += 1\n",
        "\n",
        "        metrics = {}\n",
        "        micro_tp = micro_fp = micro_fn = 0\n",
        "\n",
        "        for c in cats:\n",
        "            tp, fp, fn = TP[c], FP[c], FN[c]\n",
        "            prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "            f1   = (2*prec*rec)/(prec+rec) if (prec+rec) > 0 else 0.0\n",
        "            metrics[c] = {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
        "            micro_tp += tp; micro_fp += fp; micro_fn += fn\n",
        "\n",
        "        micro_prec = micro_tp / (micro_tp + micro_fp) if (micro_tp + micro_fp) > 0 else 0.0\n",
        "        micro_rec  = micro_tp / (micro_tp + micro_fn) if (micro_tp + micro_fn) > 0 else 0.0\n",
        "        micro_f1   = (2*micro_prec*micro_rec)/(micro_prec+micro_rec) if (micro_prec+micro_rec) > 0 else 0.0\n",
        "        macro_f1   = np.mean([m[\"f1\"] for m in metrics.values()]) if metrics else 0.0\n",
        "\n",
        "        return metrics, {\"micro_precision\": micro_prec, \"micro_recall\": micro_rec, \"micro_f1\": micro_f1, \"macro_f1\": macro_f1}\n",
        "\n",
        "    def calculate_corrected_f1_scores(self, fuzzy_threshold=0.92):\n",
        "        print(\"\\nCALCULATING INSTANCE-ALIGNED F1 METRICS\")\n",
        "        print(\"-\" * 44)\n",
        "        print(f\"Assumptions: sentence-level alignment (exact, then fuzzy ≥ {fuzzy_threshold}); category = 'Category_Framework'.\")\n",
        "\n",
        "        manual_df = self.datasets.get('manual', pd.DataFrame())\n",
        "        if manual_df.empty or 'Sentence_Context' not in manual_df or 'Category_Framework' not in manual_df:\n",
        "            print(\"F1 unavailable: manual annotations missing/invalid.\")\n",
        "            self.comparison_results['f1_analysis'] = None\n",
        "            return None, None\n",
        "\n",
        "        out = {}\n",
        "\n",
        "        def eval_one(pred_df, pred_name):\n",
        "            if pred_df.empty or 'Sentence_Context' not in pred_df or 'Category_Framework' not in pred_df:\n",
        "                print(f\"{pred_name}: dataset missing required columns.\")\n",
        "                return None\n",
        "            pairs, gdf, pdf = self._pair_rows_by_sentence(manual_df, pred_df, fuzzy_threshold=fuzzy_threshold)\n",
        "            metrics, overall = self._compute_f1_from_pairs(gdf, pdf, pairs)\n",
        "            print(f\"{pred_name}: pairs={len(pairs)}  micro-F1={overall['micro_f1']:.3f}  macro-F1={overall['macro_f1']:.3f}\")\n",
        "            return {\"pairs\": len(pairs), \"category_metrics\": metrics, \"overall\": overall}\n",
        "\n",
        "        rb = self.datasets.get('rule_based', pd.DataFrame())\n",
        "        nl = self.datasets.get('nlp', pd.DataFrame())\n",
        "\n",
        "        out['rule_based_vs_manual'] = eval_one(rb, \"Rule-Based vs Manual\")\n",
        "        out['nlp_vs_manual'] = eval_one(nl, \"NLP vs Manual\")\n",
        "\n",
        "        self.comparison_results['f1_analysis'] = out\n",
        "        primary = out['rule_based_vs_manual']['overall']['micro_f1'] if out.get('rule_based_vs_manual') else None\n",
        "        return out, primary\n",
        "\n",
        "    # ---------- Save / Export ----------\n",
        "\n",
        "    def save_comprehensive_results(self, output_path=\"comprehensive_linguistic_analysis_corrected.csv\"):\n",
        "        print(\"\\nSAVING COMPREHENSIVE RESULTS …\")\n",
        "        frames = []\n",
        "        for name, df in self.datasets.items():\n",
        "            if df is None or df.empty:\n",
        "                continue\n",
        "            d = df.copy()\n",
        "            for col, default in [\n",
        "                ('Original_Dataset', name),\n",
        "                ('Instance_ID', None),\n",
        "                ('Sentence_Context', None),\n",
        "                ('Category_Framework', None),\n",
        "                ('Comparator_Type', None)\n",
        "            ]:\n",
        "                if col not in d.columns:\n",
        "                    d[col] = default\n",
        "\n",
        "            if d['Instance_ID'].isna().any() or (not d['Instance_ID'].astype(str).is_unique):\n",
        "                d = self._ensure_ids(d, name)\n",
        "\n",
        "            base = ['Instance_ID','Original_Dataset','Sentence_Context','Category_Framework','Comparator_Type']\n",
        "            others = [c for c in d.columns if c not in base]\n",
        "            d = d[base + others]\n",
        "            frames.append(d)\n",
        "\n",
        "        if not frames:\n",
        "            print(\"No data to save.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        combined = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "        # Stable sort: Manual → Restrictive → Less-Restrictive PG → BNC\n",
        "        order = {\n",
        "            'Manual_CloseReading': 1,\n",
        "            'Restrictive_Dubliners': 2,\n",
        "            'NLP_LessRestrictive_PG': 3,\n",
        "            'BNC_Baseline': 4\n",
        "        }\n",
        "        combined['__order__'] = combined['Original_Dataset'].map(order).fillna(99).astype(int)\n",
        "\n",
        "        def _id_numeric_tail(x):\n",
        "            m = re.search(r'(\\d+)$', str(x))\n",
        "            return int(m.group(1)) if m else 0\n",
        "\n",
        "        combined = combined.sort_values(\n",
        "            by=['__order__','Original_Dataset','Instance_ID'],\n",
        "            key=lambda s: s.map(_id_numeric_tail) if s.name == 'Instance_ID' else s\n",
        "        ).drop(columns='__order__')\n",
        "\n",
        "        combined.to_csv(output_path, index=False)\n",
        "        print(f\"Saved: {output_path}\")\n",
        "        print(\"Integrity:\",\n",
        "              \"missing Instance_ID =\", combined['Instance_ID'].isna().sum(),\n",
        "              \"| missing Original_Dataset =\", combined['Original_Dataset'].isna().sum(),\n",
        "              \"| rows =\", len(combined))\n",
        "        print(\"Environment (for reproducibility):\", self.env_info)\n",
        "        return combined\n",
        "\n",
        "\n",
        "# ========= RUN THE PIPELINE (with your filenames) =========\n",
        "# manual_path = \"All Similes - Dubliners cont.csv\"           # close reading (manual)\n",
        "# rule_based_path = \"dubliners_corrected_extraction.csv\"     # restrictive\n",
        "# nlp_path = \"dubliners_nlp_basic_extraction.csv\"            # less-restrictive PG Dubliners\n",
        "# bnc_processed_path = \"bnc_processed_similes.csv\"           # BNC baseline\n",
        "\n",
        "# Example of how to use with uploaded files:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# manual_file_content = uploaded['All Similes - Dubliners cont.csv'] if 'All Similes - Dubliners cont.csv' in uploaded else None\n",
        "# rule_based_file_content = uploaded['dubliners_corrected_extraction.csv'] if 'dubliners_corrected_extraction.csv' in uploaded else None\n",
        "# nlp_file_content = uploaded['dubliners_nlp_basic_extraction.csv'] if 'dubliners_nlp_basic_extraction.csv' in uploaded else None\n",
        "# bnc_file_content = uploaded['bnc_processed_similes.csv'] if 'bnc_processed_similes.csv' in uploaded else None\n",
        "\n",
        "# Temporarily using existing files for demonstration\n",
        "manual_file_content = \"/content/All Similes - Dubliners cont.csv\"\n",
        "rule_based_file_content = \"/content/dubliners_rulebased_extraction.csv\"\n",
        "nlp_file_content = \"/content/dubliners_nlp_less_restrictive_extraction.csv\"\n",
        "bnc_file_content = \"/content/bnc_processed_similes.csv\"\n",
        "\n",
        "comparator = ComprehensiveLinguisticComparator()\n",
        "comparator.load_datasets(manual_file_content, rule_based_file_content, nlp_file_content, bnc_file_content)\n",
        "comparator.perform_comprehensive_linguistic_analysis()\n",
        "f1_analysis, primary_f1 = comparator.calculate_corrected_f1_scores()  # instance-aligned F1\n",
        "results_df = comparator.save_comprehensive_results(\"comprehensive_linguistic_analysis_corrected.csv\")\n",
        "\n",
        "print(\"\\nPIPELINE COMPLETED.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn9hj4I8zAEU",
        "outputId": "ab376355-dc39-44cd-d5a7-482288fb1cc1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS (FIXED)\n",
            "===========================================================================\n",
            "Dataset 1: Manual Annotations (Ground Truth - Close Reading)\n",
            "Dataset 2: Rule-Based Extraction (Restrictive - Domain-Informed)\n",
            "Dataset 3: NLP Extraction (Less-Restrictive - PG Dubliners)\n",
            "Dataset 4: BNC Baseline Corpus (Standard English Reference)\n",
            "===========================================================================\n",
            "spaCy pipeline loaded: en_core_web_sm\n",
            "Environment: {'python': '3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]', 'pandas': '2.2.2', 'numpy': '2.0.2', 'spacy': '3.8.7', 'textblob': 'n/a'}\n",
            "\n",
            "LOADING DATASETS WITH FIXED ID HANDLING & EXPLICIT LABELS\n",
            "----------------------------------------------------------------------\n",
            "Loading manual annotations…\n",
            "Loading rule-based (restrictive)…\n",
            "Loading NLP (less-restrictive PG)…\n",
            "Loading BNC baseline…\n",
            "Standardizing column names & adding Dataset_Source…\n",
            "Standardization complete.\n",
            "Harmonizing Category_Framework labels…\n",
            "Category harmonization complete.\n",
            "      manual: rows= 184  missing_IDs=0  missing_Original_Dataset=0\n",
            "  rule_based: rows= 218  missing_IDs=0  missing_Original_Dataset=0\n",
            "         nlp: rows= 330  missing_IDs=0  missing_Original_Dataset=0\n",
            "         bnc: rows= 200  missing_IDs=0  missing_Original_Dataset=0\n",
            "Total instances: 932\n",
            "\n",
            "PERFORMING LINGUISTIC ANALYSIS\n",
            "-----------------------------------\n",
            "Finished linguistic analysis for manual.\n",
            "Finished linguistic analysis for rule_based.\n",
            "Finished linguistic analysis for nlp.\n",
            "Finished linguistic analysis for bnc.\n",
            "All datasets processed.\n",
            "\n",
            "CALCULATING INSTANCE-ALIGNED F1 METRICS\n",
            "--------------------------------------------\n",
            "Assumptions: sentence-level alignment (exact, then fuzzy ≥ 0.92); category = 'Category_Framework'.\n",
            "Rule-Based vs Manual: pairs=110  micro-F1=0.343  macro-F1=0.178\n",
            "NLP vs Manual: pairs=127  micro-F1=0.292  macro-F1=0.059\n",
            "\n",
            "SAVING COMPREHENSIVE RESULTS …\n",
            "Saved: comprehensive_linguistic_analysis_corrected.csv\n",
            "Integrity: missing Instance_ID = 0 | missing Original_Dataset = 0 | rows = 932\n",
            "Environment (for reproducibility): {'python': '3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]', 'pandas': '2.2.2', 'numpy': '2.0.2', 'spacy': '3.8.7', 'textblob': 'n/a'}\n",
            "\n",
            "PIPELINE COMPLETED.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# MULTI-COMPARATOR SEGMENT METRICS (pre / between / post) + PATTERN FLAGS\n",
        "# Between_Tokens_List exported as \"10, 10\" (or \"0\" if none); raw list preserved.\n",
        "# =============================================================================\n",
        "\n",
        "import os, re, json, time, ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    _HAS_SPACY = True if 'nlp' in globals() and nlp is not None else False\n",
        "except Exception:\n",
        "    _HAS_SPACY = False\n",
        "\n",
        "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "out_dir = \"analysis_outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "def _pick_df_for_segments():\n",
        "    for name in (\"GLOBAL_DF\",\"df_vis\",\"results_df\",\"df\"):\n",
        "        if name in globals() and isinstance(globals()[name], pd.DataFrame) and not globals()[name].empty:\n",
        "            print(f\"Using in-memory dataset: {name} ({len(globals()[name])} rows)\")\n",
        "            return globals()[name].copy()\n",
        "    for p in [\n",
        "        \"comprehensive_linguistic_analysis_enriched.csv\",\n",
        "        \"comprehensive_linguistic_analysis_corrected.csv\",\n",
        "        \"comprehensive_linguistic_analysis.csv\",\n",
        "        \"/content/comprehensive_linguistic_analysis_enriched.csv\",\n",
        "        \"/content/comprehensive_linguistic_analysis_corrected.csv\",\n",
        "        \"/content/comprehensive_linguistic_analysis.csv\",\n",
        "    ]:\n",
        "        if os.path.exists(p):\n",
        "            print(f\"Loaded dataset from disk: {p}\")\n",
        "            return pd.read_csv(p)\n",
        "    raise FileNotFoundError(\"No comprehensive dataset found in memory or standard paths.\")\n",
        "\n",
        "_SUBORDINATORS = {\"that\",\"who\",\"which\",\"when\",\"because\",\"though\",\"although\",\"if\",\"as\",\n",
        "                  \"while\",\"since\",\"unless\",\"until\",\"where\",\"whereas\"}\n",
        "\n",
        "def _simple_tokens(s):\n",
        "    s = re.sub(r\"[“”‘’\\\"']\", \"\", str(s))\n",
        "    s = s.replace(\"—\",\"-\").replace(\"–\",\"-\")\n",
        "    return re.findall(r\"\\w+|[:;,\\.\\-\\(\\)…]\", s)\n",
        "\n",
        "def _detect_comparator_spans(doc_tokens, use_spacy=False, doc_obj=None):\n",
        "    spans = []\n",
        "    n = len(doc_tokens)\n",
        "    tt = (lambda i: (doc_tokens[i].text if use_spacy else doc_tokens[i]))\n",
        "    lm = (lambda i: (getattr(doc_tokens[i], \"lemma_\", tt(i)).lower() if use_spacy else tt(i).lower()))\n",
        "    lower = [tt(i).lower() for i in range(n)]\n",
        "    lemmas = [lm(i) for i in range(n)]\n",
        "\n",
        "    used = set()\n",
        "    i = 0\n",
        "    while i < n-1:\n",
        "        if lower[i] == \"as\" and lower[i+1] == \"if\":\n",
        "            spans.append((i, i+1, \"as_if\")); used.update((i,i+1)); i += 2; continue\n",
        "        if lower[i] == \"as\" and lower[i+1] == \"though\":\n",
        "            spans.append((i, i+1, \"as_though\")); used.update((i,i+1)); i += 2; continue\n",
        "        i += 1\n",
        "\n",
        "    as_positions = [i for i,w in enumerate(lower) if w == \"as\" and i not in used]\n",
        "    consumed = set()\n",
        "    for i in as_positions:\n",
        "        if i in consumed:\n",
        "            continue\n",
        "        partner = None\n",
        "        for j in as_positions:\n",
        "            if j > i and (j - i) <= 8 and j not in consumed:\n",
        "                partner = j; break\n",
        "        if partner is not None:\n",
        "            spans.append((i, partner, \"as_as\"))\n",
        "            consumed.update({i, partner})\n",
        "    used.update(consumed)\n",
        "\n",
        "    for i,w in enumerate(lower):\n",
        "        if w == \"like\":\n",
        "            spans.append((i,i,\"like\"))\n",
        "\n",
        "    for i,l in enumerate(lemmas):\n",
        "        if l.startswith(\"resembl\") or l.startswith(\"seem\") or l.startswith(\"appear\"):\n",
        "            spans.append((i,i,l.split(\"-\")[0] if \"-\" in l else l))\n",
        "\n",
        "    for i,w in enumerate([tt(k) for k in range(n)]):\n",
        "        if w in {\":\",\";\",\"—\",\"–\",\"-\",\"...\",\"…\"}:\n",
        "            spans.append((i,i,f\"punc_{w}\"))\n",
        "\n",
        "    spans = sorted(spans, key=lambda x: (x[0], x[1]))\n",
        "    coalesced, last = [], None\n",
        "    for s in spans:\n",
        "        if last is None:\n",
        "            last = s\n",
        "        else:\n",
        "            if s[0] <= last[1] and s[1] >= last[0]:\n",
        "                last = (min(last[0], s[0]), max(last[1], s[1]), last[2])\n",
        "            else:\n",
        "                coalesced.append(last); last = s\n",
        "    if last is not None:\n",
        "        coalesced.append(last)\n",
        "    return coalesced\n",
        "\n",
        "def _segments_from_spans(doc_tokens, spans, use_spacy=False):\n",
        "    content_idx, doc_to_content = [], {}\n",
        "    for i, tok in enumerate(doc_tokens):\n",
        "        if use_spacy:\n",
        "            if getattr(tok, \"is_space\", False) or getattr(tok, \"is_punct\", False): continue\n",
        "        else:\n",
        "            if re.fullmatch(r\"[:;,.\\-\\(\\)…]\", str(tok)): continue\n",
        "        doc_to_content[i] = len(content_idx)\n",
        "        content_idx.append(i)\n",
        "\n",
        "    def nearest(i):\n",
        "        if i in doc_to_content: return doc_to_content[i]\n",
        "        L, R = i-1, i+1\n",
        "        while L >= 0 or R < len(doc_tokens):\n",
        "            if L >= 0 and L in doc_to_content: return doc_to_content[L]\n",
        "            if R < len(doc_tokens) and R in doc_to_content: return doc_to_content[R]\n",
        "            L -= 1; R += 1\n",
        "        return 0\n",
        "\n",
        "    spans_np = []\n",
        "    for a,b,_ in spans:\n",
        "        a_np, b_np = nearest(a), nearest(b)\n",
        "        spans_np.append((min(a_np,b_np), max(a_np,b_np)))\n",
        "    spans_np = sorted(spans_np)\n",
        "\n",
        "    total = len(content_idx)\n",
        "    if total == 0:\n",
        "        return {\"Pre_Tokens\":0,\"Post_Tokens\":0,\"Between_Tokens_List\":[],\n",
        "                \"Between_Tokens_Total\":0,\"Between_Segments\":0,\"Between_Max\":0,\n",
        "                \"Between_Mean\":0.0,\"Between_Has_Verb\":False,\"Between_Has_Subordinator\":False}\n",
        "\n",
        "    if not spans_np:\n",
        "        pre = total // 2; post = total - pre\n",
        "        return {\"Pre_Tokens\":pre,\"Post_Tokens\":post,\"Between_Tokens_List\":[],\n",
        "                \"Between_Tokens_Total\":0,\"Between_Segments\":0,\"Between_Max\":0,\n",
        "                \"Between_Mean\":0.0,\"Between_Has_Verb\":False,\"Between_Has_Subordinator\":False}\n",
        "\n",
        "    first_start, last_end = spans_np[0][0], spans_np[-1][1]\n",
        "    pre = first_start\n",
        "    post = max(total - (last_end + 1), 0)\n",
        "\n",
        "    between = []\n",
        "    for i in range(len(spans_np)-1):\n",
        "        gap = max(spans_np[i+1][0] - (spans_np[i][1] + 1), 0)\n",
        "        between.append(gap)\n",
        "\n",
        "    has_verb = False; has_sub = False\n",
        "    if len(spans) >= 2:\n",
        "        for i in range(len(spans)-1):\n",
        "            a_end, b_start = spans[i][1], spans[i+1][0]\n",
        "            chunk = doc_tokens[a_end+1:b_start]\n",
        "            for tok in chunk:\n",
        "                if _HAS_SPACY:\n",
        "                    if getattr(tok,\"pos_\",\"\") == \"VERB\": has_verb = True\n",
        "                    if tok.text.lower() in _SUBORDINATORS: has_sub = True\n",
        "                else:\n",
        "                    if re.fullmatch(r\"[A-Za-z]+\", str(tok)) and str(tok).lower() in _SUBORDINATORS: has_sub = True\n",
        "                    if re.fullmatch(r\".*(ed|ing)$\", str(tok).lower()) or str(tok).lower() in {\"be\",\"is\",\"are\",\"was\",\"were\",\"been\",\"am\",\"have\",\"has\",\"had\",\"do\",\"did\",\"does\"}:\n",
        "                        has_verb = True\n",
        "\n",
        "    bt_total = int(np.sum(between)) if between else 0\n",
        "    bt_max   = int(np.max(between)) if between else 0\n",
        "    bt_mean  = float(np.mean(between)) if between else 0.0\n",
        "\n",
        "    return {\"Pre_Tokens\":int(pre),\"Post_Tokens\":int(post),\"Between_Tokens_List\":between,\n",
        "            \"Between_Tokens_Total\":bt_total,\"Between_Segments\":int(len(between)),\n",
        "            \"Between_Max\":bt_max,\"Between_Mean\":bt_mean,\n",
        "            \"Between_Has_Verb\":bool(has_verb),\"Between_Has_Subordinator\":bool(has_sub)}\n",
        "\n",
        "def _pattern_label(labels):\n",
        "    if not labels: return \"none\"\n",
        "    if labels == [\"as_as\"]: return \"as_as\"\n",
        "    if labels.count(\"like\") >= 2: return \"like_like\"\n",
        "    if any(l.startswith(\"punc_\") for l in labels): return \"framed_multi\" if len(labels) >= 2 else \"framed_single\"\n",
        "    return \"single\" if len(labels) == 1 else \"multi\"\n",
        "\n",
        "# ---------- main ----------\n",
        "df0 = _pick_df_for_segments()\n",
        "\n",
        "# Sentence column\n",
        "if \"Sentence_Context\" not in df0.columns:\n",
        "    for c in [\"Sentence Context\",\"sentence\",\"text\",\"context\",\"Text\"]:\n",
        "        if c in df0.columns:\n",
        "            df0 = df0.rename(columns={c: \"Sentence_Context\"})\n",
        "            break\n",
        "if \"Sentence_Context\" not in df0.columns:\n",
        "    raise RuntimeError(\"No 'Sentence_Context' column found.\")\n",
        "\n",
        "# Drop old metric columns to avoid duplicates\n",
        "metric_cols = [\n",
        "    \"Comp_Count\",\"Comp_Labels\",\"Pre_Tokens\",\"Between_Tokens_List\",\"Between_Tokens_Total\",\n",
        "    \"Between_Segments\",\"Between_Max\",\"Between_Mean\",\"Post_Tokens\",\n",
        "    \"Between_Has_Verb\",\"Between_Has_Subordinator\",\"Pattern_Label\",\n",
        "    \"Between_Tokens_List_Raw\",\"Pre_Share\",\"Between_Share\",\"Post_Share\"\n",
        "]\n",
        "existing = [c for c in metric_cols if c in df0.columns]\n",
        "if existing:\n",
        "    df0 = df0.drop(columns=existing)\n",
        "\n",
        "rows = []\n",
        "use_spacy = _HAS_SPACY\n",
        "print(\"spaCy available → using spaCy tokens.\" if use_spacy else \"spaCy unavailable → using simplified tokenization.\")\n",
        "\n",
        "for _, r in df0.iterrows():\n",
        "    sent = str(r.get(\"Sentence_Context\",\"\") or \"\").strip()\n",
        "    if not sent:\n",
        "        rows.append({\"Comp_Count\":0,\"Comp_Labels\":json.dumps([]),\n",
        "                     \"Pre_Tokens\":0,\"Between_Tokens_List\":[],\"Between_Tokens_Total\":0,\n",
        "                     \"Between_Segments\":0,\"Between_Max\":0,\"Between_Mean\":0.0,\n",
        "                     \"Post_Tokens\":0,\"Between_Has_Verb\":False,\"Between_Has_Subordinator\":False,\n",
        "                     \"Pattern_Label\":\"none\"})\n",
        "        continue\n",
        "\n",
        "    if use_spacy:\n",
        "        doc = nlp(sent); toks = list(doc)\n",
        "        spans = _detect_comparator_spans(toks, use_spacy=True, doc_obj=doc)\n",
        "    else:\n",
        "        toks = _simple_tokens(sent)\n",
        "        spans = _detect_comparator_spans(toks, use_spacy=False)\n",
        "\n",
        "    segs = _segments_from_spans(toks, spans, use_spacy=use_spacy)\n",
        "    labels = [lab for *_ab, lab in spans]\n",
        "    rows.append({\"Comp_Count\":len(spans),\"Comp_Labels\":json.dumps(labels),**segs,\"Pattern_Label\":_pattern_label(labels)})\n",
        "\n",
        "seg_df = pd.DataFrame(rows, index=df0.index)\n",
        "\n",
        "# Merge and ensure no duplicate column names remain\n",
        "aug = pd.concat([df0, seg_df], axis=1)\n",
        "aug = aug.loc[:, ~aug.columns.duplicated(keep=\"last\")]  # keep new metrics if any name collides\n",
        "\n",
        "# Serialise Between_Tokens_List to \"10, 10\" (or \"0\"), keep raw list\n",
        "def _serialise_between(x):\n",
        "    if isinstance(x, str):\n",
        "        s = x.strip()\n",
        "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "            try: x = ast.literal_eval(s)\n",
        "            except Exception: return \"0\" if s in (\"[]\",\"\") else s\n",
        "        else:\n",
        "            return s if s else \"0\"\n",
        "    if isinstance(x, (list, tuple, np.ndarray, pd.Series)):\n",
        "        vals = [str(int(v)) for v in x if pd.notna(v)]\n",
        "        return \", \".join(vals) if vals else \"0\"\n",
        "    if pd.isna(x) or x == \"\": return \"0\"\n",
        "    try: return str(int(x))\n",
        "    except Exception: return \"0\"\n",
        "\n",
        "aug[\"Between_Tokens_List_Raw\"] = aug[\"Between_Tokens_List\"]\n",
        "aug[\"Between_Tokens_List\"] = aug[\"Between_Tokens_List\"].apply(_serialise_between)\n",
        "\n",
        "# Normalized shares\n",
        "pre  = pd.to_numeric(aug[\"Pre_Tokens\"], errors=\"coerce\").fillna(0)\n",
        "btw  = pd.to_numeric(aug[\"Between_Tokens_Total\"], errors=\"coerce\").fillna(0)\n",
        "post = pd.to_numeric(aug[\"Post_Tokens\"], errors=\"coerce\").fillna(0)\n",
        "den  = (pre + btw + post).replace(0, np.nan)\n",
        "aug[\"Pre_Share\"]     = pre / den\n",
        "aug[\"Between_Share\"] = btw / den\n",
        "aug[\"Post_Share\"]    = post / den\n",
        "\n",
        "# Save & expose\n",
        "out_csv = os.path.join(out_dir, f\"comparator_segments_{ts}.csv\")\n",
        "aug.to_csv(out_csv, index=False)\n",
        "latest_csv = \"comparator_segments_latest.csv\"\n",
        "aug.to_csv(latest_csv, index=False)\n",
        "\n",
        "print(f\"✓ Saved comparator segment metrics → {out_csv}\")\n",
        "print(f\"✓ Also saved as → {latest_csv}\")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(out_csv)\n",
        "    files.download(latest_csv)\n",
        "    print(\"Downloads triggered.\")\n",
        "except Exception:\n",
        "    print(\"Colab download unavailable. Files are on disk.\")\n",
        "\n",
        "GLOBAL_DF = aug.copy()\n",
        "df_vis = aug.copy()\n",
        "print(\"GLOBAL_DF/df_vis updated with segment metrics.\")\n",
        "\n",
        "print(\"\\nSegment metrics summary (Joyce vs BNC if available):\")\n",
        "if \"Original_Dataset\" in aug.columns:\n",
        "    grp = aug.groupby(aug[\"Original_Dataset\"].astype(str).str.contains(\"BNC\", case=False, na=False).map({True:\"BNC\", False:\"Joyce\"}))\n",
        "    print(grp[[\"Comp_Count\",\"Between_Tokens_Total\",\"Between_Segments\",\"Between_Has_Verb\",\"Between_Has_Subordinator\"]]\n",
        "          .agg({\"Comp_Count\":\"mean\",\"Between_Tokens_Total\":\"mean\",\"Between_Segments\":\"mean\",\n",
        "                \"Between_Has_Verb\":\"mean\",\"Between_Has_Subordinator\":\"mean\"}).round(2))\n",
        "else:\n",
        "    print(aug[[\"Comp_Count\",\"Between_Tokens_Total\",\"Between_Segments\"]].describe().round(2))\n"
      ],
      "metadata": {
        "id": "bRh-dwNY3Q3c",
        "outputId": "10dd0c27-ed20-46d4-b624-3970e35886f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using in-memory dataset: GLOBAL_DF (705 rows)\n",
            "spaCy available → using spaCy tokens.\n",
            "✓ Saved comparator segment metrics → analysis_outputs/comparator_segments_20250825_105404.csv\n",
            "✓ Also saved as → comparator_segments_latest.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a86a86c9-1153-4a35-a714-08decdce108f\", \"comparator_segments_20250825_105404.csv\", 734657)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c0a3439a-10e4-4c7d-9e15-1ba884d0cbe4\", \"comparator_segments_latest.csv\", 734657)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloads triggered.\n",
            "GLOBAL_DF/df_vis updated with segment metrics.\n",
            "\n",
            "Segment metrics summary (Joyce vs BNC if available):\n",
            "                  Comp_Count  Between_Tokens_Total  Between_Segments  \\\n",
            "Original_Dataset                                                       \n",
            "BNC                     1.48                  2.50              0.48   \n",
            "Joyce                   1.42                  4.76              0.50   \n",
            "\n",
            "                  Between_Has_Verb  Between_Has_Subordinator  \n",
            "Original_Dataset                                              \n",
            "BNC                           0.16                      0.06  \n",
            "Joyce                         0.26                      0.12  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Statistical Significance Testing\n",
        "# 7.1 Multi-Group Comparative Analysis\n",
        "The statistical analysis distinguishes between Joyce Manual, Joyce Restrictive, Joyce Less-Restrictive, and BNC subsets to provide granular assessment of methodological differences.\n",
        "\n",
        "# 7.2 Robust Statistical Framework\n",
        "Implementation includes:\n",
        "\n",
        "Four-way chi-square analysis for categorical distribution testing\n",
        "Newcombe-Wilson confidence intervals for two-proportion comparisons\n",
        "Binomial testing against BNC reference proportions\n",
        "Welch t-tests and Mann-Whitney U tests for continuous feature assessment\n",
        "\n",
        "# 7.3 Topic Modeling Integration\n",
        "Latent Dirichlet Allocation provides thematic analysis across all dataset subsets, revealing content-based distinctions complementing statistical findings."
      ],
      "metadata": {
        "id": "ft6Meu_eeMDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ROBUST STATISTICAL SIGNIFICANCE + TOPIC MODELLING (Joyce subsets vs BNC)\n",
        "# =============================================================================\n",
        "\n",
        "import os, json, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from math import asin, sqrt\n",
        "from scipy.stats import chi2_contingency, mannwhitneyu, ttest_ind, binomtest, norm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "try:\n",
        "    from statsmodels.stats.proportion import proportions_ztest, confint_proportions_2indep\n",
        "    _HAS_STATSMODELS = True\n",
        "except Exception:\n",
        "    _HAS_STATSMODELS = False\n",
        "\n",
        "# ---------- Setup ----------\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "out_dir = os.path.join(\"analysis_outputs\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "print(\"\\nROBUST STATISTICAL ANALYSIS (Joyce subsets vs BNC)\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# --- Sanity: results_df must exist from Cell 1 ---\n",
        "if 'results_df' not in globals() or results_df is None or results_df.empty:\n",
        "    raise RuntimeError(\"results_df not found or empty. Run Cell 1 first.\")\n",
        "\n",
        "# --- Group labels (from Cell 1 'Original_Dataset') ---\n",
        "LABELS = {\n",
        "    \"Manual_CloseReading\":      \"Joyce_Manual\",\n",
        "    \"Restrictive_Dubliners\":    \"Joyce_Restrictive\",\n",
        "    \"NLP_LessRestrictive_PG\":   \"Joyce_LessRestrictive\",\n",
        "    \"BNC_Baseline\":             \"BNC\"\n",
        "}\n",
        "\n",
        "df = results_df.copy()\n",
        "if \"Original_Dataset\" not in df.columns:\n",
        "    raise RuntimeError(\"results_df is missing 'Original_Dataset'.\")\n",
        "if \"Category_Framework\" not in df.columns:\n",
        "    raise RuntimeError(\"results_df is missing 'Category_Framework'.\")\n",
        "\n",
        "df[\"__Group__\"] = df[\"Original_Dataset\"].map(LABELS).fillna(df[\"Original_Dataset\"])\n",
        "\n",
        "# --- Category whitelist (Joycean_Quasi was merged → keep unified 'Quasi_Similes') ---\n",
        "KNOWN_CATEGORIES = {\n",
        "    'Standard',\n",
        "    'Quasi_Similes',          # unified quasi-simile label\n",
        "    'Joycean_Quasi_Fuzzy',\n",
        "    'Joycean_Framed',\n",
        "    'Joycean_Silent',\n",
        "    'Uncategorized'\n",
        "}\n",
        "\n",
        "bad_mask = ~df[\"Category_Framework\"].isin(KNOWN_CATEGORIES)\n",
        "if bad_mask.any():\n",
        "    dropped = int(bad_mask.sum())\n",
        "    print(f\"[WARN] Dropping {dropped} rows with unexpected Category_Framework values: \"\n",
        "          f\"{sorted(df.loc[bad_mask, 'Category_Framework'].astype(str).unique())}\")\n",
        "    df = df.loc[~bad_mask].copy()\n",
        "\n",
        "# --- Quick distribution sanity table (post-harmonisation) ---\n",
        "dist = (df.pivot_table(index=\"Category_Framework\",\n",
        "                       columns=\"Original_Dataset\",\n",
        "                       values=\"Instance_ID\",\n",
        "                       aggfunc=\"count\", fill_value=0)\n",
        "          .assign(Total=lambda x: x.sum(1))\n",
        "          .sort_values(\"Total\", ascending=False))\n",
        "print(\"\\nCategory distribution after harmonisation (counts):\")\n",
        "print(dist)\n",
        "\n",
        "# --- Split groups ---\n",
        "groups = {\n",
        "    \"Joyce_Manual\":          df[df[\"__Group__\"]==\"Joyce_Manual\"],\n",
        "    \"Joyce_Restrictive\":     df[df[\"__Group__\"]==\"Joyce_Restrictive\"],\n",
        "    \"Joyce_LessRestrictive\": df[df[\"__Group__\"]==\"Joyce_LessRestrictive\"],\n",
        "    \"BNC\":                   df[df[\"__Group__\"]==\"BNC\"]\n",
        "}\n",
        "for gname, gdf in groups.items():\n",
        "    print(f\"{gname:22s}: {len(gdf)} rows\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def p_adjust_bh(p):\n",
        "    \"\"\"Benjamini–Hochberg FDR for a 1D array-like of p-values.\"\"\"\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    n = p.size\n",
        "    order = np.argsort(p)\n",
        "    ranked = np.empty(n, dtype=float)\n",
        "    cummin = 1.0\n",
        "    for i, idx in enumerate(order[::-1], start=1):\n",
        "        rank = n - i + 1\n",
        "        val = p[idx] * n / rank\n",
        "        cummin = min(cummin, val)\n",
        "        ranked[idx] = cummin\n",
        "    return np.minimum(ranked, 1.0)\n",
        "\n",
        "def cramers_v(chi2, n, r, c):\n",
        "    \"\"\"Cramér's V for r x c table.\"\"\"\n",
        "    if n <= 0 or min(r, c) <= 1:\n",
        "        return np.nan\n",
        "    return sqrt(chi2 / (n * (min(r, c) - 1)))\n",
        "\n",
        "def cohens_h(p1, p2):\n",
        "    \"\"\"Cohen's h for proportions.\"\"\"\n",
        "    p1 = min(max(float(p1), 0.0), 1.0)\n",
        "    p2 = min(max(float(p2), 0.0), 1.0)\n",
        "    return 2 * (asin(sqrt(p1)) - asin(sqrt(p2)))\n",
        "\n",
        "def hedges_g(a, b):\n",
        "    \"\"\"Hedges' g (small-sample corrected Cohen's d).\"\"\"\n",
        "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
        "    na, nb = len(a), len(b)\n",
        "    if na < 2 or nb < 2:\n",
        "        return np.nan\n",
        "    s1 = np.var(a, ddof=1); s2 = np.var(b, ddof=1)\n",
        "    pooled = ((na-1)*s1 + (nb-1)*s2) / (na+nb-2) if (na+nb-2) > 0 else np.nan\n",
        "    if not np.isfinite(pooled) or pooled <= 0:\n",
        "        return np.nan\n",
        "    d = (np.mean(a) - np.mean(b)) / np.sqrt(pooled)\n",
        "    J = 1 - (3 / (4*(na+nb) - 9)) if (na+nb) > 2 else 1.0\n",
        "    return d * J\n",
        "\n",
        "def cliffs_delta_from_u(a, b):\n",
        "    \"\"\"Cliff's delta via Mann–Whitney U (orientation A > B).\"\"\"\n",
        "    a = pd.Series(a).dropna().to_numpy()\n",
        "    b = pd.Series(b).dropna().to_numpy()\n",
        "    if len(a) == 0 or len(b) == 0:\n",
        "        return np.nan\n",
        "    u_ab, _ = mannwhitneyu(a, b, alternative=\"greater\")\n",
        "    return (2 * u_ab) / (len(a)*len(b)) - 1\n",
        "\n",
        "# ---------- 1) 4-way Chi-square on Category_Framework ----------\n",
        "cats = sorted(KNOWN_CATEGORIES)\n",
        "contingency_4way = pd.DataFrame(\n",
        "    {\n",
        "        \"Joyce_Manual\":          [groups[\"Joyce_Manual\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "        \"Joyce_Restrictive\":     [groups[\"Joyce_Restrictive\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "        \"Joyce_LessRestrictive\": [groups[\"Joyce_LessRestrictive\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "        \"BNC\":                   [groups[\"BNC\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "    },\n",
        "    index=cats\n",
        ")\n",
        "\n",
        "sim_used = False\n",
        "try:\n",
        "    chi2_4, p_4, dof_4, exp_4 = chi2_contingency(contingency_4way, correction=False)\n",
        "    if (np.asarray(exp_4) < 5).any():\n",
        "        try:\n",
        "            chi2_4, p_4, dof_4, exp_4 = chi2_contingency(\n",
        "                contingency_4way, correction=False, simulate_pval=True, num_simulation=5000\n",
        "            )\n",
        "            sim_used = True\n",
        "        except TypeError:\n",
        "            pass\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"chi2_contingency failed: {e}\")\n",
        "\n",
        "N_total = contingency_4way.values.sum()\n",
        "V_4 = cramers_v(chi2_4, N_total, *contingency_4way.shape)\n",
        "\n",
        "print(\"\\n4-way Chi-square on Category_Framework (Joyce subsets vs BNC):\")\n",
        "print(f\"χ² = {chi2_4:.4f} | df = {dof_4} | p = {p_4:.6f} | Cramér’s V = {V_4:.3f} | Monte-Carlo={sim_used}\")\n",
        "\n",
        "# Save contingency + expected + standardized residuals + per-cell p-values (FDR)\n",
        "path_cont_4 = os.path.join(out_dir, f\"chi2_contingency_by_subset_{ts}.csv\")\n",
        "path_exp_4  = os.path.join(out_dir, f\"chi2_expected_by_subset_{ts}.csv\")\n",
        "contingency_4way.to_csv(path_cont_4)\n",
        "exp_df_4 = pd.DataFrame(exp_4, index=cats, columns=contingency_4way.columns)\n",
        "exp_df_4.to_csv(path_exp_4)\n",
        "\n",
        "pearson_z = (contingency_4way - exp_df_4) / np.sqrt(exp_df_4.replace(0, np.nan))\n",
        "cell_pvals = pearson_z.applymap(lambda z: 2*norm.sf(abs(z)) if pd.notnull(z) else np.nan)\n",
        "\n",
        "flat = cell_pvals.values.flatten()\n",
        "mask = np.isfinite(flat)\n",
        "adj = np.full_like(flat, np.nan, dtype=float)\n",
        "if mask.any():\n",
        "    adj[mask] = p_adjust_bh(flat[mask])\n",
        "cell_pvals_adj = pd.DataFrame(adj.reshape(cell_pvals.shape), index=cell_pvals.index, columns=cell_pvals.columns)\n",
        "\n",
        "path_resid_z = os.path.join(out_dir, f\"chi2_pearson_z_by_subset_{ts}.csv\")\n",
        "path_resid_p = os.path.join(out_dir, f\"chi2_cell_p_by_subset_{ts}.csv\")\n",
        "path_resid_padj = os.path.join(out_dir, f\"chi2_cell_padj_BH_by_subset_{ts}.csv\")\n",
        "pearson_z.to_csv(path_resid_z)\n",
        "cell_pvals.to_csv(path_resid_p)\n",
        "cell_pvals_adj.to_csv(path_resid_padj)\n",
        "\n",
        "# Print the strongest drivers (optional but helpful)\n",
        "absz = pearson_z.abs().stack().sort_values(ascending=False)\n",
        "print(\"\\nTop 10 standardized residuals (|z|):\")\n",
        "for (cat, grp), z in absz.head(10).items():\n",
        "    obs = contingency_4way.loc[cat, grp]\n",
        "    exp = exp_df_4.loc[cat, grp]\n",
        "    print(f\"  {grp:22s} | {cat:20s}  z={z:6.2f}  obs={obs} exp={exp:.1f}\")\n",
        "\n",
        "# ---------- 2) Two-proportion tests (each Joyce subset vs BNC) ----------\n",
        "print(\"\\nTwo-proportion tests (Newcombe–Wilson) for each Joyce subset vs BNC:\")\n",
        "two_prop_rows = []\n",
        "bnc_total = len(groups[\"BNC\"])\n",
        "bnc_counts = groups[\"BNC\"][\"Category_Framework\"].value_counts()\n",
        "\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "    subset_total = len(groups[subset])\n",
        "    subset_counts = groups[subset][\"Category_Framework\"].value_counts()\n",
        "    for cat in cats:\n",
        "        cA = subset_counts.get(cat,0); nA = subset_total\n",
        "        cB = bnc_counts.get(cat,0);    nB = bnc_total\n",
        "        pA = cA/nA if nA>0 else 0.0\n",
        "        pB = cB/nB if nB>0 else 0.0\n",
        "        row = {\"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset, \"Category\":cat,\n",
        "               \"count_A\":cA, \"n_A\":nA, \"prop_A\":pA, \"count_B\":cB, \"n_B\":nB, \"prop_B\":pB,\n",
        "               \"cohens_h\": float(cohens_h(pA, pB))}\n",
        "        # Skip boundary cases where both groups are 0 or 1 for this category\n",
        "        if (cA == 0 and cB == 0) or (cA == nA and cB == nB) or (nA == 0 or nB == 0):\n",
        "            row.update({\"z\": np.nan, \"p_value\": 1.0, \"CI_low\": np.nan, \"CI_up\": np.nan})\n",
        "            print(f\"  {subset:22s} | {cat:20s} (both groups at boundary → skip) h={row['cohens_h']:.3f}\")\n",
        "            two_prop_rows.append(row); continue\n",
        "        if _HAS_STATSMODELS:\n",
        "            try:\n",
        "                z, pz = proportions_ztest(np.array([cA,cB]), np.array([nA,nB]))\n",
        "            except Exception:\n",
        "                # Haldane–Anscombe continuity correction fallback\n",
        "                pA_ha = (cA + 0.5) / (nA + 1)\n",
        "                pB_ha = (cB + 0.5) / (nB + 1)\n",
        "                se = np.sqrt(pA_ha*(1-pA_ha)/(nA+1) + pB_ha*(1-pB_ha)/(nB+1))\n",
        "                z = (pA_ha - pB_ha) / se if se>0 else np.nan\n",
        "                pz = 2*norm.sf(abs(z)) if np.isfinite(z) else np.nan\n",
        "            ci_low, ci_up = (np.nan, np.nan)\n",
        "            try:\n",
        "                ci_low, ci_up = confint_proportions_2indep(cA, nA, cB, nB, method=\"newcombe\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            row.update({\"z\":float(z), \"p_value\":float(pz), \"CI_low\":float(ci_low), \"CI_up\":float(ci_up)})\n",
        "            print(f\"  {subset:22s} | {cat:20s} z={z:6.3f} p={pz:.6g} CI[{ci_low:.3f},{ci_up:.3f}] h={row['cohens_h']:.3f}\")\n",
        "        else:\n",
        "            row.update({\"z\":np.nan, \"p_value\":np.nan, \"CI_low\":np.nan, \"CI_up\":np.nan})\n",
        "            print(f\"  {subset:22s} | {cat:20s} (statsmodels unavailable → skipping z/CI) h={row['cohens_h']:.3f}\")\n",
        "        two_prop_rows.append(row)\n",
        "\n",
        "two_prop_df = pd.DataFrame(two_prop_rows)\n",
        "if \"p_value\" in two_prop_df.columns:\n",
        "    two_prop_df[\"p_adj_BH\"] = p_adjust_bh(two_prop_df[\"p_value\"].fillna(1.0).to_numpy())\n",
        "path_two_prop = os.path.join(out_dir, f\"two_prop_newcombe_by_subset_{ts}.csv\")\n",
        "two_prop_df.to_csv(path_two_prop, index=False)\n",
        "\n",
        "# ---------- 3) Binomial tests (subset vs BNC reference proportion) ----------\n",
        "print(\"\\nBinomial tests (each Joyce subset vs BNC category proportion):\")\n",
        "binom_rows = []\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "    nA = len(groups[subset])\n",
        "    subset_counts = groups[subset][\"Category_Framework\"].value_counts()\n",
        "    for cat in cats:\n",
        "        cA = subset_counts.get(cat,0)\n",
        "        cB = bnc_counts.get(cat,0); nB = bnc_total\n",
        "        p_ref = (cB/nB) if nB>0 else 0.0\n",
        "        if nA>0 and 0 < p_ref < 1:\n",
        "            bt = binomtest(cA, n=nA, p=p_ref)\n",
        "            pv = bt.pvalue\n",
        "            binom_rows.append({\"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset, \"Category\":cat,\n",
        "                               \"count_A\":cA, \"n_A\":nA, \"p_ref_BNC\":p_ref, \"p_value\":pv})\n",
        "            print(f\"  {subset:22s} | {cat:20s} {cA}/{nA} vs p_ref={p_ref:.4f} p={pv:.6g}\")\n",
        "        else:\n",
        "            binom_rows.append({\"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset, \"Category\":cat,\n",
        "                               \"count_A\":cA, \"n_A\":nA, \"p_ref_BNC\":p_ref, \"p_value\":np.nan})\n",
        "\n",
        "binom_df = pd.DataFrame(binom_rows)\n",
        "if \"p_value\" in binom_df.columns:\n",
        "    binom_df[\"p_adj_BH\"] = p_adjust_bh(binom_df[\"p_value\"].fillna(1.0).to_numpy())\n",
        "path_binom = os.path.join(out_dir, f\"binomial_tests_by_subset_{ts}.csv\")\n",
        "binom_df.to_csv(path_binom, index=False)\n",
        "\n",
        "# ---------- 4) Continuous features (subset vs BNC) ----------\n",
        "print(\"\\nContinuous features (Welch t + Mann–Whitney U) each Joyce subset vs BNC:\")\n",
        "continuous_feats = [\"Sentence_Length\",\"Pre_Post_Ratio\",\"Sentiment_Polarity\",\"Sentiment_Subjectivity\"]\n",
        "cont_rows = []\n",
        "\n",
        "for feat in continuous_feats:\n",
        "    for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "        A = pd.to_numeric(groups[subset].get(feat, pd.Series(dtype=float)), errors=\"coerce\").dropna()\n",
        "        B = pd.to_numeric(groups[\"BNC\"].get(feat, pd.Series(dtype=float)), errors=\"coerce\").dropna()\n",
        "        row = {\"Feature\":feat, \"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset,\n",
        "               \"A_n\":int(A.shape[0]), \"B_n\":int(B.shape[0])}\n",
        "        if len(A)>10 and len(B)>10:\n",
        "            t,p_t = ttest_ind(A,B,equal_var=False)\n",
        "            u,p_u = mannwhitneyu(A,B,alternative=\"two-sided\")\n",
        "            row.update({\n",
        "                \"A_mean\":float(A.mean()), \"A_median\":float(A.median()),\n",
        "                \"B_mean\":float(B.mean()), \"B_median\":float(B.median()),\n",
        "                \"t_stat\":float(t), \"t_pvalue\":float(p_t),\n",
        "                \"U_stat\":float(u), \"U_pvalue\":float(p_u),\n",
        "                \"hedges_g\": float(hedges_g(A, B)),\n",
        "                \"cliffs_delta\": float(cliffs_delta_from_u(A, B))\n",
        "            })\n",
        "            print(f\"  {feat:22s} | {subset:22s} t={t:7.3f} p={p_t:.6g} | U={u:9.1f} p={p_u:.6g} | g={row['hedges_g']:.3f} δ={row['cliffs_delta']:.3f}\")\n",
        "        cont_rows.append(row)\n",
        "\n",
        "cont_df = pd.DataFrame(cont_rows)\n",
        "if \"t_pvalue\" in cont_df.columns:\n",
        "    cont_df[\"t_padj_BH\"] = p_adjust_bh(cont_df[\"t_pvalue\"].fillna(1.0).to_numpy())\n",
        "if \"U_pvalue\" in cont_df.columns:\n",
        "    cont_df[\"U_padj_BH\"] = p_adjust_bh(cont_df[\"U_pvalue\"].fillna(1.0).to_numpy())\n",
        "\n",
        "path_cont = os.path.join(out_dir, f\"continuous_tests_by_subset_{ts}.csv\")\n",
        "cont_df.to_csv(path_cont, index=False)\n",
        "\n",
        "# ---------- 4B) Comparator-segment metrics (Pre / Between / Post) ----------\n",
        "# Tries comparator_segments_latest.csv first, then the most recent timestamped file in analysis_outputs/\n",
        "import glob\n",
        "\n",
        "print(\"\\nComparator-segment statistics (Pre / Between / Post)\")\n",
        "seg_candidates = [\"comparator_segments_latest.csv\"] + sorted(\n",
        "    glob.glob(os.path.join(\"analysis_outputs\", \"comparator_segments_*.csv\")),\n",
        "    key=os.path.getctime, reverse=True\n",
        ")\n",
        "seg_path = next((p for p in seg_candidates if os.path.exists(p)), None)\n",
        "\n",
        "if seg_path is None:\n",
        "    print(\"  [WARN] No comparator segments file found — skipping section 4B.\")\n",
        "else:\n",
        "    print(f\"  Using segments file: {seg_path}\")\n",
        "    seg_raw = pd.read_csv(seg_path)\n",
        "\n",
        "    # --- flexible column detection (now includes new names from the enrichment cell) ---\n",
        "    def _pick(cols, cand):\n",
        "        for c in cand:\n",
        "            if c in cols: return c\n",
        "        return None\n",
        "\n",
        "    id_col   = _pick(seg_raw.columns, [\"Instance_ID\",\"instance_id\",\"ID\",\"Id\"])\n",
        "    pre_col  = _pick(seg_raw.columns, [\"Pre_Tokens\",\"Pre_Comparator_Tokens\",\"pre_tokens\",\"pre_len\",\"Pre_Length\"])\n",
        "    # NOTE: include the new name 'Between_Tokens_Total'\n",
        "    bet_col  = _pick(seg_raw.columns, [\"Between_Tokens_Total\",\"Between_Tokens\",\"Between_Comparator_Tokens\",\n",
        "                                       \"between_tokens\",\"between_len\",\"Between_Length\",\"InBetween_Tokens\"])\n",
        "    post_col = _pick(seg_raw.columns, [\"Post_Tokens\",\"Post_Comparator_Tokens\",\"post_tokens\",\"post_len\",\"Post_Length\"])\n",
        "    ccount   = _pick(seg_raw.columns, [\"Comp_Count\",\"Comparator_Count\",\"comparators\",\"n_comparators\"])\n",
        "\n",
        "    if id_col is None or (pre_col is None and post_col is None and bet_col is None):\n",
        "        print(\"  [WARN] Required columns not found in segments CSV — skipping 4B.\")\n",
        "    else:\n",
        "        keep_cols = [id_col] + [c for c in [pre_col, bet_col, post_col, ccount] if c is not None]\n",
        "        seg = seg_raw[keep_cols].copy()\n",
        "\n",
        "        rename_map = {}\n",
        "        if pre_col:  rename_map[pre_col]  = \"Seg_Pre\"\n",
        "        if bet_col:  rename_map[bet_col]  = \"Seg_Between\"\n",
        "        if post_col: rename_map[post_col] = \"Seg_Post\"\n",
        "        if ccount:   rename_map[ccount]   = \"Seg_Comp_Count\"\n",
        "        seg = seg.rename(columns=rename_map)\n",
        "\n",
        "        # Ensure all expected columns exist (fill missing with 0)\n",
        "        for c in [\"Seg_Pre\",\"Seg_Between\",\"Seg_Post\",\"Seg_Comp_Count\"]:\n",
        "            if c not in seg.columns:\n",
        "                seg[c] = 0\n",
        "\n",
        "        # Numeric coercion\n",
        "        for c in [\"Seg_Pre\",\"Seg_Between\",\"Seg_Post\",\"Seg_Comp_Count\"]:\n",
        "            seg[c] = pd.to_numeric(seg[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "        # Totals + ratios (safe divide)\n",
        "        seg[\"Seg_Total\"] = seg[[\"Seg_Pre\",\"Seg_Between\",\"Seg_Post\"]].sum(axis=1, min_count=1)\n",
        "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "            seg[\"Seg_Pre_Ratio\"]     = seg[\"Seg_Pre\"]     / seg[\"Seg_Total\"].replace(0, np.nan)\n",
        "            seg[\"Seg_Between_Ratio\"] = seg[\"Seg_Between\"] / seg[\"Seg_Total\"].replace(0, np.nan)\n",
        "            seg[\"Seg_Post_Ratio\"]    = seg[\"Seg_Post\"]    / seg[\"Seg_Total\"].replace(0, np.nan)\n",
        "        seg[\"Seg_Between_gt0\"] = (seg[\"Seg_Between\"] > 0).astype(int)\n",
        "\n",
        "        # Merge onto working df (which already has __Group__)\n",
        "        merged_seg = df.merge(seg, left_on=\"Instance_ID\", right_on=id_col, how=\"left\")\n",
        "\n",
        "        # Build grouped view (mirror of 'groups')\n",
        "        groups_seg = {\n",
        "            \"Joyce_Manual\":          merged_seg[merged_seg[\"__Group__\"]==\"Joyce_Manual\"],\n",
        "            \"Joyce_Restrictive\":     merged_seg[merged_seg[\"__Group__\"]==\"Joyce_Restrictive\"],\n",
        "            \"Joyce_LessRestrictive\": merged_seg[merged_seg[\"__Group__\"]==\"Joyce_LessRestrictive\"],\n",
        "            \"BNC\":                   merged_seg[merged_seg[\"__Group__\"]==\"BNC\"],\n",
        "        }\n",
        "\n",
        "        seg_features = [c for c in [\n",
        "            \"Seg_Pre\",\"Seg_Between\",\"Seg_Post\",\"Seg_Total\",\n",
        "            \"Seg_Pre_Ratio\",\"Seg_Between_Ratio\",\"Seg_Post_Ratio\",\n",
        "            \"Seg_Comp_Count\"\n",
        "        ] if c in merged_seg.columns]\n",
        "\n",
        "        print(\"  Features available:\", seg_features if seg_features else \"None\")\n",
        "\n",
        "        # ---- (i) Continuous tests for segment metrics (subset vs BNC) ----\n",
        "        seg_rows = []\n",
        "        for feat in seg_features:\n",
        "            for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "                A = pd.to_numeric(groups_seg[subset][feat], errors=\"coerce\").dropna()\n",
        "                B = pd.to_numeric(groups_seg[\"BNC\"][feat], errors=\"coerce\").dropna()\n",
        "                row = {\"Feature\": feat, \"Comparison\": f\"{subset}_vs_BNC\", \"Subset\": subset,\n",
        "                       \"A_n\": int(A.shape[0]), \"B_n\": int(B.shape[0])}\n",
        "                if len(A) > 10 and len(B) > 10 and np.isfinite(A).any() and np.isfinite(B).any():\n",
        "                    t, p_t = ttest_ind(A, B, equal_var=False)\n",
        "                    u, p_u = mannwhitneyu(A, B, alternative=\"two-sided\")\n",
        "                    row.update({\n",
        "                        \"A_mean\": float(A.mean()), \"A_median\": float(A.median()),\n",
        "                        \"B_mean\": float(B.mean()), \"B_median\": float(B.median()),\n",
        "                        \"t_stat\": float(t), \"t_pvalue\": float(p_t),\n",
        "                        \"U_stat\": float(u), \"U_pvalue\": float(p_u),\n",
        "                        \"hedges_g\": float(hedges_g(A, B)),\n",
        "                        \"cliffs_delta\": float(cliffs_delta_from_u(A, B))\n",
        "                    })\n",
        "                    print(f\"  {feat:20s} | {subset:22s} t={t:7.3f} p={p_t:.3g} | U={u:9.1f} p={p_u:.3g} | g={row['hedges_g']:.3f} δ={row['cliffs_delta']:.3f}\")\n",
        "                seg_rows.append(row)\n",
        "\n",
        "        seg_cont_df = pd.DataFrame(seg_rows)\n",
        "        if \"t_pvalue\" in seg_cont_df.columns:\n",
        "            seg_cont_df[\"t_padj_BH\"] = p_adjust_bh(seg_cont_df[\"t_pvalue\"].fillna(1.0).to_numpy())\n",
        "        if \"U_pvalue\" in seg_cont_df.columns:\n",
        "            seg_cont_df[\"U_padj_BH\"] = p_adjust_bh(seg_cont_df[\"U_pvalue\"].fillna(1.0).to_numpy())\n",
        "\n",
        "        path_seg_cont = os.path.join(out_dir, f\"continuous_segment_tests_by_subset_{ts}.csv\")\n",
        "        seg_cont_df.to_csv(path_seg_cont, index=False)\n",
        "\n",
        "       # ---- (ii) Does 'Between > 0' differ by group? (χ² with robust fallbacks) ----\n",
        "from scipy.stats import fisher_exact\n",
        "\n",
        "# fill NaN as 0 so groups without segment rows still contribute\n",
        "col_between = merged_seg.get(\"Seg_Between_gt0\", pd.Series(index=merged_seg.index)).fillna(0).astype(int)\n",
        "tmp = merged_seg.assign(_between=col_between)\n",
        "\n",
        "pres = tmp.groupby(\"__Group__\")[\"_between\"].agg([\"sum\",\"count\"])\n",
        "# keep a stable order if present\n",
        "order = [g for g in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\",\"BNC\"] if g in pres.index]\n",
        "pres = pres.loc[order]\n",
        "\n",
        "presence_tbl = pd.DataFrame({\n",
        "    \"Between>0\": pres[\"sum\"].astype(int),\n",
        "    \"Between=0\": (pres[\"count\"] - pres[\"sum\"]).astype(int)\n",
        "})\n",
        "\n",
        "# drop rows/cols with zero total to avoid zero expected cells\n",
        "presence_tbl = presence_tbl[presence_tbl.sum(axis=1) > 0]\n",
        "presence_tbl = presence_tbl.loc[:, presence_tbl.sum(axis=0) > 0]\n",
        "\n",
        "if presence_tbl.shape[0] < 2 or presence_tbl.shape[1] != 2:\n",
        "    print(\"\\nBetween-clause presence × Group (χ²):\")\n",
        "    print(presence_tbl if not presence_tbl.empty else \"(no usable data)\")\n",
        "    print(\"  Not enough non-zero groups/columns for χ²; skipping.\")\n",
        "    path_presence_csv = path_presence_exp = path_presence_resid = None\n",
        "else:\n",
        "    print(\"\\nBetween-clause presence × Group (χ²):\")\n",
        "    print(presence_tbl)\n",
        "\n",
        "    exp_seg = None\n",
        "    try:\n",
        "        chi2_seg, p_seg, dof_seg, exp_seg = chi2_contingency(presence_tbl, correction=False)\n",
        "        V_seg = cramers_v(chi2_seg, presence_tbl.values.sum(), *presence_tbl.shape)\n",
        "        print(f\"  χ² = {chi2_seg:.3f} | df = {dof_seg} | p = {p_seg:.6f} | Cramér’s V = {V_seg:.3f}\")\n",
        "    except ValueError:\n",
        "        # Try Monte Carlo χ² (SciPy ≥1.7); if not available, fall back to Fisher pairwise vs BNC.\n",
        "        try:\n",
        "            chi2_seg, p_seg, dof_seg, exp_seg = chi2_contingency(\n",
        "                presence_tbl, correction=False, simulate_pval=True, num_simulation=5000\n",
        "            )\n",
        "            V_seg = cramers_v(chi2_seg, presence_tbl.values.sum(), *presence_tbl.shape)\n",
        "            print(f\"  Monte-Carlo χ² = {chi2_seg:.3f} | df = {dof_seg} | p = {p_seg:.6f} | V = {V_seg:.3f}\")\n",
        "        except TypeError:\n",
        "            print(\"  χ² not valid (zero expected). Pairwise Fisher exact p-values vs BNC:\")\n",
        "            for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "                if subset in presence_tbl.index and \"BNC\" in presence_tbl.index:\n",
        "                    tbl2 = presence_tbl.loc[[subset, \"BNC\"]].to_numpy()\n",
        "                    _, p_f = fisher_exact(tbl2)  # two-sided\n",
        "                    print(f\"   - {subset:22s}: p = {p_f:.6g}\")\n",
        "            p_seg = np.nan; chi2_seg = np.nan; dof_seg = 1\n",
        "\n",
        "    # Save presence table + expected + residuals if we have them\n",
        "    path_presence_csv = os.path.join(out_dir, f\"between_presence_by_group_{ts}.csv\")\n",
        "    presence_tbl.to_csv(path_presence_csv)\n",
        "\n",
        "    if exp_seg is not None:\n",
        "        path_presence_exp = os.path.join(out_dir, f\"between_presence_expected_by_group_{ts}.csv\")\n",
        "        path_presence_resid = os.path.join(out_dir, f\"between_presence_residuals_by_group_{ts}.csv\")\n",
        "        exp_df = pd.DataFrame(exp_seg, index=presence_tbl.index, columns=presence_tbl.columns)\n",
        "        exp_df.to_csv(path_presence_exp)\n",
        "        resid = (presence_tbl - exp_df) / np.sqrt(exp_df.replace(0, np.nan))\n",
        "        resid.to_csv(path_presence_resid)\n",
        "    else:\n",
        "        path_presence_exp = path_presence_resid = None\n",
        "\n",
        "# ---------- 5) Topic modelling (per subset + BNC) ----------\n",
        "print(\"\\nTOPIC MODELLING (per subset + BNC)\")\n",
        "\n",
        "def prepare_corpus(gdf):\n",
        "    \"\"\"Prefer Lemmatized_Text from Cell 1; fallback to Sentence_Context.\"\"\"\n",
        "    if \"Lemmatized_Text\" in gdf.columns and gdf[\"Lemmatized_Text\"].notna().any():\n",
        "        texts = gdf[\"Lemmatized_Text\"].fillna(\"\").astype(str).tolist()\n",
        "    else:\n",
        "        texts = gdf[\"Sentence_Context\"].fillna(\"\").astype(str).tolist() if \"Sentence_Context\" in gdf.columns else []\n",
        "    return [t for t in texts if t.strip()]\n",
        "\n",
        "def lda_topics(corpus, n_topics=5, n_top_words=10, max_df=0.85, min_df=2, max_features=5000, random_state=RANDOM_STATE):\n",
        "    if not corpus:\n",
        "        return None, None, None\n",
        "    vectorizer = CountVectorizer(\n",
        "        max_df=max_df, min_df=min_df, max_features=max_features,\n",
        "        stop_words=\"english\",\n",
        "        token_pattern=r\"(?u)\\b[^\\W\\d_][^\\W\\d_]+\\b\"\n",
        "    )\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    lda = LatentDirichletAllocation(\n",
        "        n_components=n_topics, random_state=random_state, learning_method=\"batch\"\n",
        "    )\n",
        "    lda.fit(X)\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for comp in lda.components_:\n",
        "        top_idx = comp.argsort()[:-n_top_words-1:-1]\n",
        "        topics.append([terms[i] for i in top_idx])\n",
        "    doc_topic = lda.transform(X)  # rows sum ~1\n",
        "    return topics, doc_topic, terms\n",
        "\n",
        "topics_summary = {\"params\":{\"n_topics\":5,\"n_top_words\":10,\"vectorizer\":\"CountVectorizer\"}, \"groups\":{}}\n",
        "topic_rows = []\n",
        "topicmix_rows = []\n",
        "\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\",\"BNC\"]:\n",
        "    corpus = prepare_corpus(groups[subset])\n",
        "    if corpus:\n",
        "        tpcs, doc_topic, terms = lda_topics(corpus, n_topics=5, n_top_words=10)\n",
        "        topics_summary[\"groups\"][subset] = tpcs if tpcs is not None else []\n",
        "        if tpcs:\n",
        "            for i, words in enumerate(tpcs, 1):\n",
        "                topic_rows.append({\"Group\":subset, \"Topic\":i, \"Top_Words\":\", \".join(words)})\n",
        "            topic_means = doc_topic.mean(axis=0) if doc_topic is not None else None\n",
        "            if topic_means is not None:\n",
        "                for i, val in enumerate(topic_means, 1):\n",
        "                    topicmix_rows.append({\"Group\":subset, \"Topic\":i, \"Mean_Weight\":float(val)})\n",
        "        print(f\"  Topics generated for {subset}: {len(tpcs) if tpcs else 0}\")\n",
        "    else:\n",
        "        topics_summary[\"groups\"][subset] = []\n",
        "        print(f\"  Not enough text for {subset}\")\n",
        "\n",
        "topics_json_path = os.path.join(out_dir, f\"lda_topics_by_subset_{ts}.json\")\n",
        "with open(topics_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(topics_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "topics_csv = pd.DataFrame(topic_rows, columns=[\"Group\",\"Topic\",\"Top_Words\"])\n",
        "topics_csv_path = os.path.join(out_dir, f\"lda_topics_by_subset_{ts}.csv\")\n",
        "topics_csv.to_csv(topics_csv_path, index=False)\n",
        "\n",
        "topicmix_csv = pd.DataFrame(topicmix_rows, columns=[\"Group\",\"Topic\",\"Mean_Weight\"])\n",
        "topicmix_csv_path = os.path.join(out_dir, f\"lda_topic_mix_by_subset_{ts}.csv\")\n",
        "topicmix_csv.to_csv(topicmix_csv_path, index=False)\n",
        "\n",
        "# ---------- 6) Optional robustness: 3-way χ² (drop NLP group) ----------\n",
        "try:\n",
        "    cont_3 = contingency_4way.drop(columns=[\"Joyce_LessRestrictive\"])\n",
        "    chi2_3, p_3, dof_3, _ = chi2_contingency(cont_3, correction=False)\n",
        "    V_3 = cramers_v(chi2_3, cont_3.values.sum(), *cont_3.shape)\n",
        "    print(f\"\\n3-way χ² (Manual/Restrictive/BNC): χ²={chi2_3:.2f} df={dof_3} p={p_3:.3g} V={V_3:.3f}\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------- 7) Master summary JSON ----------\n",
        "master = {\n",
        "    \"generated_at\": ts,\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "    \"note\": \"By-subset outputs (Manual / Restrictive / Less-Restrictive vs BNC) with unified Quasi_Similes label and FDR corrections.\",\n",
        "    \"chi_square_4way\": {\n",
        "        \"chi2\": float(chi2_4), \"dof\": int(dof_4), \"p_value\": float(p_4),\n",
        "        \"cramers_v\": float(V_4), \"monte_carlo\": bool(sim_used),\n",
        "        \"N\": int(N_total)\n",
        "    },\n",
        "    \"files\": {\n",
        "        \"chi2_contingency_by_subset_csv\": path_cont_4,\n",
        "        \"chi2_expected_by_subset_csv\": path_exp_4,\n",
        "        \"chi2_pearson_z_by_subset_csv\": path_resid_z,\n",
        "        \"chi2_cell_p_by_subset_csv\": path_resid_p,\n",
        "        \"chi2_cell_padj_BH_by_subset_csv\": path_resid_padj,\n",
        "        \"two_prop_newcombe_by_subset_csv\": path_two_prop,\n",
        "        \"binomial_tests_by_subset_csv\": path_binom,\n",
        "        \"continuous_tests_by_subset_csv\": path_cont,\n",
        "        \"lda_topics_by_subset_json\": topics_json_path,\n",
        "        \"lda_topics_by_subset_csv\": topics_csv_path,\n",
        "        \"lda_topic_mix_by_subset_csv\": topicmix_csv_path\n",
        "    }\n",
        "}\n",
        "master_path = os.path.join(out_dir, f\"stats_and_topics_summary_by_subset_{ts}.json\")\n",
        "with open(master_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(master, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nSAVED OUTPUTS (by-subset)\")\n",
        "print(\" - 4-way contingency:\", master[\"files\"][\"chi2_contingency_by_subset_csv\"])\n",
        "print(\" - 4-way expected:\", master[\"files\"][\"chi2_expected_by_subset_csv\"])\n",
        "print(\" - 4-way Pearson z:\", master[\"files\"][\"chi2_pearson_z_by_subset_csv\"])\n",
        "print(\" - 4-way cell p-values:\", master[\"files\"][\"chi2_cell_p_by_subset_csv\"])\n",
        "print(\" - 4-way cell p-values (BH):\", master[\"files\"][\"chi2_cell_padj_BH_by_subset_csv\"])\n",
        "print(\" - Two-proportion (subset vs BNC):\", master[\"files\"][\"two_prop_newcombe_by_subset_csv\"])\n",
        "print(\" - Binomial (subset vs BNC):\", master[\"files\"][\"binomial_tests_by_subset_csv\"])\n",
        "print(\" - Continuous tests (subset vs BNC):\", master[\"files\"][\"continuous_tests_by_subset_csv\"])\n",
        "print(\" - Topics JSON (per subset):\", master[\"files\"][\"lda_topics_by_subset_json\"])\n",
        "print(\" - Topics CSV (per subset):\", master[\"files\"][\"lda_topics_by_subset_csv\"])\n",
        "print(\" - Topic mix CSV (per subset):\", master[\"files\"][\"lda_topic_mix_by_subset_csv\"])\n",
        "print(\" - Master summary JSON:\", master_path)\n",
        "print(\"\\nDONE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFsJ5blk0D6b",
        "outputId": "a9bc5e98-5828-4296-a06f-4a9b3ea4c36c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROBUST STATISTICAL ANALYSIS (Joyce subsets vs BNC)\n",
            "===========================================================================\n",
            "\n",
            "Category distribution after harmonisation (counts):\n",
            "Original_Dataset     BNC_Baseline  Manual_CloseReading  \\\n",
            "Category_Framework                                       \n",
            "Standard                      118                   93   \n",
            "Quasi_Similes                  82                   53   \n",
            "Joycean_Quasi_Fuzzy             0                   13   \n",
            "Joycean_Framed                  0                   18   \n",
            "Joycean_Silent                  0                    6   \n",
            "Uncategorized                   0                    1   \n",
            "\n",
            "Original_Dataset     NLP_LessRestrictive_PG  Restrictive_Dubliners  Total  \n",
            "Category_Framework                                                         \n",
            "Standard                                330                    150    691  \n",
            "Quasi_Similes                             0                     47    182  \n",
            "Joycean_Quasi_Fuzzy                       0                     14     27  \n",
            "Joycean_Framed                            0                      4     22  \n",
            "Joycean_Silent                            0                      3      9  \n",
            "Uncategorized                             0                      0      1  \n",
            "Joyce_Manual          : 184 rows\n",
            "Joyce_Restrictive     : 218 rows\n",
            "Joyce_LessRestrictive : 330 rows\n",
            "BNC                   : 200 rows\n",
            "\n",
            "4-way Chi-square on Category_Framework (Joyce subsets vs BNC):\n",
            "χ² = 281.8806 | df = 15 | p = 0.000000 | Cramér’s V = 0.318 | Monte-Carlo=False\n",
            "\n",
            "Top 10 standardized residuals (|z|):\n",
            "  Joyce_LessRestrictive  | Quasi_Similes         z=  8.03  obs=0 exp=64.4\n",
            "  BNC                    | Quasi_Similes         z=  6.87  obs=82 exp=39.1\n",
            "  Joyce_Manual           | Joycean_Framed        z=  6.55  obs=18 exp=4.3\n",
            "  Joyce_LessRestrictive  | Standard              z=  5.46  obs=330 exp=244.7\n",
            "  Joyce_Manual           | Standard              z=  3.72  obs=93 exp=136.4\n",
            "  Joyce_Manual           | Joycean_Quasi_Fuzzy   z=  3.32  obs=13 exp=5.3\n",
            "  Joyce_Manual           | Joycean_Silent        z=  3.17  obs=6 exp=1.8\n",
            "  Joyce_LessRestrictive  | Joycean_Quasi_Fuzzy   z=  3.09  obs=0 exp=9.6\n",
            "  Joyce_Restrictive      | Joycean_Quasi_Fuzzy   z=  3.06  obs=14 exp=6.3\n",
            "  Joyce_Manual           | Quasi_Similes         z=  2.85  obs=53 exp=35.9\n",
            "\n",
            "Two-proportion tests (Newcombe–Wilson) for each Joyce subset vs BNC:\n",
            "  Joyce_Manual           | Joycean_Framed       z= 4.531 p=5.87825e-06 CI[0.058,0.149] h=0.636\n",
            "  Joyce_Manual           | Joycean_Quasi_Fuzzy  z= 3.824 p=0.000131123 CI[0.036,0.117] h=0.538\n",
            "  Joyce_Manual           | Joycean_Silent       z= 2.574 p=0.0100543 CI[0.007,0.069] h=0.363\n",
            "  Joyce_Manual           | Quasi_Similes        z=-2.501 p=0.0124016 CI[-0.214,-0.026] h=-0.257\n",
            "  Joyce_Manual           | Standard             z=-1.664 p=0.0961402 CI[-0.182,0.015] h=-0.170\n",
            "  Joyce_Manual           | Uncategorized        z= 1.044 p=0.296517 CI[-0.014,0.030] h=0.148\n",
            "  Joyce_Restrictive      | Joycean_Framed       z= 1.925 p=0.0542438 CI[-0.004,0.046] h=0.272\n",
            "  Joyce_Restrictive      | Joycean_Quasi_Fuzzy  z= 3.645 p=0.00026695 CI[0.032,0.105] h=0.512\n",
            "  Joyce_Restrictive      | Joycean_Silent       z= 1.665 p=0.0959149 CI[-0.007,0.040] h=0.235\n",
            "  Joyce_Restrictive      | Quasi_Similes        z=-4.298 p=1.72149e-05 CI[-0.279,-0.106] h=-0.424\n",
            "  Joyce_Restrictive      | Standard             z= 2.088 p=0.0367809 CI[0.006,0.188] h=0.205\n",
            "  Joyce_Restrictive      | Uncategorized        (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Joycean_Framed       (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Joycean_Quasi_Fuzzy  (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Joycean_Silent       (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Quasi_Similes        z=-12.652 p=1.09523e-36 CI[-0.479,-0.343] h=-1.390\n",
            "  Joyce_LessRestrictive  | Standard             z=12.652 p=1.09523e-36 CI[0.343,0.479] h=1.390\n",
            "  Joyce_LessRestrictive  | Uncategorized        (both groups at boundary → skip) h=0.000\n",
            "\n",
            "Binomial tests (each Joyce subset vs BNC category proportion):\n",
            "  Joyce_Manual           | Quasi_Similes        53/184 vs p_ref=0.4100 p=0.00070983\n",
            "  Joyce_Manual           | Standard             93/184 vs p_ref=0.5900 p=0.0242733\n",
            "  Joyce_Restrictive      | Quasi_Similes        47/218 vs p_ref=0.4100 p=1.9121e-09\n",
            "  Joyce_Restrictive      | Standard             150/218 vs p_ref=0.5900 p=0.00303334\n",
            "  Joyce_LessRestrictive  | Quasi_Similes        0/330 vs p_ref=0.4100 p=3.46102e-76\n",
            "  Joyce_LessRestrictive  | Standard             330/330 vs p_ref=0.5900 p=3.46102e-76\n",
            "\n",
            "Continuous features (Welch t + Mann–Whitney U) each Joyce subset vs BNC:\n",
            "  Sentence_Length        | Joyce_Manual           t=  2.804 p=0.00541318 | U=  20123.5 p=0.112632 | g=0.293 δ=0.094\n",
            "  Sentence_Length        | Joyce_Restrictive      t=  1.806 p=0.0716264 | U=  23076.0 p=0.301001 | g=0.175 δ=0.059\n",
            "  Sentence_Length        | Joyce_LessRestrictive  t=  1.731 p=0.0841489 | U=  34228.5 p=0.472192 | g=0.145 δ=0.037\n",
            "  Pre_Post_Ratio         | Joyce_Manual           t=  1.309 p=0.191305 | U=  20310.0 p=0.0786362 | g=0.133 δ=0.104\n",
            "  Pre_Post_Ratio         | Joyce_Restrictive      t=  0.251 p=0.802209 | U=  21925.0 p=0.919566 | g=0.025 δ=0.006\n",
            "  Pre_Post_Ratio         | Joyce_LessRestrictive  t=  0.540 p=0.589421 | U=  33247.5 p=0.657815 | g=0.049 δ=0.023\n",
            "  Sentiment_Polarity     | Joyce_Manual           t= -0.825 p=0.40996 | U=  17385.5 p=0.343227 | g=-0.084 δ=-0.055\n",
            "  Sentiment_Polarity     | Joyce_Restrictive      t= -0.634 p=0.526346 | U=  20806.5 p=0.414288 | g=-0.062 δ=-0.046\n",
            "  Sentiment_Polarity     | Joyce_LessRestrictive  t=  0.510 p=0.610387 | U=  33618.5 p=0.714242 | g=0.046 δ=0.019\n",
            "  Sentiment_Subjectivity | Joyce_Manual           t= -0.308 p=0.757975 | U=  18113.5 p=0.790903 | g=-0.031 δ=-0.016\n",
            "  Sentiment_Subjectivity | Joyce_Restrictive      t= -0.374 p=0.708807 | U=  21360.5 p=0.719938 | g=-0.036 δ=-0.020\n",
            "  Sentiment_Subjectivity | Joyce_LessRestrictive  t= -0.664 p=0.507057 | U=  31990.0 p=0.551874 | g=-0.059 δ=-0.031\n",
            "\n",
            "Comparator-segment statistics (Pre / Between / Post)\n",
            "  Using segments file: comparator_segments_latest.csv\n",
            "  Features available: ['Seg_Pre', 'Seg_Between', 'Seg_Post', 'Seg_Total', 'Seg_Pre_Ratio', 'Seg_Between_Ratio', 'Seg_Post_Ratio', 'Seg_Comp_Count']\n",
            "  Seg_Pre              | Joyce_Manual           t=  1.504 p=0.133 | U=  21444.5 p=0.00501 | g=0.152 δ=0.165\n",
            "  Seg_Pre              | Joyce_LessRestrictive  t= -0.373 p=0.709 | U=  11130.5 p=0.25 | g=-0.040 δ=0.081\n",
            "  Seg_Between          | Joyce_Manual           t=  2.878 p=0.00438 | U=  20211.5 p=0.0436 | g=0.303 δ=0.098\n",
            "  Seg_Between          | Joyce_LessRestrictive  t=  1.233 p=0.219 | U=  10438.5 p=0.809 | g=0.164 δ=0.013\n",
            "  Seg_Post             | Joyce_Manual           t=  0.830 p=0.407 | U=  17984.0 p=0.702 | g=0.086 δ=-0.023\n",
            "  Seg_Post             | Joyce_LessRestrictive  t=  1.524 p=0.129 | U=  11524.0 p=0.0897 | g=0.194 δ=0.119\n",
            "  Seg_Total            | Joyce_Manual           t=  3.062 p=0.00242 | U=  20653.0 p=0.0381 | g=0.320 δ=0.122\n",
            "  Seg_Total            | Joyce_LessRestrictive  t=  1.436 p=0.153 | U=  11139.0 p=0.245 | g=0.180 δ=0.081\n",
            "  Seg_Pre_Ratio        | Joyce_Manual           t=  1.144 p=0.254 | U=  19638.0 p=0.255 | g=0.116 δ=0.067\n",
            "  Seg_Pre_Ratio        | Joyce_LessRestrictive  t= -0.433 p=0.665 | U=  10181.0 p=0.87 | g=-0.048 δ=-0.012\n",
            "  Seg_Between_Ratio    | Joyce_Manual           t=  1.914 p=0.0564 | U=  20015.0 p=0.0721 | g=0.197 δ=0.088\n",
            "  Seg_Between_Ratio    | Joyce_LessRestrictive  t=  0.355 p=0.723 | U=  10315.0 p=0.98 | g=0.044 δ=0.001\n",
            "  Seg_Post_Ratio       | Joyce_Manual           t= -2.858 p=0.00449 | U=  15619.0 p=0.0105 | g=-0.290 δ=-0.151\n",
            "  Seg_Post_Ratio       | Joyce_LessRestrictive  t=  0.101 p=0.92 | U=  10737.0 p=0.546 | g=0.011 δ=0.042\n",
            "  Seg_Comp_Count       | Joyce_Manual           t=  0.365 p=0.715 | U=  18962.0 p=0.546 | g=0.037 δ=0.031\n",
            "  Seg_Comp_Count       | Joyce_LessRestrictive  t= -2.772 p=0.00602 | U=   8701.0 p=0.00875 | g=-0.322 δ=-0.155\n",
            "\n",
            "Between-clause presence × Group (χ²):\n",
            "                       Between>0  Between=0\n",
            "__Group__                                  \n",
            "Joyce_Manual                  66        118\n",
            "Joyce_Restrictive              0        218\n",
            "Joyce_LessRestrictive         28        302\n",
            "BNC                           56        144\n",
            "  χ² = 130.242 | df = 3 | p = 0.000000 | Cramér’s V = 0.374\n",
            "\n",
            "TOPIC MODELLING (per subset + BNC)\n",
            "  Topics generated for Joyce_Manual: 5\n",
            "  Topics generated for Joyce_Restrictive: 5\n",
            "  Topics generated for Joyce_LessRestrictive: 5\n",
            "  Topics generated for BNC: 5\n",
            "\n",
            "3-way χ² (Manual/Restrictive/BNC): χ²=69.65 df=10 p=5.18e-11 V=0.241\n",
            "\n",
            "SAVED OUTPUTS (by-subset)\n",
            " - 4-way contingency: analysis_outputs/chi2_contingency_by_subset_20250825_110434.csv\n",
            " - 4-way expected: analysis_outputs/chi2_expected_by_subset_20250825_110434.csv\n",
            " - 4-way Pearson z: analysis_outputs/chi2_pearson_z_by_subset_20250825_110434.csv\n",
            " - 4-way cell p-values: analysis_outputs/chi2_cell_p_by_subset_20250825_110434.csv\n",
            " - 4-way cell p-values (BH): analysis_outputs/chi2_cell_padj_BH_by_subset_20250825_110434.csv\n",
            " - Two-proportion (subset vs BNC): analysis_outputs/two_prop_newcombe_by_subset_20250825_110434.csv\n",
            " - Binomial (subset vs BNC): analysis_outputs/binomial_tests_by_subset_20250825_110434.csv\n",
            " - Continuous tests (subset vs BNC): analysis_outputs/continuous_tests_by_subset_20250825_110434.csv\n",
            " - Topics JSON (per subset): analysis_outputs/lda_topics_by_subset_20250825_110434.json\n",
            " - Topics CSV (per subset): analysis_outputs/lda_topics_by_subset_20250825_110434.csv\n",
            " - Topic mix CSV (per subset): analysis_outputs/lda_topic_mix_by_subset_20250825_110434.csv\n",
            " - Master summary JSON: analysis_outputs/stats_and_topics_summary_by_subset_20250825_110434.json\n",
            "\n",
            "DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Academic Reporting and Documentation\n",
        "\n",
        "## 8.1 Professional Report Generation\n",
        "The HTML report generator produces a polished, self-contained document suitable for:\n",
        "- **Peer review** and journal **supplementary materials**\n",
        "- Internal/external **research documentation** and **reproducibility** records\n",
        "- **Academic presentations** (slides/posters) and dissemination to non-specialists\n",
        "\n",
        "It compiles results directly from the timestamped CSV/JSON artifacts written to `analysis_outputs/`, ensuring the report is traceable to concrete files.\n",
        "\n",
        "## 8.2 Results Integration\n",
        "The report synthesizes all major components of the analysis, including:\n",
        "- **Instance-aligned F1** summaries (exact + fuzzy sentence matching) for extractor vs manual annotations\n",
        "- **4-way χ²** contingency results with **expected counts**, **standardized residuals (z)**, and **BH-FDR** per-cell p-values\n",
        "- **Two-proportion tests** (Newcombe CIs) with **Cohen’s h** effect sizes, BH-FDR corrected\n",
        "- **Binomial tests** vs BNC reference proportions, BH-FDR corrected\n",
        "- **Continuous feature** comparisons (Welch *t*, Mann–Whitney *U*) with **Hedges’ g**, **Cliff’s δ**, and optional **Hodges–Lehmann** shifts (bootstrap CIs)\n",
        "- **Topic modelling** per subset (LDA top words) and **mean topic-mix** weights\n",
        "- **Dataset and category overviews**, reflecting the **harmonised taxonomy** with **Quasi_Similes** as the unified label\n",
        "\n",
        "## 8.3 Academic Standards and Transparency\n",
        "The report follows scholarly communication norms:\n",
        "- Clear sectioning, professional typography, descriptive figure/table captions\n",
        "- Explicit **statistical choices** (e.g., BH-FDR; Monte-Carlo χ² fallback when expected counts are low)\n",
        "- **Environment stamping** (Python/library versions), deterministic seeds, and a **file manifest** with timestamps for reproducibility\n",
        "- **Taxonomy harmonisation** note (Joycean_Quasi → **Quasi_Similes**) to prevent label inflation\n",
        "- Methodological caveats (e.g., **TextBlob sentiment** reported as exploratory; the **less-restrictive NLP** subset being **all Standard** is treated as a control and excluded in robustness checks when appropriate)\n",
        "\n",
        "## 8.4 Reproducibility & Data Availability (recommended)\n",
        "- Archive the full `analysis_outputs/` directory, `comprehensive_linguistic_analysis_corrected.csv`, and this notebook.\n",
        "- Include licensing/usage notes for external corpora (e.g., **BNC** access terms) and cite the **Project Gutenberg** source for *Dubliners*.\n",
        "- Provide a short **README** describing how to re-run Cells 1–3 and regenerate the identical HTML using the saved artifacts.\n"
      ],
      "metadata": {
        "id": "RwoglydbeaCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ACADEMIC HTML REPORT GENERATOR — UPDATED (data-driven, harmonised labels)\n",
        "# Now also includes comparator-segment metrics (Pre / Between / Post)\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "# for presence χ² (Between>0) if available\n",
        "try:\n",
        "    from scipy.stats import chi2_contingency\n",
        "    _HAS_SCIPY = True\n",
        "except Exception:\n",
        "    _HAS_SCIPY = False\n",
        "\n",
        "print(\"GENERATING ACADEMIC HTML REPORT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Timestamp for report\n",
        "now = datetime.now()\n",
        "report_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "report_date = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def latest_file(pattern, base=\"analysis_outputs\"):\n",
        "    files = glob.glob(os.path.join(base, pattern))\n",
        "    return max(files, key=os.path.getctime) if files else None\n",
        "\n",
        "def safe_read_csv(path):\n",
        "    if not path or not os.path.exists(path):\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load CSV {path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def safe_read_json(path):\n",
        "    if not path or not os.path.exists(path):\n",
        "        return {}\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load JSON {path}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def df_for_html(df, index=False, max_rows=None):\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "    d = df.copy()\n",
        "    if max_rows is not None and len(d) > max_rows:\n",
        "        d = d.head(max_rows)\n",
        "        d.__truncated__ = True\n",
        "    return d\n",
        "\n",
        "def create_table_html(df, title=\"\", max_rows=20, index=False):\n",
        "    \"\"\"Create HTML table with styling; auto-handle empty and truncation note.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return f\"<p><em>No data available for {title}</em></p>\"\n",
        "    d = df.copy()\n",
        "    trunc = False\n",
        "    if max_rows and len(d) > max_rows:\n",
        "        d = d.head(max_rows)\n",
        "        trunc = True\n",
        "    # If the index is meaningful (named or not default RangeIndex), show it as a column\n",
        "    if index:\n",
        "        d = d.reset_index()\n",
        "    table_html = d.to_html(classes='analysis-table', escape=False, index=False)\n",
        "    note = f\"<p class='truncated-note'><em>Showing first {max_rows} of {len(df)} rows</em></p>\" if trunc else \"\"\n",
        "    return f\"\"\"\n",
        "    <div class=\"table-container\">\n",
        "        <h4>{title}</h4>\n",
        "        <div class=\"table-wrapper\">{table_html}</div>\n",
        "        {note}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# ---------- Pull in analysis outputs from Cell 2 ----------\n",
        "out_dir = \"analysis_outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "paths = {\n",
        "    \"chi2_cont\": latest_file(\"chi2_contingency_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_exp\": latest_file(\"chi2_expected_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_z\": latest_file(\"chi2_pearson_z_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_p\": latest_file(\"chi2_cell_p_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_padj\": latest_file(\"chi2_cell_padj_BH_by_subset_*.csv\", out_dir),\n",
        "    \"two_prop\": latest_file(\"two_prop_newcombe_by_subset_*.csv\", out_dir),\n",
        "    \"binom\": latest_file(\"binomial_tests_by_subset_*.csv\", out_dir),\n",
        "    \"cont\": latest_file(\"continuous_tests_by_subset_*.csv\", out_dir),\n",
        "    \"topics\": latest_file(\"lda_topics_by_subset_*.csv\", out_dir),\n",
        "    \"topicmix\": latest_file(\"lda_topic_mix_by_subset_*.csv\", out_dir),\n",
        "    \"hl\": latest_file(\"continuous_HL_shifts_*.csv\", out_dir),\n",
        "    \"master\": latest_file(\"stats_and_topics_summary_by_subset_*.json\", out_dir),\n",
        "    # NEW: comparator-segment outputs\n",
        "    \"seg_cont\": latest_file(\"continuous_segment_tests_by_subset_*.csv\", out_dir),\n",
        "    \"seg_presence\": latest_file(\"between_presence_by_group_*.csv\", out_dir),\n",
        "    \"seg_presence_exp\": latest_file(\"between_presence_expected_by_group_*.csv\", out_dir),\n",
        "    \"seg_presence_resid\": latest_file(\"between_presence_residuals_by_group_*.csv\", out_dir),\n",
        "    # convenience (optional merge for group means)\n",
        "    \"seg_latest\": \"comparator_segments_latest.csv\"\n",
        "}\n",
        "\n",
        "chi2_cont_df = safe_read_csv(paths[\"chi2_cont\"])\n",
        "chi2_exp_df  = safe_read_csv(paths[\"chi2_exp\"])\n",
        "chi2_z_df    = safe_read_csv(paths[\"chi2_z\"])\n",
        "chi2_p_df    = safe_read_csv(paths[\"chi2_p\"])\n",
        "chi2_padj_df = safe_read_csv(paths[\"chi2_padj\"])\n",
        "two_prop_df  = safe_read_csv(paths[\"two_prop\"])\n",
        "binom_df     = safe_read_csv(paths[\"binom\"])\n",
        "cont_df      = safe_read_csv(paths[\"cont\"])\n",
        "topics_df    = safe_read_csv(paths[\"topics\"])\n",
        "topicmix_df  = safe_read_csv(paths[\"topicmix\"])\n",
        "hl_df        = safe_read_csv(paths[\"hl\"])\n",
        "master_json  = safe_read_json(paths[\"master\"])\n",
        "\n",
        "# NEW: segment metrics\n",
        "seg_cont_df       = safe_read_csv(paths[\"seg_cont\"])\n",
        "seg_presence_df   = safe_read_csv(paths[\"seg_presence\"])\n",
        "seg_presence_exp  = safe_read_csv(paths[\"seg_presence_exp\"])\n",
        "seg_presence_res  = safe_read_csv(paths[\"seg_presence_resid\"])\n",
        "seg_latest_df     = safe_read_csv(paths[\"seg_latest\"])\n",
        "\n",
        "# ---------- Summaries from Cell 1 (results_df, f1_analysis, comparator.env_info) ----------\n",
        "def create_summary_stats_html():\n",
        "    if 'results_df' not in globals() or results_df is None or results_df.empty:\n",
        "        return \"<p><em>No results data available</em></p>\"\n",
        "    dcounts = results_df['Original_Dataset'].value_counts()\n",
        "    ccounts = results_df['Category_Framework'].value_counts()\n",
        "\n",
        "    items_d = \"\".join([f\"<li><strong>{k}:</strong> {int(v):,} instances</li>\" for k,v in dcounts.items()])\n",
        "    items_c = \"\".join([f\"<li><strong>{k}:</strong> {int(v):,} ({(v/len(results_df))*100:.1f}%)</li>\" for k,v in ccounts.items()])\n",
        "\n",
        "    return f\"\"\"\n",
        "    <div class=\"summary-stats\">\n",
        "        <div class=\"stat-group\">\n",
        "            <h4>Dataset Distribution</h4>\n",
        "            <ul>{items_d}</ul>\n",
        "            <p><strong>Total Instances:</strong> {len(results_df):,}</p>\n",
        "        </div>\n",
        "        <div class=\"stat-group\">\n",
        "            <h4>Category Distribution</h4>\n",
        "            <ul>{items_c}</ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def f1_summary_html():\n",
        "    # Try multiple sources\n",
        "    micro_rb = macro_rb = micro_nlp = macro_nlp = None\n",
        "    pairs_rb = pairs_nlp = None\n",
        "    if 'f1_analysis' in globals() and isinstance(f1_analysis, dict):\n",
        "        rb = f1_analysis.get('rule_based_vs_manual') or f1_analysis.get('Rule-Based vs Manual')\n",
        "        nl = f1_analysis.get('nlp_vs_manual') or f1_analysis.get('NLP vs Manual')\n",
        "        if rb and rb.get(\"overall\"):\n",
        "            micro_rb = rb[\"overall\"].get(\"micro_f1\")\n",
        "            macro_rb = rb[\"overall\"].get(\"macro_f1\")\n",
        "            pairs_rb = rb.get(\"pairs\")\n",
        "        if nl and nl.get(\"overall\"):\n",
        "            micro_nlp = nl[\"overall\"].get(\"micro_f1\")\n",
        "            macro_nlp = nl[\"overall\"].get(\"macro_f1\")\n",
        "            pairs_nlp = nl.get(\"pairs\")\n",
        "    elif 'comparator' in globals():\n",
        "        try:\n",
        "            fa = comparator.comparison_results.get('f1_analysis', {})\n",
        "            rb = fa.get('rule_based_vs_manual')\n",
        "            nl = fa.get('nlp_vs_manual')\n",
        "            if rb and rb.get(\"overall\"):\n",
        "                micro_rb = rb[\"overall\"].get(\"micro_f1\")\n",
        "                macro_rb = rb[\"overall\"].get(\"macro_f1\")\n",
        "                pairs_rb = rb.get(\"pairs\")\n",
        "            if nl and nl.get(\"overall\"):\n",
        "                micro_nlp = nl[\"overall\"].get(\"micro_f1\")\n",
        "                macro_nlp = nl[\"overall\"].get(\"macro_f1\")\n",
        "                pairs_nlp = nl.get(\"pairs\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if micro_rb is None and micro_nlp is None:\n",
        "        return \"<p><em>F1 metrics unavailable (run Cell 1 in this session).</em></p>\"\n",
        "\n",
        "    lines = []\n",
        "    if micro_rb is not None:\n",
        "        lines.append(f\"• Rule-Based vs Manual: micro-F1 = {micro_rb:.3f}, macro-F1 = {macro_rb:.3f}{' (pairs=' + str(pairs_rb) + ')' if pairs_rb else ''}\")\n",
        "    if micro_nlp is not None:\n",
        "        lines.append(f\"• NLP vs Manual: micro-F1 = {micro_nlp:.3f}, macro-F1 = {macro_nlp:.3f}{' (pairs=' + str(pairs_nlp) + ')' if pairs_nlp else ''}\")\n",
        "\n",
        "    return f\"\"\"\n",
        "    <div class=\"highlight\">\n",
        "      <strong>F1 Summary:</strong><br>\n",
        "      {'<br>'.join(lines)}\n",
        "      <br>• Total instances processed: {len(results_df):,} (across all datasets)\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def env_html():\n",
        "    info = {}\n",
        "    if 'comparator' in globals():\n",
        "        try:\n",
        "            info = comparator.env_info\n",
        "        except Exception:\n",
        "            info = {}\n",
        "    if not info:\n",
        "        return \"\"\n",
        "    items = \"\".join([f\"<li><strong>{k}:</strong> {v}</li>\" for k,v in info.items()])\n",
        "    return f\"\"\"\n",
        "    <div class=\"methodology\">\n",
        "      <strong>Environment:</strong>\n",
        "      <ul>{items}</ul>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# ---------- Analytic highlight snippets ----------\n",
        "def chi2_summary_html():\n",
        "    if not master_json:\n",
        "        return \"\"\n",
        "    chi = master_json.get(\"chi_square_4way\", {})\n",
        "    chi_line = (f\"χ² = {chi.get('chi2', float('nan')):.3f} | df = {chi.get('dof', 0)} | \"\n",
        "                f\"p = {chi.get('p_value', float('nan')):.3g} | Cramér’s V = {chi.get('cramers_v', float('nan')):.3f} \"\n",
        "                f\"| Monte-Carlo={chi.get('monte_carlo', False)} | N={chi.get('N', 0)}\")\n",
        "    return f\"<p>{chi_line}</p>\"\n",
        "\n",
        "def top_residuals_html(k=10):\n",
        "    if chi2_z_df.empty or chi2_cont_df.empty or chi2_exp_df.empty:\n",
        "        return \"\"\n",
        "    # Melt z\n",
        "    z_long = chi2_z_df.copy()\n",
        "    z_long = z_long.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in z_long.columns else z_long\n",
        "    z_long = z_long.melt(id_vars=[c for c in [\"Category_Framework\"] if c in z_long.columns],\n",
        "                         var_name=\"Group\", value_name=\"z\")\n",
        "    z_long[\"abs_z\"] = z_long[\"z\"].abs()\n",
        "    # Get obs/exp\n",
        "    cont = chi2_cont_df.copy()\n",
        "    cont = cont.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in cont.columns else cont\n",
        "    exp = chi2_exp_df.copy()\n",
        "    exp = exp.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in exp.columns else exp\n",
        "    # Align to long\n",
        "    obs_long = cont.melt(id_vars=[c for c in [\"Category_Framework\"] if c in cont.columns],\n",
        "                         var_name=\"Group\", value_name=\"obs\")\n",
        "    exp_long = exp.melt(id_vars=[c for c in [\"Category_Framework\"] if c in exp.columns],\n",
        "                        var_name=\"Group\", value_name=\"exp\")\n",
        "    m = (z_long.merge(obs_long, on=[\"Category_Framework\",\"Group\"], how=\"left\")\n",
        "               .merge(exp_long, on=[\"Category_Framework\",\"Group\"], how=\"left\"))\n",
        "    m = m.sort_values(\"abs_z\", ascending=False).head(k)\n",
        "    return create_table_html(m, f\"Top {k} standardized residuals (|z|)\", max_rows=k)\n",
        "\n",
        "def sig_two_prop_html(alpha=0.05, top=15):\n",
        "    if two_prop_df.empty or \"p_adj_BH\" not in two_prop_df.columns:\n",
        "        return \"\"\n",
        "    df = two_prop_df.copy()\n",
        "    # Keep only significant rows, order by adjusted p then effect size magnitude\n",
        "    df[\"|h|\"] = df.get(\"cohens_h\", 0).abs()\n",
        "    sig = df[df[\"p_adj_BH\"] < alpha].sort_values([\"p_adj_BH\",\"|h|\"])\n",
        "    cols = [c for c in [\"Subset\",\"Category\",\"prop_A\",\"prop_B\",\"z\",\"p_value\",\"p_adj_BH\",\"cohens_h\"] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Two-proportion (BH<{alpha}) — top {top}\", max_rows=top)\n",
        "\n",
        "def sig_binom_html(alpha=0.05, top=15):\n",
        "    if binom_df.empty or \"p_adj_BH\" not in binom_df.columns:\n",
        "        return \"\"\n",
        "    df = binom_df.copy()\n",
        "    sig = df[df[\"p_adj_BH\"] < alpha].sort_values(\"p_adj_BH\")\n",
        "    cols = [c for c in [\"Subset\",\"Category\",\"count_A\",\"n_A\",\"p_ref_BNC\",\"p_value\",\"p_adj_BH\"] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Binomial vs BNC (BH<{alpha}) — top {top}\", max_rows=top)\n",
        "\n",
        "def sig_continuous_html(alpha=0.05):\n",
        "    if cont_df.empty:\n",
        "        return \"\"\n",
        "    d = cont_df.copy()\n",
        "    # Prefer Mann–Whitney U adjusted p; fall back to Welch t\n",
        "    if \"U_padj_BH\" in d.columns:\n",
        "        sig = d[d[\"U_padj_BH\"] < alpha].sort_values(\"U_padj_BH\")\n",
        "        label = \"Mann–Whitney U (BH)\"\n",
        "        padj_col = \"U_padj_BH\"\n",
        "        p_col = \"U_pvalue\"\n",
        "    elif \"t_padj_BH\" in d.columns:\n",
        "        sig = d[d[\"t_padj_BH\"] < alpha].sort_values(\"t_padj_BH\")\n",
        "        label = \"Welch t (BH)\"\n",
        "        padj_col = \"t_padj_BH\"\n",
        "        p_col = \"t_pvalue\"\n",
        "    else:\n",
        "        return \"\"\n",
        "    if sig.empty:\n",
        "        return \"<p><em>No continuous feature differences were significant after FDR correction.</em></p>\"\n",
        "    cols = [c for c in [\"Feature\",\"Subset\",\"A_n\",\"B_n\",\"A_mean\",\"B_mean\",\"A_median\",\"B_median\",\n",
        "                        \"t_stat\",\"t_pvalue\",\"U_stat\",\"U_pvalue\",\"hedges_g\",\"cliffs_delta\", padj_col] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Continuous features — significant by {label}\", max_rows=15)\n",
        "\n",
        "# NEW: comparator-segment highlights for Exec Summary (optional)\n",
        "def seg_exec_highlights_html(alpha=0.05):\n",
        "    if seg_cont_df.empty:\n",
        "        return \"\"\n",
        "    d = seg_cont_df.copy()\n",
        "    # Prefer BH on U, else BH on t, else raw p on U.\n",
        "    if \"U_padj_BH\" in d.columns:\n",
        "        d[\"sig\"] = d[\"U_padj_BH\"] < alpha\n",
        "        pcol = \"U_padj_BH\"\n",
        "    elif \"t_padj_BH\" in d.columns:\n",
        "        d[\"sig\"] = d[\"t_padj_BH\"] < alpha\n",
        "        pcol = \"t_padj_BH\"\n",
        "    else:\n",
        "        if \"U_pvalue\" in d.columns:\n",
        "            d[\"sig\"] = d[\"U_pvalue\"] < alpha\n",
        "            pcol = \"U_pvalue\"\n",
        "        else:\n",
        "            return \"\"\n",
        "    lines = []\n",
        "    # Focus on Manual vs BNC for readability\n",
        "    dd = d[(d[\"Subset\"]==\"Joyce_Manual\") & (d[\"sig\"]==True)].copy()\n",
        "    for feat in [\"Seg_Between\",\"Seg_Post_Ratio\",\"Seg_Total\",\"Seg_Pre\"]:\n",
        "        if feat in dd[\"Feature\"].unique():\n",
        "            row = dd[dd[\"Feature\"]==feat].sort_values(pcol).head(1)\n",
        "            if not row.empty:\n",
        "                r = row.iloc[0]\n",
        "                a_mean = r.get(\"A_mean\", np.nan)\n",
        "                b_mean = r.get(\"B_mean\", np.nan)\n",
        "                g = r.get(\"hedges_g\", np.nan)\n",
        "                lines.append(f\"• <strong>{feat}</strong>: Manual vs BNC differs (p≈{r.get(pcol, np.nan):.3g}); \"\n",
        "                             f\"means {a_mean:.2f} vs {b_mean:.2f}, Hedges’ g≈{g:.2f}.\")\n",
        "    if not lines:\n",
        "        return \"\"\n",
        "    return f\"\"\"\n",
        "    <div class=\"highlight\">\n",
        "      <strong>Comparator-Segment Highlights:</strong><br>\n",
        "      {'<br>'.join(lines)}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# NEW: tables for segment metrics\n",
        "def seg_cont_tables_html():\n",
        "    if seg_cont_df.empty:\n",
        "        return \"\"\n",
        "    blocks = []\n",
        "    blocks.append(\"<h3>Comparator-Segment Metrics (Pre / Between / Post)</h3>\")\n",
        "    blocks.append(\"<p>Welch t and Mann–Whitney U on segment token counts and ratios (BH correction when available).</p>\")\n",
        "    blocks.append(create_table_html(seg_cont_df, \"All comparator-segment tests (subset vs BNC)\", max_rows=20))\n",
        "    # Significant only\n",
        "    d = seg_cont_df.copy()\n",
        "    label = None\n",
        "    if \"U_padj_BH\" in d.columns:\n",
        "        sig = d[d[\"U_padj_BH\"] < 0.05].sort_values(\"U_padj_BH\")\n",
        "        label = \"Mann–Whitney U (BH)\"\n",
        "    elif \"t_padj_BH\" in d.columns:\n",
        "        sig = d[d[\"t_padj_BH\"] < 0.05].sort_values(\"t_padj_BH\")\n",
        "        label = \"Welch t (BH)\"\n",
        "    else:\n",
        "        sig = pd.DataFrame()\n",
        "    if not sig.empty and label:\n",
        "        cols = [c for c in [\"Feature\",\"Subset\",\"A_n\",\"B_n\",\"A_mean\",\"B_mean\",\"A_median\",\"B_median\",\n",
        "                            \"t_stat\",\"t_pvalue\",\"U_stat\",\"U_pvalue\",\"hedges_g\",\"cliffs_delta\",\n",
        "                            \"U_padj_BH\" if \"U_padj_BH\" in sig.columns else \"t_padj_BH\"] if c in sig.columns]\n",
        "        blocks.append(create_table_html(sig[cols], f\"Significant comparator-segment tests — {label}\", max_rows=20))\n",
        "    return \"\\n\".join(blocks)\n",
        "\n",
        "def seg_presence_html():\n",
        "    if seg_presence_df.empty:\n",
        "        return \"\"\n",
        "    blocks = []\n",
        "    blocks.append(\"<h3>Between-Clause Presence × Group (χ²)</h3>\")\n",
        "    blocks.append(create_table_html(seg_presence_df, \"Observed counts: presence vs absence by group\", max_rows=10, index=True))\n",
        "    # If we have expected/residuals, include them\n",
        "    if not seg_presence_exp.empty:\n",
        "        blocks.append(create_table_html(seg_presence_exp, \"Expected counts under independence\", max_rows=10, index=True))\n",
        "    if not seg_presence_res.empty:\n",
        "        blocks.append(create_table_html(seg_presence_res, \"Pearson residuals\", max_rows=10, index=True))\n",
        "    # Compute χ² here as well (defensive)\n",
        "    if _HAS_SCIPY:\n",
        "        try:\n",
        "            tbl = seg_presence_df.copy()\n",
        "            if \"Unnamed: 0\" in tbl.columns:\n",
        "                tbl = tbl.rename(columns={\"Unnamed: 0\":\"__Group__\"})\n",
        "                tbl = tbl.set_index(\"__Group__\")\n",
        "            chi2, p, dof, exp = chi2_contingency(tbl, correction=False)\n",
        "            blocks.append(f\"<p>χ² = {chi2:.3f} | df = {dof} | p = {p:.6g}</p>\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    return \"\\n\".join(blocks)\n",
        "\n",
        "def seg_group_means_html():\n",
        "    \"\"\"\n",
        "    If we can merge comparator_segments_latest.csv to results_df via Instance_ID,\n",
        "    show group means of segment metrics (Joyce_Manual / Restrictive / Less-Restrictive / BNC).\n",
        "    \"\"\"\n",
        "    if seg_latest_df.empty or 'results_df' not in globals() or results_df is None or results_df.empty:\n",
        "        return \"\"\n",
        "    seg = seg_latest_df.copy()\n",
        "    # try to detect join key and segment columns\n",
        "    id_col = None\n",
        "    for c in [\"Instance_ID\",\"instance_id\",\"ID\",\"Id\"]:\n",
        "        if c in seg.columns:\n",
        "            id_col = c; break\n",
        "    if id_col is None or \"Instance_ID\" not in results_df.columns:\n",
        "        return \"\"\n",
        "    # build __Group__ from Original_Dataset like in stats cell\n",
        "    LABELS = {\n",
        "        \"Manual_CloseReading\":      \"Joyce_Manual\",\n",
        "        \"Restrictive_Dubliners\":    \"Joyce_Restrictive\",\n",
        "        \"NLP_LessRestrictive_PG\":   \"Joyce_LessRestrictive\",\n",
        "        \"BNC_Baseline\":             \"BNC\"\n",
        "    }\n",
        "    base = results_df[[\"Instance_ID\",\"Original_Dataset\"]].copy()\n",
        "    base[\"__Group__\"] = base[\"Original_Dataset\"].map(LABELS).fillna(base[\"Original_Dataset\"])\n",
        "    merged = base.merge(seg, left_on=\"Instance_ID\", right_on=id_col, how=\"left\")\n",
        "    seg_cols = [c for c in merged.columns if c.startswith(\"Seg_\")]  # from the stats cell\n",
        "    if not seg_cols:\n",
        "        # fallback: common names from latest CSV\n",
        "        for c_old, c_new in [(\"Pre_Tokens\",\"Seg_Pre\"),(\"Between_Tokens_Total\",\"Seg_Between\"),\n",
        "                             (\"Post_Tokens\",\"Seg_Post\")]:\n",
        "            if c_old in merged.columns and c_new not in merged.columns:\n",
        "                merged[c_new] = pd.to_numeric(merged[c_old], errors=\"coerce\")\n",
        "        seg_cols = [c for c in merged.columns if c.startswith(\"Seg_\")]\n",
        "    if not seg_cols or \"__Group__\" not in merged.columns:\n",
        "        return \"\"\n",
        "    means = merged.groupby(\"__Group__\")[seg_cols].mean(numeric_only=True).round(3)\n",
        "    return create_table_html(means, \"Comparator-segment group means (merged via Instance_ID)\", max_rows=10, index=True)\n",
        "\n",
        "# ---------- Build the HTML ----------\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/>\n",
        "<title>Joyce Simile Research: Comprehensive Linguistic Analysis Report</title>\n",
        "<style>\n",
        "    body {{\n",
        "        font-family: 'Times New Roman', serif;\n",
        "        line-height: 1.6;\n",
        "        margin: 0;\n",
        "        padding: 20px;\n",
        "        background-color: #f9f9f9;\n",
        "        color: #333;\n",
        "    }}\n",
        "    .container {{\n",
        "        max-width: 1200px;\n",
        "        margin: 0 auto;\n",
        "        background: white;\n",
        "        padding: 30px;\n",
        "        border-radius: 8px;\n",
        "        box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
        "    }}\n",
        "    .header {{\n",
        "        text-align: center;\n",
        "        border-bottom: 3px solid #2c3e50;\n",
        "        padding-bottom: 20px;\n",
        "        margin-bottom: 30px;\n",
        "    }}\n",
        "    .header h1 {{ color: #2c3e50; margin: 0; font-size: 2.2em; font-weight: bold; }}\n",
        "    .header .subtitle {{ color: #7f8c8d; font-size: 1.1em; margin: 10px 0 5px 0; font-style: italic; }}\n",
        "    .header .timestamp {{ color: #95a5a6; font-size: 0.9em; }}\n",
        "    .section {{\n",
        "        margin: 30px 0;\n",
        "        padding: 20px;\n",
        "        border-left: 4px solid #3498db;\n",
        "        background-color: #f8f9fa;\n",
        "    }}\n",
        "    .section h2 {{\n",
        "        color: #2c3e50;\n",
        "        margin-top: 0; border-bottom: 2px solid #ecf0f1; padding-bottom: 10px;\n",
        "    }}\n",
        "    .section h3 {{ color: #34495e; margin-top: 25px; }}\n",
        "    .section h4 {{ color: #5d6d7e; margin-top: 20px; margin-bottom: 10px; }}\n",
        "    .analysis-table {{ width: 100%; border-collapse: collapse; margin: 15px 0; font-size: 0.9em; }}\n",
        "    .analysis-table th {{ background:#34495e; color:#fff; padding:12px 8px; text-align:left; font-weight:bold; }}\n",
        "    .analysis-table td {{ padding:10px 8px; border-bottom:1px solid #ddd; }}\n",
        "    .analysis-table tr:nth-child(even) {{ background-color:#f2f2f2; }}\n",
        "    .analysis-table tr:hover {{ background-color:#e8f4fd; }}\n",
        "    .summary-stats {{ display:grid; grid-template-columns:1fr 1fr; gap:30px; margin:20px 0; }}\n",
        "    .stat-group {{ background:#fff; padding:20px; border-radius:6px; border:1px solid #e1e8ed; }}\n",
        "    .stat-group h4 {{ margin-top:0; color:#2c3e50; border-bottom:1px solid #ecf0f1; padding-bottom:8px; }}\n",
        "    .stat-group ul {{ list-style-type:none; padding:0; }}\n",
        "    .stat-group li {{ padding:5px 0; border-bottom:1px solid #f8f9fa; }}\n",
        "    .highlight {{ background:#d1ecf1; padding:15px; border-left:4px solid #17a2b8; margin:15px 0; }}\n",
        "    .methodology {{ background:#f8f9fa; padding:15px; border-radius:5px; margin:15px 0; font-style:italic; }}\n",
        "    .table-container {{ margin:20px 0; }}\n",
        "    .table-wrapper {{ overflow-x:auto; }}\n",
        "    .truncated-note {{ color:#6c757d; font-size:0.9em; margin-top:5px; }}\n",
        "    .footer {{ text-align:center; margin-top:40px; padding-top:20px; border-top:2px solid #ecf0f1; color:#7f8c8d; font-size:0.9em; }}\n",
        "    @media (max-width: 768px) {{\n",
        "        .summary-stats {{ grid-template-columns:1fr; }}\n",
        "        .container {{ padding:15px; }}\n",
        "        .analysis-table {{ font-size:0.85em; }}\n",
        "    }}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <div class=\"header\">\n",
        "        <h1>Joyce Simile Research</h1>\n",
        "        <div class=\"subtitle\">Comprehensive Linguistic Analysis Report</div>\n",
        "        <div class=\"subtitle\">Manual Annotations vs Extraction Methods vs BNC Baseline</div>\n",
        "        <div class=\"timestamp\">Generated on {report_timestamp}</div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Executive Summary</h2>\n",
        "        <p>This report compiles the results of a comparative analysis of simile usage in <em>Dubliners</em>, spanning manual expert annotations, a restrictive rule-based extractor, a less-restrictive NLP extractor, and a BNC baseline. Categories were harmonised with <strong>Quasi_Similes</strong> as the unified label for the Joyce/BNC quasi-simile phenomenon.</p>\n",
        "        {chi2_summary_html()}\n",
        "        {seg_exec_highlights_html(alpha=0.05)}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Dataset Overview</h2>\n",
        "        <p>Four datasets were analysed, representing complementary identification approaches and a baseline:</p>\n",
        "        {create_summary_stats_html()}\n",
        "        {env_html()}\n",
        "        <div class=\"methodology\">\n",
        "            <strong>Methodology:</strong> Features include token counts, pre/post-comparator ratio, POS distribution, syntactic complexity, TextBlob sentiment (exploratory), and comparative structure flags.\n",
        "            <br><em>New:</em> Comparator-segment metrics quantify token distributions <strong>before</strong> comparators (Pre), <strong>between</strong> multiple comparators (Between), and <strong>after</strong> comparators (Post). Between spans are computed across detected comparator anchors (e.g., <code>as … as</code>, <code>like</code>, framing punctuation); we report totals and ratios, and assess group differences with non-parametric tests and χ² for presence (Between&gt;0).\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Performance Metrics</h2>\n",
        "        <h3>Instance-Aligned F1 Scores</h3>\n",
        "        {f1_summary_html()}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Statistical Results</h2>\n",
        "        <h3>4-way Category Distribution</h3>\n",
        "\"\"\"\n",
        "\n",
        "# 4-way chi-square tables\n",
        "if not chi2_cont_df.empty:\n",
        "    html_content += create_table_html(chi2_cont_df, \"Observed counts by group × category\", max_rows=10, index=True)\n",
        "if not chi2_exp_df.empty:\n",
        "    html_content += create_table_html(chi2_exp_df, \"Expected counts under independence\", max_rows=10, index=True)\n",
        "if not chi2_z_df.empty:\n",
        "    html_content += create_table_html(chi2_z_df, \"Pearson standardized residuals (z)\", max_rows=10, index=True)\n",
        "    html_content += top_residuals_html(k=10)\n",
        "if not chi2_padj_df.empty:\n",
        "    html_content += create_table_html(chi2_padj_df, \"Per-cell p-values (BH-adjusted)\", max_rows=10, index=True)\n",
        "\n",
        "# Two-proportion\n",
        "if not two_prop_df.empty:\n",
        "    html_content += \"\"\"\n",
        "        <h3>Two-Proportion Tests (Joyce subsets vs BNC)</h3>\n",
        "        <p>Newcombe confidence intervals and Cohen’s h. BH correction applied across tests.</p>\n",
        "    \"\"\"\n",
        "    # Ensure adj col exists (defensive)\n",
        "    if \"p_adj_BH\" not in two_prop_df.columns and \"p_value\" in two_prop_df.columns:\n",
        "        pv = two_prop_df[\"p_value\"].fillna(1.0).to_numpy()\n",
        "        order = np.argsort(pv)\n",
        "        ranked = np.empty_like(pv, dtype=float)\n",
        "        cummin = 1.0\n",
        "        n = pv.size\n",
        "        for i, idx in enumerate(order[::-1], start=1):\n",
        "            rank = n - i + 1\n",
        "            val = pv[idx] * n / rank\n",
        "            cummin = min(cummin, val)\n",
        "            ranked[idx] = min(cummin, 1.0)\n",
        "        two_prop_df[\"p_adj_BH\"] = ranked\n",
        "    html_content += create_table_html(two_prop_df, \"All two-proportion results\", max_rows=15)\n",
        "    html_content += sig_two_prop_html(alpha=0.05, top=15)\n",
        "\n",
        "# Binomial\n",
        "if not binom_df.empty:\n",
        "    if \"p_adj_BH\" not in binom_df.columns and \"p_value\" in binom_df.columns:\n",
        "        pv = binom_df[\"p_value\"].fillna(1.0).to_numpy()\n",
        "        order = np.argsort(pv)\n",
        "        ranked = np.empty_like(pv, dtype=float)\n",
        "        cummin = 1.0\n",
        "        n = pv.size\n",
        "        for i, idx in enumerate(order[::-1], start=1):\n",
        "            rank = n - i + 1\n",
        "            val = pv[idx] * n / rank\n",
        "            cummin = min(cummin, val)\n",
        "            ranked[idx] = min(cummin, 1.0)\n",
        "        binom_df[\"p_adj_BH\"] = ranked\n",
        "    html_content += \"\"\"\n",
        "        <h3>Binomial Tests (Joyce subsets vs BNC reference proportion)</h3>\n",
        "        <p>One-sample tests per category per subset; BH correction across tests.</p>\n",
        "    \"\"\"\n",
        "    html_content += create_table_html(binom_df, \"All binomial results\", max_rows=12)\n",
        "    html_content += sig_binom_html(alpha=0.05, top=12)\n",
        "\n",
        "# Continuous\n",
        "if not cont_df.empty:\n",
        "    html_content += \"\"\"\n",
        "        <h3>Continuous Features</h3>\n",
        "        <p>Welch t and Mann–Whitney U with effect sizes (Hedges’ g, Cliff’s δ). BH correction applied.</p>\n",
        "    \"\"\"\n",
        "    html_content += create_table_html(cont_df, \"Continuous feature comparisons\", max_rows=12)\n",
        "    html_content += sig_continuous_html(alpha=0.05)\n",
        "if not hl_df.empty:\n",
        "    html_content += create_table_html(hl_df, \"Hodges–Lehmann location shifts (A − B) with bootstrap CIs\", max_rows=12)\n",
        "\n",
        "# NEW: Comparator-segment section\n",
        "if not seg_cont_df.empty or not seg_presence_df.empty:\n",
        "    html_content += seg_cont_tables_html()\n",
        "    html_content += seg_presence_html()\n",
        "\n",
        "# Optional: group means from latest segments merged to results_df\n",
        "html_content += seg_group_means_html()\n",
        "\n",
        "# Topics\n",
        "html_content += \"\"\"\n",
        "    </div>\n",
        "    <div class=\"section\">\n",
        "        <h2>Topic Modelling</h2>\n",
        "        <p>Per-subset LDA topics (CountVectorizer; 5 topics × 10 words). Topic mixture weights summarised by mean.</p>\n",
        "\"\"\"\n",
        "if not topics_df.empty:\n",
        "    html_content += create_table_html(topics_df, \"Top words per topic × group\", max_rows=20)\n",
        "if not topicmix_df.empty:\n",
        "    html_content += create_table_html(topicmix_df, \"Mean topic weights per group\", max_rows=20)\n",
        "\n",
        "# Sample of comprehensive results\n",
        "if 'results_df' in globals() and isinstance(results_df, pd.DataFrame) and not results_df.empty:\n",
        "    sample_cols = [c for c in ['Instance_ID','Original_Dataset','Category_Framework','Comparator_Type',\n",
        "                               'Sentence_Length','Sentiment_Polarity','Pre_Post_Ratio'] if c in results_df.columns]\n",
        "    sample_results = results_df.head(25)[sample_cols].round(3)\n",
        "    html_content += f\"\"\"\n",
        "    </div>\n",
        "    <div class=\"section\">\n",
        "        <h2>Comprehensive Results — Sample</h2>\n",
        "        <p>Representative snippet of the full dataset (first 25 rows):</p>\n",
        "        {create_table_html(sample_results, \"Sample of comprehensive analysis\", max_rows=25)}\n",
        "        <div class=\"methodology\">\n",
        "            The full dataset includes lemmatisation, POS distributions, syntactic complexity, and comparative structure features.\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# Files list\n",
        "files_list = []\n",
        "files_list.append(\"comprehensive_linguistic_analysis_corrected.csv\")\n",
        "for k, p in paths.items():\n",
        "    if p:\n",
        "        files_list.append(os.path.relpath(p))\n",
        "files_items = \"\".join([f\"<li><code>{f}</code></li>\" for f in sorted(set(files_list))])\n",
        "\n",
        "html_content += f\"\"\"\n",
        "    <div class=\"section\">\n",
        "        <h2>Files Generated</h2>\n",
        "        <ul>{files_items}</ul>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"footer\">\n",
        "        <p>Generated by Comprehensive Linguistic Analysis Pipeline</p>\n",
        "        <p>Joyce Simile Research Project • {report_timestamp}</p>\n",
        "        <p><em>Categories harmonised with unified <strong>Quasi_Similes</strong>; results computed using FDR corrections where applicable.</em></p>\n",
        "    </div>\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Save HTML report\n",
        "report_filename = f\"joyce_simile_analysis_report_{report_date}.html\"\n",
        "with open(report_filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(f\"✓ Academic HTML report generated: {report_filename}\")\n",
        "print(f\"✓ File size: {os.path.getsize(report_filename):,} bytes\")\n",
        "print(f\"✓ Report length: {len(html_content):,} characters\")\n",
        "print(\"\\nREPORT READY — open in a browser to view.\")\n"
      ],
      "metadata": {
        "id": "jCd3moooAOxY",
        "outputId": "2833618b-7e08-4c06-adf6-e680aabe79f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATING ACADEMIC HTML REPORT\n",
            "==================================================\n",
            "✓ Academic HTML report generated: joyce_simile_analysis_report_20250825_113445.html\n",
            "✓ File size: 60,206 bytes\n",
            "✓ Report length: 60,166 characters\n",
            "\n",
            "REPORT READY — open in a browser to view.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ACADEMIC HTML REPORT GENERATOR — UPDATED (data-driven, harmonised labels)\n",
        "# Generates a comprehensive, reproducible HTML report from Cells 1–2 outputs\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "print(\"GENERATING ACADEMIC HTML REPORT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Timestamp for report\n",
        "now = datetime.now()\n",
        "report_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "report_date = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def latest_file(pattern, base=\"analysis_outputs\"):\n",
        "    files = glob.glob(os.path.join(base, pattern))\n",
        "    return max(files, key=os.path.getctime) if files else None\n",
        "\n",
        "def safe_read_csv(path):\n",
        "    if not path or not os.path.exists(path):\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load CSV {path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def safe_read_json(path):\n",
        "    if not path or not os.path.exists(path):\n",
        "        return {}\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load JSON {path}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def df_for_html(df, index=False, max_rows=None):\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "    d = df.copy()\n",
        "    if max_rows is not None and len(d) > max_rows:\n",
        "        d = d.head(max_rows)\n",
        "        d.__truncated__ = True\n",
        "    return d\n",
        "\n",
        "def create_table_html(df, title=\"\", max_rows=20, index=False):\n",
        "    \"\"\"Create HTML table with styling; auto-handle empty and truncation note.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return f\"<p><em>No data available for {title}</em></p>\"\n",
        "    d = df.copy()\n",
        "    trunc = False\n",
        "    if max_rows and len(d) > max_rows:\n",
        "        d = d.head(max_rows)\n",
        "        trunc = True\n",
        "    # If the index is meaningful (named or not default RangeIndex), show it as a column\n",
        "    if index:\n",
        "        d = d.reset_index()\n",
        "    table_html = d.to_html(classes='analysis-table', escape=False, index=False)\n",
        "    note = f\"<p class='truncated-note'><em>Showing first {max_rows} of {len(df)} rows</em></p>\" if trunc else \"\"\n",
        "    return f\"\"\"\n",
        "    <div class=\"table-container\">\n",
        "        <h4>{title}</h4>\n",
        "        <div class=\"table-wrapper\">{table_html}</div>\n",
        "        {note}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# ---------- Pull in analysis outputs from Cell 2 ----------\n",
        "out_dir = \"analysis_outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "paths = {\n",
        "    \"chi2_cont\": latest_file(\"chi2_contingency_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_exp\": latest_file(\"chi2_expected_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_z\": latest_file(\"chi2_pearson_z_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_p\": latest_file(\"chi2_cell_p_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_padj\": latest_file(\"chi2_cell_padj_BH_by_subset_*.csv\", out_dir),\n",
        "    \"two_prop\": latest_file(\"two_prop_newcombe_by_subset_*.csv\", out_dir),\n",
        "    \"binom\": latest_file(\"binomial_tests_by_subset_*.csv\", out_dir),\n",
        "    \"cont\": latest_file(\"continuous_tests_by_subset_*.csv\", out_dir),\n",
        "    \"topics\": latest_file(\"lda_topics_by_subset_*.csv\", out_dir),\n",
        "    \"topicmix\": latest_file(\"lda_topic_mix_by_subset_*.csv\", out_dir),\n",
        "    \"hl\": latest_file(\"continuous_HL_shifts_*.csv\", out_dir),\n",
        "    \"master\": latest_file(\"stats_and_topics_summary_by_subset_*.json\", out_dir),\n",
        "}\n",
        "\n",
        "chi2_cont_df = safe_read_csv(paths[\"chi2_cont\"])\n",
        "chi2_exp_df  = safe_read_csv(paths[\"chi2_exp\"])\n",
        "chi2_z_df    = safe_read_csv(paths[\"chi2_z\"])\n",
        "chi2_p_df    = safe_read_csv(paths[\"chi2_p\"])\n",
        "chi2_padj_df = safe_read_csv(paths[\"chi2_padj\"])\n",
        "two_prop_df  = safe_read_csv(paths[\"two_prop\"])\n",
        "binom_df     = safe_read_csv(paths[\"binom\"])\n",
        "cont_df      = safe_read_csv(paths[\"cont\"])\n",
        "topics_df    = safe_read_csv(paths[\"topics\"])\n",
        "topicmix_df  = safe_read_csv(paths[\"topicmix\"])\n",
        "hl_df        = safe_read_csv(paths[\"hl\"])\n",
        "master_json  = safe_read_json(paths[\"master\"])\n",
        "\n",
        "# ---------- Summaries from Cell 1 (results_df, f1_analysis, comparator.env_info) ----------\n",
        "def create_summary_stats_html():\n",
        "    if 'results_df' not in globals() or results_df is None or results_df.empty:\n",
        "        return \"<p><em>No results data available</em></p>\"\n",
        "    dcounts = results_df['Original_Dataset'].value_counts()\n",
        "    ccounts = results_df['Category_Framework'].value_counts()\n",
        "\n",
        "    items_d = \"\".join([f\"<li><strong>{k}:</strong> {int(v):,} instances</li>\" for k,v in dcounts.items()])\n",
        "    items_c = \"\".join([f\"<li><strong>{k}:</strong> {int(v):,} ({(v/len(results_df))*100:.1f}%)</li>\" for k,v in ccounts.items()])\n",
        "\n",
        "    return f\"\"\"\n",
        "    <div class=\"summary-stats\">\n",
        "        <div class=\"stat-group\">\n",
        "            <h4>Dataset Distribution</h4>\n",
        "            <ul>{items_d}</ul>\n",
        "            <p><strong>Total Instances:</strong> {len(results_df):,}</p>\n",
        "        </div>\n",
        "        <div class=\"stat-group\">\n",
        "            <h4>Category Distribution</h4>\n",
        "            <ul>{items_c}</ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def f1_summary_html():\n",
        "    # Try multiple sources\n",
        "    micro_rb = macro_rb = micro_nlp = macro_nlp = None\n",
        "    pairs_rb = pairs_nlp = None\n",
        "    if 'f1_analysis' in globals() and isinstance(f1_analysis, dict):\n",
        "        rb = f1_analysis.get('rule_based_vs_manual') or f1_analysis.get('Rule-Based vs Manual')\n",
        "        nl = f1_analysis.get('nlp_vs_manual') or f1_analysis.get('NLP vs Manual')\n",
        "        if rb and rb.get(\"overall\"):\n",
        "            micro_rb = rb[\"overall\"].get(\"micro_f1\")\n",
        "            macro_rb = rb[\"overall\"].get(\"macro_f1\")\n",
        "            pairs_rb = rb.get(\"pairs\")\n",
        "        if nl and nl.get(\"overall\"):\n",
        "            micro_nlp = nl[\"overall\"].get(\"micro_f1\")\n",
        "            macro_nlp = nl[\"overall\"].get(\"macro_f1\")\n",
        "            pairs_nlp = nl.get(\"pairs\")\n",
        "    elif 'comparator' in globals():\n",
        "        try:\n",
        "            fa = comparator.comparison_results.get('f1_analysis', {})\n",
        "            rb = fa.get('rule_based_vs_manual')\n",
        "            nl = fa.get('nlp_vs_manual')\n",
        "            if rb and rb.get(\"overall\"):\n",
        "                micro_rb = rb[\"overall\"].get(\"micro_f1\")\n",
        "                macro_rb = rb[\"overall\"].get(\"macro_f1\")\n",
        "                pairs_rb = rb.get(\"pairs\")\n",
        "            if nl and nl.get(\"overall\"):\n",
        "                micro_nlp = nl[\"overall\"].get(\"micro_f1\")\n",
        "                macro_nlp = nl[\"overall\"].get(\"macro_f1\")\n",
        "                pairs_nlp = nl.get(\"pairs\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if micro_rb is None and micro_nlp is None:\n",
        "        return \"<p><em>F1 metrics unavailable (run Cell 1 in this session).</em></p>\"\n",
        "\n",
        "    lines = []\n",
        "    if micro_rb is not None:\n",
        "        lines.append(f\"• Rule-Based vs Manual: micro-F1 = {micro_rb:.3f}, macro-F1 = {macro_rb:.3f}{' (pairs=' + str(pairs_rb) + ')' if pairs_rb else ''}\")\n",
        "    if micro_nlp is not None:\n",
        "        lines.append(f\"• NLP vs Manual: micro-F1 = {micro_nlp:.3f}, macro-F1 = {macro_nlp:.3f}{' (pairs=' + str(pairs_nlp) + ')' if pairs_nlp else ''}\")\n",
        "\n",
        "    return f\"\"\"\n",
        "    <div class=\"highlight\">\n",
        "      <strong>F1 Summary:</strong><br>\n",
        "      {'<br>'.join(lines)}\n",
        "      <br>• Total instances processed: {len(results_df):,} (across all datasets)\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def env_html():\n",
        "    info = {}\n",
        "    if 'comparator' in globals():\n",
        "        try:\n",
        "            info = comparator.env_info\n",
        "        except Exception:\n",
        "            info = {}\n",
        "    if not info:\n",
        "        return \"\"\n",
        "    items = \"\".join([f\"<li><strong>{k}:</strong> {v}</li>\" for k,v in info.items()])\n",
        "    return f\"\"\"\n",
        "    <div class=\"methodology\">\n",
        "      <strong>Environment:</strong>\n",
        "      <ul>{items}</ul>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# ---------- Build analytic highlight snippets ----------\n",
        "def chi2_summary_html():\n",
        "    if not master_json:\n",
        "        return \"\"\n",
        "    chi = master_json.get(\"chi_square_4way\", {})\n",
        "    chi_line = (f\"χ² = {chi.get('chi2', float('nan')):.3f} | df = {chi.get('dof', 0)} | \"\n",
        "                f\"p = {chi.get('p_value', float('nan')):.3g} | Cramér’s V = {chi.get('cramers_v', float('nan')):.3f} \"\n",
        "                f\"| Monte-Carlo={chi.get('monte_carlo', False)} | N={chi.get('N', 0)}\")\n",
        "    return f\"<p>{chi_line}</p>\"\n",
        "\n",
        "def top_residuals_html(k=10):\n",
        "    if chi2_z_df.empty or chi2_cont_df.empty or chi2_exp_df.empty:\n",
        "        return \"\"\n",
        "    # Melt z\n",
        "    z_long = chi2_z_df.copy()\n",
        "    z_long = z_long.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in z_long.columns else z_long\n",
        "    z_long = z_long.melt(id_vars=[c for c in [\"Category_Framework\"] if c in z_long.columns],\n",
        "                         var_name=\"Group\", value_name=\"z\")\n",
        "    z_long[\"abs_z\"] = z_long[\"z\"].abs()\n",
        "    # Get obs/exp\n",
        "    cont = chi2_cont_df.copy()\n",
        "    cont = cont.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in cont.columns else cont\n",
        "    exp = chi2_exp_df.copy()\n",
        "    exp = exp.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in exp.columns else exp\n",
        "    # Align to long\n",
        "    obs_long = cont.melt(id_vars=[c for c in [\"Category_Framework\"] if c in cont.columns],\n",
        "                         var_name=\"Group\", value_name=\"obs\")\n",
        "    exp_long = exp.melt(id_vars=[c for c in [\"Category_Framework\"] if c in exp.columns],\n",
        "                        var_name=\"Group\", value_name=\"exp\")\n",
        "    m = (z_long.merge(obs_long, on=[\"Category_Framework\",\"Group\"], how=\"left\")\n",
        "               .merge(exp_long, on=[\"Category_Framework\",\"Group\"], how=\"left\"))\n",
        "    m = m.sort_values(\"abs_z\", ascending=False).head(k)\n",
        "    return create_table_html(m, f\"Top {k} standardized residuals (|z|)\", max_rows=k)\n",
        "\n",
        "def sig_two_prop_html(alpha=0.05, top=15):\n",
        "    if two_prop_df.empty or \"p_adj_BH\" not in two_prop_df.columns:\n",
        "        return \"\"\n",
        "    df = two_prop_df.copy()\n",
        "    # Keep only significant rows, order by adjusted p then effect size magnitude\n",
        "    df[\"|h|\"] = df.get(\"cohens_h\", 0).abs()\n",
        "    sig = df[df[\"p_adj_BH\"] < alpha].sort_values([\"p_adj_BH\",\"|h|\"])\n",
        "    cols = [c for c in [\"Subset\",\"Category\",\"prop_A\",\"prop_B\",\"z\",\"p_value\",\"p_adj_BH\",\"cohens_h\"] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Two-proportion (BH<{alpha}) — top {top}\", max_rows=top)\n",
        "\n",
        "def sig_binom_html(alpha=0.05, top=15):\n",
        "    if binom_df.empty or \"p_adj_BH\" not in binom_df.columns:\n",
        "        return \"\"\n",
        "    df = binom_df.copy()\n",
        "    sig = df[df[\"p_adj_BH\"] < alpha].sort_values(\"p_adj_BH\")\n",
        "    cols = [c for c in [\"Subset\",\"Category\",\"count_A\",\"n_A\",\"p_ref_BNC\",\"p_value\",\"p_adj_BH\"] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Binomial vs BNC (BH<{alpha}) — top {top}\", max_rows=top)\n",
        "\n",
        "def sig_continuous_html(alpha=0.05):\n",
        "    if cont_df.empty:\n",
        "        return \"\"\n",
        "    d = cont_df.copy()\n",
        "    # Prefer Mann–Whitney U adjusted p; fall back to Welch t\n",
        "    if \"U_padj_BH\" in d.columns:\n",
        "        sig = d[d[\"U_padj_BH\"] < alpha].sort_values(\"U_padj_BH\")\n",
        "        label = \"Mann–Whitney U (BH)\"\n",
        "        padj_col = \"U_padj_BH\"\n",
        "        p_col = \"U_pvalue\"\n",
        "    elif \"t_padj_BH\" in d.columns:\n",
        "        sig = d[d[\"t_padj_BH\"] < alpha].sort_values(\"t_padj_BH\")\n",
        "        label = \"Welch t (BH)\"\n",
        "        padj_col = \"t_padj_BH\"\n",
        "        p_col = \"t_pvalue\"\n",
        "    else:\n",
        "        return \"\"\n",
        "    if sig.empty:\n",
        "        return \"<p><em>No continuous feature differences were significant after FDR correction.</em></p>\"\n",
        "    cols = [c for c in [\"Feature\",\"Subset\",\"A_n\",\"B_n\",\"A_mean\",\"B_mean\",\"A_median\",\"B_median\",\n",
        "                        \"t_stat\",\"t_pvalue\",\"U_stat\",\"U_pvalue\",\"hedges_g\",\"cliffs_delta\", padj_col] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Continuous features — significant by {label}\", max_rows=15)\n",
        "\n",
        "# ---------- Build the HTML ----------\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/>\n",
        "<title>Joyce Simile Research: Comprehensive Linguistic Analysis Report</title>\n",
        "<style>\n",
        "    body {{\n",
        "        font-family: 'Times New Roman', serif;\n",
        "        line-height: 1.6;\n",
        "        margin: 0;\n",
        "        padding: 20px;\n",
        "        background-color: #f9f9f9;\n",
        "        color: #333;\n",
        "    }}\n",
        "    .container {{\n",
        "        max-width: 1200px;\n",
        "        margin: 0 auto;\n",
        "        background: white;\n",
        "        padding: 30px;\n",
        "        border-radius: 8px;\n",
        "        box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
        "    }}\n",
        "    .header {{\n",
        "        text-align: center;\n",
        "        border-bottom: 3px solid #2c3e50;\n",
        "        padding-bottom: 20px;\n",
        "        margin-bottom: 30px;\n",
        "    }}\n",
        "    .header h1 {{ color: #2c3e50; margin: 0; font-size: 2.2em; font-weight: bold; }}\n",
        "    .header .subtitle {{ color: #7f8c8d; font-size: 1.1em; margin: 10px 0 5px 0; font-style: italic; }}\n",
        "    .header .timestamp {{ color: #95a5a6; font-size: 0.9em; }}\n",
        "    .section {{\n",
        "        margin: 30px 0;\n",
        "        padding: 20px;\n",
        "        border-left: 4px solid #3498db;\n",
        "        background-color: #f8f9fa;\n",
        "    }}\n",
        "    .section h2 {{\n",
        "        color: #2c3e50;\n",
        "        margin-top: 0; border-bottom: 2px solid #ecf0f1; padding-bottom: 10px;\n",
        "    }}\n",
        "    .section h3 {{ color: #34495e; margin-top: 25px; }}\n",
        "    .section h4 {{ color: #5d6d7e; margin-top: 20px; margin-bottom: 10px; }}\n",
        "    .analysis-table {{ width: 100%; border-collapse: collapse; margin: 15px 0; font-size: 0.9em; }}\n",
        "    .analysis-table th {{ background:#34495e; color:#fff; padding:12px 8px; text-align:left; font-weight:bold; }}\n",
        "    .analysis-table td {{ padding:10px 8px; border-bottom:1px solid #ddd; }}\n",
        "    .analysis-table tr:nth-child(even) {{ background-color:#f2f2f2; }}\n",
        "    .analysis-table tr:hover {{ background-color:#e8f4fd; }}\n",
        "    .summary-stats {{ display:grid; grid-template-columns:1fr 1fr; gap:30px; margin:20px 0; }}\n",
        "    .stat-group {{ background:#fff; padding:20px; border-radius:6px; border:1px solid #e1e8ed; }}\n",
        "    .stat-group h4 {{ margin-top:0; color:#2c3e50; border-bottom:1px solid #ecf0f1; padding-bottom:8px; }}\n",
        "    .stat-group ul {{ list-style-type:none; padding:0; }}\n",
        "    .stat-group li {{ padding:5px 0; border-bottom:1px solid #f8f9fa; }}\n",
        "    .highlight {{ background:#d1ecf1; padding:15px; border-left:4px solid #17a2b8; margin:15px 0; }}\n",
        "    .methodology {{ background:#f8f9fa; padding:15px; border-radius:5px; margin:15px 0; font-style:italic; }}\n",
        "    .table-container {{ margin:20px 0; }}\n",
        "    .table-wrapper {{ overflow-x:auto; }}\n",
        "    .truncated-note {{ color:#6c757d; font-size:0.9em; margin-top:5px; }}\n",
        "    .footer {{ text-align:center; margin-top:40px; padding-top:20px; border-top:2px solid #ecf0f1; color:#7f8c8d; font-size:0.9em; }}\n",
        "    @media (max-width: 768px) {{\n",
        "        .summary-stats {{ grid-template-columns:1fr; }}\n",
        "        .container {{ padding:15px; }}\n",
        "        .analysis-table {{ font-size:0.85em; }}\n",
        "    }}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <div class=\"header\">\n",
        "        <h1>Joyce Simile Research</h1>\n",
        "        <div class=\"subtitle\">Comprehensive Linguistic Analysis Report</div>\n",
        "        <div class=\"subtitle\">Manual Annotations vs Extraction Methods vs BNC Baseline</div>\n",
        "        <div class=\"timestamp\">Generated on {report_timestamp}</div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Executive Summary</h2>\n",
        "        <p>This report compiles the results of a comparative analysis of simile usage in <em>Dubliners</em>, spanning manual expert annotations, a restrictive rule-based extractor, a less-restrictive NLP extractor, and a BNC baseline. Categories were harmonised with <strong>Quasi_Similes</strong> as the unified label for the Joyce/BNC quasi-simile phenomenon.</p>\n",
        "        {chi2_summary_html()}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Dataset Overview</h2>\n",
        "        <p>Four datasets were analysed, representing complementary identification approaches and a baseline:</p>\n",
        "        {create_summary_stats_html()}\n",
        "        {env_html()}\n",
        "        <div class=\"methodology\">\n",
        "            <strong>Methodology:</strong> Features include token counts, pre/post-comparator ratio, POS distribution, syntactic complexity, TextBlob sentiment (exploratory), and comparative structure flags. Instance-aligned F1 (exact+fuzzy sentence matching) was used to evaluate extractors against manual ground truth.\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Performance Metrics</h2>\n",
        "        <h3>Instance-Aligned F1 Scores</h3>\n",
        "        {f1_summary_html()}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Statistical Results</h2>\n",
        "        <h3>4-way Category Distribution</h3>\n",
        "\"\"\"\n",
        "\n",
        "# 4-way chi-square tables\n",
        "if not chi2_cont_df.empty:\n",
        "    html_content += create_table_html(chi2_cont_df, \"Observed counts by group × category\", max_rows=10, index=True)\n",
        "if not chi2_exp_df.empty:\n",
        "    html_content += create_table_html(chi2_exp_df, \"Expected counts under independence\", max_rows=10, index=True)\n",
        "if not chi2_z_df.empty:\n",
        "    html_content += create_table_html(chi2_z_df, \"Pearson standardized residuals (z)\", max_rows=10, index=True)\n",
        "    html_content += top_residuals_html(k=10)\n",
        "if not chi2_padj_df.empty:\n",
        "    html_content += create_table_html(chi2_padj_df, \"Per-cell p-values (BH-adjusted)\", max_rows=10, index=True)\n",
        "\n",
        "# Two-proportion\n",
        "if not two_prop_df.empty:\n",
        "    html_content += \"\"\"\n",
        "        <h3>Two-Proportion Tests (Joyce subsets vs BNC)</h3>\n",
        "        <p>Newcombe confidence intervals and Cohen’s h. BH correction applied across tests.</p>\n",
        "    \"\"\"\n",
        "    # Ensure adj col exists (defensive)\n",
        "    if \"p_adj_BH\" not in two_prop_df.columns and \"p_value\" in two_prop_df.columns:\n",
        "        pv = two_prop_df[\"p_value\"].fillna(1.0).to_numpy()\n",
        "        # simple BH\n",
        "        order = np.argsort(pv)\n",
        "        ranked = np.empty_like(pv, dtype=float)\n",
        "        cummin = 1.0\n",
        "        n = pv.size\n",
        "        for i, idx in enumerate(order[::-1], start=1):\n",
        "            rank = n - i + 1\n",
        "            val = pv[idx] * n / rank\n",
        "            cummin = min(cummin, val)\n",
        "            ranked[idx] = min(cummin, 1.0)\n",
        "        two_prop_df[\"p_adj_BH\"] = ranked\n",
        "    html_content += create_table_html(two_prop_df, \"All two-proportion results\", max_rows=15)\n",
        "    html_content += sig_two_prop_html(alpha=0.05, top=15)\n",
        "\n",
        "# Binomial\n",
        "if not binom_df.empty:\n",
        "    if \"p_adj_BH\" not in binom_df.columns and \"p_value\" in binom_df.columns:\n",
        "        pv = binom_df[\"p_value\"].fillna(1.0).to_numpy()\n",
        "        order = np.argsort(pv)\n",
        "        ranked = np.empty_like(pv, dtype=float)\n",
        "        cummin = 1.0\n",
        "        n = pv.size\n",
        "        for i, idx in enumerate(order[::-1], start=1):\n",
        "            rank = n - i + 1\n",
        "            val = pv[idx] * n / rank\n",
        "            cummin = min(cummin, val)\n",
        "            ranked[idx] = min(cummin, 1.0)\n",
        "        binom_df[\"p_adj_BH\"] = ranked\n",
        "    html_content += \"\"\"\n",
        "        <h3>Binomial Tests (Joyce subsets vs BNC reference proportion)</h3>\n",
        "        <p>One-sample tests per category per subset; BH correction across tests.</p>\n",
        "    \"\"\"\n",
        "    html_content += create_table_html(binom_df, \"All binomial results\", max_rows=12)\n",
        "    html_content += sig_binom_html(alpha=0.05, top=12)\n",
        "\n",
        "# Continuous\n",
        "if not cont_df.empty:\n",
        "    html_content += \"\"\"\n",
        "        <h3>Continuous Features</h3>\n",
        "        <p>Welch t and Mann–Whitney U with effect sizes (Hedges’ g, Cliff’s δ). BH correction applied.</p>\n",
        "    \"\"\"\n",
        "    html_content += create_table_html(cont_df, \"Continuous feature comparisons\", max_rows=12)\n",
        "    html_content += sig_continuous_html(alpha=0.05)\n",
        "if not hl_df.empty:\n",
        "    html_content += create_table_html(hl_df, \"Hodges–Lehmann location shifts (A − B) with bootstrap CIs\", max_rows=12)\n",
        "\n",
        "# Topics\n",
        "html_content += \"\"\"\n",
        "    </div>\n",
        "    <div class=\"section\">\n",
        "        <h2>Topic Modelling</h2>\n",
        "        <p>Per-subset LDA topics (CountVectorizer; 5 topics × 10 words). Topic mixture weights summarised by mean.</p>\n",
        "\"\"\"\n",
        "if not topics_df.empty:\n",
        "    html_content += create_table_html(topics_df, \"Top words per topic × group\", max_rows=20)\n",
        "if not topicmix_df.empty:\n",
        "    html_content += create_table_html(topicmix_df, \"Mean topic weights per group\", max_rows=20)\n",
        "\n",
        "# Sample of comprehensive results\n",
        "if 'results_df' in globals() and isinstance(results_df, pd.DataFrame) and not results_df.empty:\n",
        "    sample_cols = [c for c in ['Instance_ID','Original_Dataset','Category_Framework','Comparator_Type',\n",
        "                               'Sentence_Length','Sentiment_Polarity','Pre_Post_Ratio'] if c in results_df.columns]\n",
        "    sample_results = results_df.head(25)[sample_cols].round(3)\n",
        "    html_content += f\"\"\"\n",
        "    </div>\n",
        "    <div class=\"section\">\n",
        "        <h2>Comprehensive Results — Sample</h2>\n",
        "        <p>Representative snippet of the full dataset (first 25 rows):</p>\n",
        "        {create_table_html(sample_results, \"Sample of comprehensive analysis\", max_rows=25)}\n",
        "        <div class=\"methodology\">\n",
        "            The full dataset includes lemmatisation, POS distributions, syntactic complexity, and comparative structure features.\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# Files list\n",
        "files_list = []\n",
        "files_list.append(\"comprehensive_linguistic_analysis_corrected.csv\")\n",
        "for k, p in paths.items():\n",
        "    if p:\n",
        "        files_list.append(os.path.relpath(p))\n",
        "files_items = \"\".join([f\"<li><code>{f}</code></li>\" for f in sorted(set(files_list))])\n",
        "\n",
        "html_content += f\"\"\"\n",
        "    <div class=\"section\">\n",
        "        <h2>Files Generated</h2>\n",
        "        <ul>{files_items}</ul>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"footer\">\n",
        "        <p>Generated by Comprehensive Linguistic Analysis Pipeline</p>\n",
        "        <p>Joyce Simile Research Project • {report_timestamp}</p>\n",
        "        <p><em>Categories harmonised with unified <strong>Quasi_Similes</strong>; results computed using FDR corrections where applicable.</em></p>\n",
        "    </div>\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Save HTML report\n",
        "report_filename = f\"joyce_simile_analysis_report_{report_date}.html\"\n",
        "with open(report_filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(f\"✓ Academic HTML report generated: {report_filename}\")\n",
        "print(f\"✓ File size: {os.path.getsize(report_filename):,} bytes\")\n",
        "print(f\"✓ Report length: {len(html_content):,} characters\")\n",
        "print(\"\\nREPORT READY — open in a browser to view.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMyOrRz-4kmz",
        "outputId": "64ae16fa-60eb-4516-828b-4788c5abf525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATING ACADEMIC HTML REPORT\n",
            "==================================================\n",
            "✓ Academic HTML report generated: joyce_simile_analysis_report_20250824_210058.html\n",
            "✓ File size: 46,522 bytes\n",
            "✓ Report length: 46,491 characters\n",
            "\n",
            "REPORT READY — open in a browser to view.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Research Implications and Future Directions\n",
        "\n",
        "## 9.1 Computational Literary Analysis\n",
        "Using sentence-level **instance-aligned evaluation** (exact match, then fuzzy ≥ 0.92), the extractors partially recover the manual categories:\n",
        "- **Rule-Based vs Manual:** micro-F1 ≈ **0.343**, macro-F1 ≈ **0.178** (pairs matched: see Cell 1 output)\n",
        "- **NLP (Less-Restrictive) vs Manual:** micro-F1 ≈ **0.292**, macro-F1 ≈ **0.059**\n",
        "\n",
        "These scores indicate **meaningful but incomplete replication** of expert judgments—appropriate for unsupervised/heuristic extractors—and motivate targeted improvements (Section 9.4). Importantly, categorical distributions still differ strongly across corpora (**χ²≈281.88**, **Cramér’s V≈0.318**, *p*≪.001), so the **signal is primarily categorical**, not scalar.\n",
        "\n",
        "**Key distributional contrasts (harmonised labels):**\n",
        "- **BNC**: *Quasi_Similes* ≈ **41%** (82/200) vs **Joyce-Manual** ≈ **29%** (53/184).  \n",
        "- **Joyce-Manual** shows enrichments in **Joycean_Framed**, **Joycean_Quasi_Fuzzy**, and **Joycean_Silent**.\n",
        "- **NLP (Less-Restrictive)** is **all Standard** (330/330), by design—useful as a control but excluded in robustness checks where appropriate.\n",
        "\n",
        "## 9.2 Innovation Quantification\n",
        "Under the unified scheme:\n",
        "- **Joycean-specific categories** (Framed + Quasi_Fuzzy + Silent) account for **~20%** of **Joyce-Manual** (37/184).\n",
        "- **Quasi_Similes** appear **less** in **Joyce-Manual** (~29%) relative to **BNC** (~41%), with significant two-proportion differences (BH-corrected; moderate effects, e.g., |*h*|≈0.26–0.42 for Manual/Restrictive vs BNC).\n",
        "\n",
        "This suggests Joyce’s corpus **rebalances** the space of similes: fewer conventional quasi-similes than BNC and **more Joycean-specific forms**, evidencing stylistic innovation distinct from baseline English.\n",
        "\n",
        "## 9.3 Methodological Contributions\n",
        "- **Label harmonisation**: *Joycean_Quasi* merged into **Quasi_Similes** to align BNC and Joyce phenomena, avoiding double-counting and artificial χ² inflation.\n",
        "- **Robust inference**: 4-way χ² with per-cell residuals and **BH-FDR**; two-proportion tests with **Newcombe CIs** + **Cohen’s h**; binomial tests vs BNC reference; Welch-*t* & Mann–Whitney-*U* with **Hedges’ g** and **Cliff’s δ**; optional 3-way χ² excluding the degenerate NLP group.\n",
        "- **Instance-aligned evaluation**: sentence-pairing (exact+fuzzy) for extractor vs manual categories, a more faithful alternative to bag-of-counts F1.\n",
        "- **Reproducibility**: environment stamping; deterministic random seeds; saved CSV/JSON artifacts.\n",
        "\n",
        "## 9.4 Future Directions\n",
        "1. **Improve extractor recall** for Joycean categories  \n",
        "   - Add patterns for **punctuation-mediated** comparators (colon/semicolon/ellipsis) and **‘as … as’** spans.  \n",
        "   - Incorporate dependency features (governor-comparator-target) and discourse cues.\n",
        "\n",
        "2. **Supervised modelling**  \n",
        "   - Train a classifier on paired manual/extractor sentences (use current pairs as silver labels), with features from syntax, lemmata, and comparator spans.\n",
        "\n",
        "3. **Error analysis & ablations**  \n",
        "   - Per-category confusion and residuals to identify systematic misses; ablate comparator detection vs category mapping.\n",
        "\n",
        "4. **Topic–category linkage**  \n",
        "   - Relate LDA topic mixtures to categories (e.g., Framed vs Standard) and test with permutation or regression.\n",
        "\n",
        "5. **Generalisation**  \n",
        "   - Validate on other Joyce texts or contemporaries; check stability of the Joycean category enrichments.\n",
        "\n",
        "---\n",
        "\n",
        "## References and Data Sources\n",
        "\n",
        "**Primary Text**  \n",
        "- Joyce, James. *Dubliners*. Project Gutenberg.\n",
        "\n",
        "**Baseline Corpus**  \n",
        "- British National Corpus (BNC), used as a **standard English** reference. *Ensure appropriate licensing/access for the specific BNC source employed.*\n",
        "\n",
        "---\n",
        "\n",
        "## Computational Tools\n",
        "- **spaCy** (`en_core_web_sm`): tokenisation, POS, dependencies  \n",
        "- **scikit-learn**: CountVectorizer, LDA, utilities  \n",
        "- **SciPy**: χ², *t*-tests, Mann–Whitney U, binomial  \n",
        "- **statsmodels** (if available): two-proportion z-tests, Newcombe CIs  \n",
        "- **TextBlob**: exploratory sentiment  \n",
        "- **pandas / numpy**: data handling and numerics\n",
        "\n",
        "---\n",
        "\n",
        "## Research Framework (Summary)\n",
        "- **Evaluation**: instance-aligned F1 (micro/macro) via sentence pairing (exact + fuzzy ≥ 0.92).  \n",
        "- **Categorical inference**: 4-way χ² with per-cell residual z and BH-FDR; two-proportion tests (Newcombe CIs, Cohen’s h); binomial tests vs BNC proportions.  \n",
        "- **Continuous inference**: Welch-*t* and Mann–Whitney-*U* with **Hedges’ g**, **Cliff’s δ**, and **Hodges–Lehmann** shifts (bootstrap CIs).  \n",
        "- **Topics**: LDA per subset using **CountVectorizer** (5 topics × 10 words), with mean topic-mix summaries.  \n",
        "- **Harmonised taxonomy**: **Quasi_Similes** (unified), **Joycean_Framed**, **Joycean_Quasi_Fuzzy**, **Joycean_Silent**, **Standard**, **Uncategorized**.\n",
        "\n",
        "> **Terminology note:** *Quasi_Similes* is the unified tag covering both the BNC’s “Quasi_Similes” and Joyce’s former “Joycean_Quasi,” representing the same phenomenon.\n"
      ],
      "metadata": {
        "id": "C71bNmH3ejeg"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}