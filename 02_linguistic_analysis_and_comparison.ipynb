{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/joyce-dubliners-similes-analysis/blob/main/02_linguistic_analysis_and_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw07RNuhGhxA"
      },
      "source": [
        "# Joyce Simile Research: Comprehensive Linguistic Analysis and Comparison Framework\n",
        "\n",
        "# Abstract\n",
        "\n",
        "This notebook implements a comprehensive computational linguistic analysis framework for comparing simile extraction methodologies in James Joyce's Dubliners. The research examines the effectiveness of manual expert annotation versus algorithmic extraction methods, establishing benchmarks against British National Corpus baseline data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Comprehensive Linguistic Analysis Framework\n",
        "# 6.1 Multi-Dataset Integration\n",
        "The comprehensive analysis pipeline implements robust loading and standardization procedures to ensure data integrity across all four datasets while preserving original categorical frameworks.\n",
        "\n",
        "# 6.2 Advanced Linguistic Feature Extraction\n",
        "Utilizing spaCy and TextBlob, the framework extracts:\n",
        "\n",
        "Syntactic complexity measures through dependency parsing\n",
        "Comparative structural analysis identifying explicit and implicit comparison markers\n",
        "Sentiment and subjectivity scoring for emotional content assessment\n",
        "Pre/post-comparator ratios for structural balance analysis\n",
        "\n",
        "# 6.3 Performance Validation\n",
        "F1 score calculations provide quantitative validation of extraction methodologies against ground truth manual annotations, establishing computational linguistic benchmarks for literary text analysis."
      ],
      "metadata": {
        "id": "zXvVrXssd9qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from difflib import SequenceMatcher\n",
        "from datetime import datetime\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plot libs not used in this cell but kept for notebook continuity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional NLP libs\n",
        "try:\n",
        "    import spacy\n",
        "except Exception:\n",
        "    spacy = None\n",
        "\n",
        "try:\n",
        "    from textblob import TextBlob\n",
        "except Exception:\n",
        "    TextBlob = None\n",
        "\n",
        "print(\"COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS (FIXED)\")\n",
        "print(\"=\" * 75)\n",
        "print(\"Dataset 1: Manual Annotations (Ground Truth - Close Reading)\")\n",
        "print(\"Dataset 2: Rule-Based Extraction (Restrictive - Domain-Informed)\")\n",
        "print(\"Dataset 3: NLP Extraction (Less-Restrictive - PG Dubliners)\")\n",
        "print(\"Dataset 4: BNC Baseline Corpus (Standard English Reference)\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# Initialize spaCy if available\n",
        "nlp = None\n",
        "if spacy is not None:\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        print(\"spaCy pipeline loaded: en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"spaCy model not found; attempting to download…\")\n",
        "        os.system(\"python -m spacy download en_core_web_sm\")\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "            print(\"spaCy pipeline loaded after download: en_core_web_sm\")\n",
        "        except Exception:\n",
        "            print(\"spaCy unavailable; analysis will use simplified methods.\")\n",
        "\n",
        "class ComprehensiveLinguisticComparator:\n",
        "    \"\"\"\n",
        "    Full pipeline:\n",
        "      - robust loading & standardisation\n",
        "      - linguistic feature extraction (spaCy/TextBlob, simplified fallback)\n",
        "      - category harmonisation (MERGES Joycean_Quasi into Quasi_Similes)\n",
        "      - instance-aligned F1 (exact + fuzzy sentence matching)\n",
        "      - reproducibility & environment stamping\n",
        "      - combined CSV export with stable ordering\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = nlp\n",
        "        self.datasets = {}\n",
        "        self.linguistic_features = {}\n",
        "        self.comparison_results = {}\n",
        "\n",
        "        # Reproducibility\n",
        "        self.random_seed = 42\n",
        "        random.seed(self.random_seed)\n",
        "        np.random.seed(self.random_seed)\n",
        "\n",
        "        # Environment info for auditability\n",
        "        tb_ver = \"n/a\"\n",
        "        try:\n",
        "            import textblob as _tb\n",
        "            tb_ver = getattr(_tb, \"__version__\", \"n/a\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.env_info = {\n",
        "            \"python\": sys.version,\n",
        "            \"pandas\": pd.__version__,\n",
        "            \"numpy\": np.__version__,\n",
        "            \"spacy\": getattr(spacy, \"__version__\", \"n/a\") if spacy is not None else \"n/a\",\n",
        "            \"textblob\": tb_ver,\n",
        "        }\n",
        "        print(\"Environment:\", self.env_info)\n",
        "\n",
        "    # ---------- ID / Loading / Standardisation ----------\n",
        "\n",
        "    def _ensure_ids(self, df, dataset_name, prefix=None):\n",
        "        \"\"\"\n",
        "        Ensure a unique, non-null 'Instance_ID' string column exists.\n",
        "        If missing, non-unique, or contains NaNs, regenerate sequential IDs with a readable prefix.\n",
        "        \"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return pd.DataFrame(columns=['Instance_ID'])\n",
        "\n",
        "        short = (prefix or {\n",
        "            'manual': 'MAN',\n",
        "            'rule_based': 'RST',\n",
        "            'nlp': 'NLP',\n",
        "            'bnc': 'BNC'\n",
        "        }.get(dataset_name, dataset_name[:3].upper()))\n",
        "\n",
        "        candidates = ['Instance_ID', 'ID', 'id', 'sentence_id', 'Sentence_ID', 'Index', 'index']\n",
        "        chosen = next((c for c in candidates if c in df.columns), None)\n",
        "        if chosen and chosen != 'Instance_ID':\n",
        "            df = df.rename(columns={chosen: 'Instance_ID'})\n",
        "        elif not chosen:\n",
        "            df['Instance_ID'] = np.nan\n",
        "\n",
        "        # Normalize and test uniqueness\n",
        "        df['Instance_ID'] = df['Instance_ID'].astype(str).replace({'nan': np.nan, '': np.nan})\n",
        "        needs_regen = df['Instance_ID'].isna().any() or (not df['Instance_ID'].is_unique)\n",
        "        if needs_regen:\n",
        "            df['Instance_ID'] = [f\"{short}_{i+1:05d}\" for i in range(len(df))]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _load_manual_dataset_robust(self, file_content):\n",
        "        \"\"\"Robust loader for manual annotations with long quoted Joycean sentences.\"\"\"\n",
        "        import csv\n",
        "        try:\n",
        "            df = pd.read_csv(\n",
        "                file_content, encoding='cp1252', quotechar='\"',\n",
        "                quoting=csv.QUOTE_MINIMAL, skipinitialspace=True, engine='python'\n",
        "            )\n",
        "            if 'Sentence Context' in df.columns:\n",
        "                df = df[df['Sentence Context'].astype(str).str.lower() != 'sentence context'].copy()\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"  pandas (python engine) failed: {e}\")\n",
        "\n",
        "        # Fallback simpler read\n",
        "        try:\n",
        "            df = pd.read_csv(file_content, encoding='cp1252')\n",
        "            if 'Sentence Context' in df.columns:\n",
        "                df = df[df['Sentence Context'].astype(str).str.lower() != 'sentence context'].copy()\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"  pandas (default) failed: {e}\")\n",
        "\n",
        "        print(\"  Manual annotations not found or failed to load.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def load_datasets(self, manual_file=None, rule_based_file=None, nlp_file=None, bnc_file=None):\n",
        "        print(\"\\nLOADING DATASETS WITH FIXED ID HANDLING & EXPLICIT LABELS\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # Manual (close reading)\n",
        "        print(\"Loading manual annotations…\")\n",
        "        self.datasets['manual'] = self._load_manual_dataset_robust(manual_file) if manual_file else pd.DataFrame()\n",
        "        self.datasets['manual'] = self._ensure_ids(self.datasets['manual'], 'manual', prefix='MAN')\n",
        "        if not self.datasets['manual'].empty:\n",
        "            self.datasets['manual']['Original_Dataset'] = 'Manual_CloseReading'\n",
        "\n",
        "        # Rule-based (restrictive)\n",
        "        print(\"Loading rule-based (restrictive)…\")\n",
        "        self.datasets['rule_based'] = pd.read_csv(rule_based_file) if rule_based_file else pd.DataFrame()\n",
        "        self.datasets['rule_based'] = self._ensure_ids(self.datasets['rule_based'], 'rule_based', prefix='RST')\n",
        "        if not self.datasets['rule_based'].empty:\n",
        "            self.datasets['rule_based']['Original_Dataset'] = 'Restrictive_Dubliners'\n",
        "\n",
        "        # NLP (less-restrictive PG)\n",
        "        print(\"Loading NLP (less-restrictive PG)…\")\n",
        "        self.datasets['nlp'] = pd.read_csv(nlp_file) if nlp_file else pd.DataFrame()\n",
        "        self.datasets['nlp'] = self._ensure_ids(self.datasets['nlp'], 'nlp', prefix='NLP')\n",
        "        if not self.datasets['nlp'].empty:\n",
        "            self.datasets['nlp']['Original_Dataset'] = 'NLP_LessRestrictive_PG'\n",
        "\n",
        "        # BNC\n",
        "        print(\"Loading BNC baseline…\")\n",
        "        self.datasets['bnc'] = pd.read_csv(bnc_file, encoding='utf-8') if bnc_file else pd.DataFrame()\n",
        "        self.datasets['bnc'] = self._ensure_ids(self.datasets['bnc'], 'bnc', prefix='BNC')\n",
        "        if not self.datasets['bnc'].empty:\n",
        "            self.datasets['bnc']['Original_Dataset'] = 'BNC_Baseline'\n",
        "\n",
        "        self._standardize_datasets()\n",
        "        self._standardize_categories()\n",
        "\n",
        "        for name, df in self.datasets.items():\n",
        "            print(f\"{name:>12}: rows={len(df):4d}  \"\n",
        "                  f\"missing_IDs={df['Instance_ID'].isna().sum() if 'Instance_ID' in df else 'N/A'}  \"\n",
        "                  f\"missing_Original_Dataset={df['Original_Dataset'].isna().sum() if 'Original_Dataset' in df else 'N/A'}\")\n",
        "        print(f\"Total instances: {sum(len(df) for df in self.datasets.values())}\")\n",
        "\n",
        "    def _standardize_datasets(self):\n",
        "        print(\"Standardizing column names & adding Dataset_Source…\")\n",
        "\n",
        "        # Manual\n",
        "        df = self.datasets.get('manual', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            ren = {\n",
        "                'Category (Framwrok)': 'Category_Framework',\n",
        "                'Comparator Type ': 'Comparator_Type',\n",
        "                'Sentence Context': 'Sentence_Context',\n",
        "                'Page No.': 'Page_Number'\n",
        "            }\n",
        "            df = df.rename(columns={k: v for k, v in ren.items() if k in df.columns})\n",
        "            df['Dataset_Source'] = 'Manual_Expert_Annotation'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['manual'] = df\n",
        "        else:\n",
        "            self.datasets['manual'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context','Page_Number',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # Rule-based\n",
        "        df = self.datasets.get('rule_based', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            df = df.rename(columns={\n",
        "                'Sentence Context': 'Sentence_Context',\n",
        "                'Comparator Type ': 'Comparator_Type',\n",
        "                'Category (Framwrok)': 'Category_Framework'\n",
        "            })\n",
        "            df['Dataset_Source'] = 'Rule_Based_Domain_Informed'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['rule_based'] = df\n",
        "        else:\n",
        "            self.datasets['rule_based'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # NLP (less-restrictive)\n",
        "        df = self.datasets.get('nlp', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            if 'Sentence_Context' not in df.columns:\n",
        "                for c in ['Sentence Context','text','sentence','context','content']:\n",
        "                    if c in df.columns:\n",
        "                        df = df.rename(columns={c: 'Sentence_Context'})\n",
        "                        break\n",
        "            if 'Comparator Type ' in df.columns:\n",
        "                df = df.rename(columns={'Comparator Type ': 'Comparator_Type'})\n",
        "            if 'Category (Framwrok)' in df.columns and 'Category_Framework' not in df.columns:\n",
        "                df = df.rename(columns={'Category (Framwrok)': 'Category_Framework'})\n",
        "            if 'Category_Framework' not in df.columns:\n",
        "                df['Category_Framework'] = 'NLP_Basic_Pattern'\n",
        "            df['Dataset_Source'] = 'NLP_General_Pattern_Recognition'\n",
        "            df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['nlp'] = df\n",
        "        else:\n",
        "            self.datasets['nlp'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # BNC\n",
        "        df = self.datasets.get('bnc', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            if 'Category (Framework)' in df.columns and 'Category_Framework' not in df.columns:\n",
        "                df = df.rename(columns={'Category (Framework)':'Category_Framework'})\n",
        "            if 'Comparator Type' in df.columns and 'Comparator_Type' not in df.columns:\n",
        "                df = df.rename(columns={'Comparator Type':'Comparator_Type'})\n",
        "            if 'Sentence Context' in df.columns and 'Sentence_Context' not in df.columns:\n",
        "                df = df.rename(columns={'Sentence Context':'Sentence_Context'})\n",
        "            df['Dataset_Source'] = 'BNC_Standard_English_Baseline'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['bnc'] = df\n",
        "        else:\n",
        "            self.datasets['bnc'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Sentence_Context','Comparator_Type','Category_Framework',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        print(\"Standardization complete.\")\n",
        "\n",
        "    def _standardize_categories(self):\n",
        "        \"\"\"\n",
        "        Harmonize Category_Framework labels.\n",
        "        IMPORTANT: Merge Joycean_Quasi and its variants into Quasi_Similes (unified quasi-simile phenomenon).\n",
        "        \"\"\"\n",
        "        print(\"Harmonizing Category_Framework labels…\")\n",
        "        mapping = {\n",
        "            # Standard variants\n",
        "            'NLP_Basic': 'Standard',\n",
        "            'NLP_Basic_Pattern': 'Standard',\n",
        "            'Standard_English_Usage': 'Standard',\n",
        "            'Standard': 'Standard',\n",
        "\n",
        "            # Joycean subtypes\n",
        "            'Joycean_Framed': 'Joycean_Framed',\n",
        "            'Joycean_Silent': 'Joycean_Silent',\n",
        "            'Joycean_Quasi_Fuzzy': 'Joycean_Quasi_Fuzzy',\n",
        "            'Joycean-Quasi-Fuzzy': 'Joycean_Quasi_Fuzzy',\n",
        "\n",
        "            # >>> UNIFY all quasi-simile tags here <<<\n",
        "            'Quasi_Similes': 'Quasi_Similes',\n",
        "            'Quasi_Simile': 'Quasi_Similes',     # singular → plural canonical\n",
        "            'Joycean_Quasi': 'Quasi_Similes',    # merge with BNC tag\n",
        "            'Joycean-Quasi': 'Quasi_Similes',    # hyphen variant\n",
        "\n",
        "            # mislabels leaking from dataset names → map to Standard\n",
        "            'NLP_LessRestrictive': 'Standard',\n",
        "            'NLP_General_Pattern': 'Standard',\n",
        "            'Less-Restrictive': 'Standard',\n",
        "\n",
        "            # housekeeping\n",
        "            'Uncategorised': 'Uncategorized',\n",
        "            'nan': 'Uncategorized', 'NaN': 'Uncategorized', '': 'Uncategorized'\n",
        "        }\n",
        "        for name, df in self.datasets.items():\n",
        "            if df.empty or 'Category_Framework' not in df.columns:\n",
        "                continue\n",
        "            df['Category_Framework'] = df['Category_Framework'].astype(str).map(mapping).fillna(df['Category_Framework'])\n",
        "            self.datasets[name] = df\n",
        "        print(\"Category harmonization complete.\")\n",
        "\n",
        "    # ---------- Utilities for matching & normalization ----------\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_sentence_for_match(s: str) -> str:\n",
        "        s = (s or \"\")\n",
        "        s = s.replace(\"—\", \"-\").replace(\"–\", \"-\")\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
        "        # strip quotes; keep colon/semicolon because they matter in Joyce\n",
        "        table = str.maketrans(\"\", \"\", \"\\\"'“”‘’\")\n",
        "        s = s.translate(table)\n",
        "        return s\n",
        "\n",
        "    def _fuzzy_equal(self, a: str, b: str, threshold: float = 0.92) -> bool:\n",
        "        if not a or not b:\n",
        "            return False\n",
        "        ra = self._normalize_sentence_for_match(a)\n",
        "        rb = self._normalize_sentence_for_match(b)\n",
        "        if ra == rb:\n",
        "            return True\n",
        "        return SequenceMatcher(None, ra, rb).ratio() >= threshold\n",
        "\n",
        "    # ---------- Linguistic analysis (spaCy/TextBlob; corrected comparator handling) ----------\n",
        "\n",
        "    def _find_comparator_span(self, doc, comparator_type):\n",
        "        \"\"\"\n",
        "        Return (start_i, end_i) in doc-token indices for the comparator span, inclusive.\n",
        "        Handles 'like', 'as if', 'as though', 'as … as', 'seem*', 'resembl*', punctuation comparators.\n",
        "        Returns None if not found.\n",
        "        \"\"\"\n",
        "        comp = (str(comparator_type) or \"\").strip().lower()\n",
        "\n",
        "        def idx_seq_match(tokens, start, seq):\n",
        "            n = len(seq)\n",
        "            if start + n > len(tokens):\n",
        "                return False\n",
        "            for k in range(n):\n",
        "                if tokens[start + k].text.lower() != seq[k]:\n",
        "                    return False\n",
        "            return True\n",
        "\n",
        "        tokens = list(doc)\n",
        "\n",
        "        # Multiword comparators\n",
        "        if comp in {\"as if\", \"as-if\"}:\n",
        "            seq = [\"as\", \"if\"]\n",
        "            for i in range(len(tokens)-1):\n",
        "                if idx_seq_match(tokens, i, seq):\n",
        "                    return (i, i+1)\n",
        "\n",
        "        if comp in {\"as though\", \"as-though\"}:\n",
        "            seq = [\"as\", \"though\"]\n",
        "            for i in range(len(tokens)-1):\n",
        "                if idx_seq_match(tokens, i, seq):\n",
        "                    return (i, i+1)\n",
        "\n",
        "        # 'as … as' construction (if comparator supplied as 'as')\n",
        "        if comp == \"as\":\n",
        "            as_positions = [i for i,t in enumerate(tokens) if t.text.lower() == \"as\"]\n",
        "            for i in as_positions:\n",
        "                for j in as_positions:\n",
        "                    if j > i and (j - i) <= 8:  # window limit\n",
        "                        return (i, j)\n",
        "            if as_positions:\n",
        "                return (as_positions[0], as_positions[0])\n",
        "\n",
        "        # Single-word comparators\n",
        "        if comp == \"like\":\n",
        "            for i,t in enumerate(tokens):\n",
        "                if t.text.lower() == \"like\":\n",
        "                    return (i, i)\n",
        "\n",
        "        # Lemma families\n",
        "        if comp.startswith(\"resembl\"):\n",
        "            for i,t in enumerate(tokens):\n",
        "                if getattr(t, \"lemma_\", t.text).lower().startswith(\"resembl\"):\n",
        "                    return (i, i)\n",
        "\n",
        "        if comp.startswith(\"seem\"):\n",
        "            for i,t in enumerate(tokens):\n",
        "                if getattr(t, \"lemma_\", t.text).lower().startswith(\"seem\"):\n",
        "                    return (i, i)\n",
        "\n",
        "        # punctuation comparators\n",
        "        if comp in {\"colon\", \":\"}:\n",
        "            for i,t in enumerate(tokens):\n",
        "                if t.text == \":\":\n",
        "                    return (i, i)\n",
        "        if comp in {\"semicolon\", \";\"}:\n",
        "            for i,t in enumerate(tokens):\n",
        "                if t.text == \";\":\n",
        "                    return (i, i)\n",
        "        if comp in {\"ellipsis\", \"...\", \"…\"}:\n",
        "            for i,t in enumerate(tokens):\n",
        "                if t.text in {\"...\", \"…\"}:\n",
        "                    return (i, i)\n",
        "        if comp in {\"en dash\", \"–\", \"—\", \"-\"}:\n",
        "            for i,t in enumerate(tokens):\n",
        "                if t.text in {\"—\", \"–\", \"-\"}:\n",
        "                    return (i, i)\n",
        "\n",
        "        # last resort: exact token string match\n",
        "        for i,t in enumerate(tokens):\n",
        "            if t.text.lower() == comp:\n",
        "                return (i, i)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _pre_post_counts_from_span(self, doc, span):\n",
        "        \"\"\"\n",
        "        Compute pre/post token counts using the same token filtering used for 'total'.\n",
        "        Ensures indices are comparable.\n",
        "        \"\"\"\n",
        "        if span is None:\n",
        "            nonpun = [t for t in doc if not t.is_space and not t.is_punct]\n",
        "            total = len(nonpun)\n",
        "            pre = total // 2\n",
        "            post = total - pre\n",
        "            ratio = (pre / post) if post > 0 else np.nan\n",
        "            return total, pre, post, ratio\n",
        "\n",
        "        start_i, end_i = span\n",
        "        nonpun = []\n",
        "        doc_to_nonpun_idx = {}\n",
        "        for idx, t in enumerate(doc):\n",
        "            if not t.is_space and not t.is_punct:\n",
        "                doc_to_nonpun_idx[idx] = len(nonpun)\n",
        "                nonpun.append(t)\n",
        "\n",
        "        total = len(nonpun)\n",
        "\n",
        "        def nearest_kept(i):\n",
        "            if i in doc_to_nonpun_idx:\n",
        "                return doc_to_nonpun_idx[i]\n",
        "            L = i - 1\n",
        "            R = i + 1\n",
        "            while L >= 0 or R < len(doc):\n",
        "                if L >= 0 and L in doc_to_nonpun_idx:\n",
        "                    return doc_to_nonpun_idx[L]\n",
        "                if R < len(doc) and R in doc_to_nonpun_idx:\n",
        "                    return doc_to_nonpun_idx[R]\n",
        "                L -= 1\n",
        "                R += 1\n",
        "            return 0\n",
        "\n",
        "        start_np = nearest_kept(start_i)\n",
        "        end_np = nearest_kept(end_i)\n",
        "\n",
        "        pre = start_np\n",
        "        post = max(total - (end_np + 1), 0)\n",
        "        ratio = (pre / post) if post > 0 else np.nan\n",
        "        return total, pre, post, ratio\n",
        "\n",
        "    def _analyze_comparative_structure(self, doc, comparator_type):\n",
        "        structure = {\n",
        "            'has_explicit_comparator': False,\n",
        "            'comparator_type': str(comparator_type).strip() or \"Unknown\",\n",
        "            'comparative_adjectives': [],\n",
        "            'superlative_adjectives': [],\n",
        "            'modal_verbs': [],\n",
        "            'epistemic_markers': []\n",
        "        }\n",
        "        for token in doc:\n",
        "            if token.text.lower() in ['like','as','than']:\n",
        "                structure['has_explicit_comparator'] = True\n",
        "            if token.tag_ in ['JJR','RBR']:\n",
        "                structure['comparative_adjectives'].append(token.text)\n",
        "            elif token.tag_ in ['JJS','RBS']:\n",
        "                structure['superlative_adjectives'].append(token.text)\n",
        "            if token.pos_ == 'AUX' and token.text.lower() in ['might','could','would','should','may']:\n",
        "                structure['modal_verbs'].append(token.text)\n",
        "            if token.text.lower() in ['perhaps','maybe','possibly','apparently','seemingly']:\n",
        "                structure['epistemic_markers'].append(token.text)\n",
        "        return structure\n",
        "\n",
        "    def _calculate_syntactic_complexity(self, doc):\n",
        "        def depth(tok, d=0):\n",
        "            if not list(tok.children):\n",
        "                return d\n",
        "            return max(depth(ch, d+1) for ch in tok.children)\n",
        "        roots = [t for t in doc if t.head == t]\n",
        "        if not roots:\n",
        "            return 0\n",
        "        try:\n",
        "            return max(depth(r) for r in roots)\n",
        "        except Exception:\n",
        "            return np.nan\n",
        "\n",
        "    def perform_comprehensive_linguistic_analysis(self):\n",
        "        print(\"\\nPERFORMING LINGUISTIC ANALYSIS\")\n",
        "        print(\"-\" * 35)\n",
        "        if self.nlp is None:\n",
        "            print(\"spaCy unavailable → simplified analysis (token counts + TextBlob sentiment).\")\n",
        "            return self._perform_simplified_analysis()\n",
        "\n",
        "        for name, df in list(self.datasets.items()):\n",
        "            if df.empty:\n",
        "                print(f\"Skipping empty dataset: {name}\")\n",
        "                continue\n",
        "\n",
        "            # Initialize feature containers\n",
        "            n = len(df)\n",
        "            feats = {\n",
        "                'Total_Tokens': [None]*n,\n",
        "                'Pre_Comparator_Tokens': [None]*n,\n",
        "                'Post_Comparator_Tokens': [None]*n,\n",
        "                'Pre_Post_Ratio': [None]*n,\n",
        "                'Lemmatized_Text': [None]*n,\n",
        "                'POS_Tags': [None]*n,\n",
        "                'POS_Distribution': [None]*n,\n",
        "                'Sentiment_Polarity': [None]*n,\n",
        "                'Sentiment_Subjectivity': [None]*n,\n",
        "                'Comparative_Structure': [None]*n,\n",
        "                'Syntactic_Complexity': [None]*n,\n",
        "                'Sentence_Length': [None]*n,\n",
        "                'Adjective_Count': [None]*n,\n",
        "                'Verb_Count': [None]*n,\n",
        "                'Noun_Count': [None]*n,\n",
        "                'Figurative_Density': [None]*n\n",
        "            }\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                sent = str(row.get('Sentence_Context', '') or '').strip()\n",
        "                comp = row.get('Comparator_Type', '')\n",
        "                if not sent:\n",
        "                    continue\n",
        "                try:\n",
        "                    doc = self.nlp(sent)\n",
        "\n",
        "                    # Comparator span & pre/post\n",
        "                    span = self._find_comparator_span(doc, comp)\n",
        "                    total, pre, post, ratio = self._pre_post_counts_from_span(doc, span)\n",
        "\n",
        "                    # Lemmas & POS (exclude spaces/punct for most features)\n",
        "                    tokens_nopunct = [t for t in doc if not t.is_space and not t.is_punct]\n",
        "                    lemmas = [t.lemma_.lower() for t in tokens_nopunct if not t.is_stop]\n",
        "                    pos_tags = [t.pos_ for t in tokens_nopunct]  # no punctuation\n",
        "                    pos_dist = Counter(pos_tags)\n",
        "\n",
        "                    # Sentiment (exploratory only)\n",
        "                    pol = subj = np.nan\n",
        "                    if TextBlob is not None:\n",
        "                        try:\n",
        "                            blob = TextBlob(sent)\n",
        "                            pol, subj = blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                    comp_struct = self._analyze_comparative_structure(doc, comp)\n",
        "                    complexity = self._calculate_syntactic_complexity(doc)\n",
        "                    slen = len(tokens_nopunct)\n",
        "                    adj = sum(1 for t in tokens_nopunct if t.pos_ == 'ADJ')\n",
        "                    vrb = sum(1 for t in tokens_nopunct if t.pos_ == 'VERB')\n",
        "                    nou = sum(1 for t in tokens_nopunct if t.pos_ == 'NOUN')\n",
        "\n",
        "                    figurative_markers = {'like','as','such','seem','appear','resemble','as if','as though'}\n",
        "                    # token-level density (multiword markers counted by token hits)\n",
        "                    fdens = (sum(1 for t in tokens_nopunct if t.text.lower() in figurative_markers) / total) if total else 0\n",
        "\n",
        "                    loc = df.index.get_loc(idx)\n",
        "                    feats['Total_Tokens'][loc] = total\n",
        "                    feats['Pre_Comparator_Tokens'][loc] = pre\n",
        "                    feats['Post_Comparator_Tokens'][loc] = post\n",
        "                    feats['Pre_Post_Ratio'][loc] = ratio\n",
        "                    feats['Lemmatized_Text'][loc] = ' '.join(lemmas)\n",
        "                    feats['POS_Tags'][loc] = '; '.join(pos_tags)\n",
        "                    feats['POS_Distribution'][loc] = dict(pos_dist)\n",
        "                    feats['Sentiment_Polarity'][loc] = pol\n",
        "                    feats['Sentiment_Subjectivity'][loc] = subj\n",
        "                    feats['Comparative_Structure'][loc] = comp_struct\n",
        "                    feats['Syntactic_Complexity'][loc] = complexity\n",
        "                    feats['Sentence_Length'][loc] = slen\n",
        "                    feats['Adjective_Count'][loc] = adj\n",
        "                    feats['Verb_Count'][loc] = vrb\n",
        "                    feats['Noun_Count'][loc] = nou\n",
        "                    feats['Figurative_Density'][loc] = fdens\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error in {name} row {idx}: {e}\")\n",
        "\n",
        "            # Serialize complex columns for CSV\n",
        "            df['POS_Distribution'] = [json.dumps(x) if isinstance(x, dict) else None for x in feats['POS_Distribution']]\n",
        "            df['Comparative_Structure'] = [json.dumps(x) if isinstance(x, dict) else None for x in feats['Comparative_Structure']]\n",
        "            for k, v in feats.items():\n",
        "                if k in ['POS_Distribution','Comparative_Structure']:\n",
        "                    continue\n",
        "                df[k] = v\n",
        "\n",
        "            self.linguistic_features[name] = feats\n",
        "            self.datasets[name] = df\n",
        "            print(f\"Finished linguistic analysis for {name}.\")\n",
        "\n",
        "        print(\"All datasets processed.\")\n",
        "\n",
        "    def _perform_simplified_analysis(self):\n",
        "        for name, df in list(self.datasets.items()):\n",
        "            if df.empty or 'Sentence_Context' not in df.columns:\n",
        "                continue\n",
        "            n = len(df)\n",
        "            df['Total_Tokens'] = [None]*n\n",
        "            df['Pre_Comparator_Tokens'] = [None]*n\n",
        "            df['Post_Comparator_Tokens'] = [None]*n\n",
        "            df['Pre_Post_Ratio'] = [np.nan]*n\n",
        "            df['Sentiment_Polarity'] = [np.nan]*n\n",
        "            df['Sentiment_Subjectivity'] = [np.nan]*n\n",
        "            df['Sentence_Length'] = [None]*n\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                sent = str(row.get('Sentence_Context','') or '').strip()\n",
        "                if not sent:\n",
        "                    continue\n",
        "                tokens = [t for t in sent.split(\" \") if t]\n",
        "                total = len(tokens)\n",
        "                df.loc[idx, 'Total_Tokens'] = total\n",
        "                df.loc[idx, 'Sentence_Length'] = total\n",
        "                if TextBlob is not None:\n",
        "                    try:\n",
        "                        blob = TextBlob(sent)\n",
        "                        df.loc[idx, 'Sentiment_Polarity'] = blob.sentiment.polarity\n",
        "                        df.loc[idx, 'Sentiment_Subjectivity'] = blob.sentiment.subjectivity\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                comp = row.get('Comparator_Type','')\n",
        "                pos = -1\n",
        "                if str(comp).strip():\n",
        "                    try:\n",
        "                        m = re.search(r'\\b' + re.escape(str(comp).strip()) + r'\\b', sent, re.IGNORECASE)\n",
        "                        if m:\n",
        "                            pre_text = sent[:m.start()]\n",
        "                            pos = len([t for t in pre_text.split(\" \") if t])\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                if total > 0 and pos != -1:\n",
        "                    pre, post = pos, total - pos - 1\n",
        "                    df.loc[idx, 'Pre_Comparator_Tokens'] = pre\n",
        "                    df.loc[idx, 'Post_Comparator_Tokens'] = post\n",
        "                    df.loc[idx, 'Pre_Post_Ratio'] = (pre / post) if post > 0 else np.nan\n",
        "            self.datasets[name] = df\n",
        "        print(\"Simplified analysis complete.\")\n",
        "\n",
        "    # ---------- Instance-aligned F1 metrics ----------\n",
        "\n",
        "    def _pair_rows_by_sentence(self, gold_df, pred_df, fuzzy_threshold=0.92):\n",
        "        \"\"\"\n",
        "        Returns list of matched pairs [(gold_idx, pred_idx)] using exact match first,\n",
        "        then fuzzy matching without reuse of already matched rows.\n",
        "        \"\"\"\n",
        "        if gold_df.empty or pred_df.empty:\n",
        "            return [], gold_df, pred_df\n",
        "\n",
        "        gold_df = gold_df.copy()\n",
        "        pred_df = pred_df.copy()\n",
        "        gold_df[\"_norm_sent\"] = gold_df[\"Sentence_Context\"].map(self._normalize_sentence_for_match)\n",
        "        pred_df[\"_norm_sent\"] = pred_df[\"Sentence_Context\"].map(self._normalize_sentence_for_match)\n",
        "\n",
        "        exact_pairs = []\n",
        "        used_pred = set()\n",
        "        pred_lookup = {}\n",
        "        for j, s in pred_df[\"_norm_sent\"].items():\n",
        "            pred_lookup.setdefault(s, []).append(j)\n",
        "\n",
        "        for i, s in gold_df[\"_norm_sent\"].items():\n",
        "            if s in pred_lookup:\n",
        "                js = [jj for jj in pred_lookup[s] if jj not in used_pred]\n",
        "                if js:\n",
        "                    j = js[0]\n",
        "                    used_pred.add(j)\n",
        "                    exact_pairs.append((i, j))\n",
        "\n",
        "        unmatched_gold = [i for i in gold_df.index if i not in {gi for gi,_ in exact_pairs}]\n",
        "        unmatched_pred = [j for j in pred_df.index if j not in used_pred]\n",
        "\n",
        "        fuzzy_pairs = []\n",
        "        for i in unmatched_gold:\n",
        "            best_j = None\n",
        "            best_r = 0.0\n",
        "            gi = gold_df.at[i, \"_norm_sent\"]\n",
        "            for j in unmatched_pred:\n",
        "                r = SequenceMatcher(None, gi, pred_df.at[j, \"_norm_sent\"]).ratio()\n",
        "                if r > best_r:\n",
        "                    best_r = r\n",
        "                    best_j = j\n",
        "            if best_j is not None and best_r >= fuzzy_threshold:\n",
        "                used_pred.add(best_j)\n",
        "                fuzzy_pairs.append((i, best_j))\n",
        "\n",
        "        pairs = exact_pairs + fuzzy_pairs\n",
        "        return pairs, gold_df, pred_df\n",
        "\n",
        "    def _compute_f1_from_pairs(self, gold_df, pred_df, pairs, category_col=\"Category_Framework\"):\n",
        "        \"\"\"\n",
        "        Build TP/FP/FN per category from aligned pairs.\n",
        "        A predicted row is a TP for category c if both gold and pred label == c.\n",
        "        If labels differ, count FP for pred's category and FN for gold's category.\n",
        "        Unmatched gold rows are FNs; unmatched pred rows are FPs.\n",
        "        \"\"\"\n",
        "        cats = sorted(set(gold_df[category_col].astype(str)) | set(pred_df[category_col].astype(str)))\n",
        "        TP = {c:0 for c in cats}\n",
        "        FP = {c:0 for c in cats}\n",
        "        FN = {c:0 for c in cats}\n",
        "\n",
        "        matched_gold = set(i for i,_ in pairs)\n",
        "        matched_pred = set(j for _,j in pairs)\n",
        "\n",
        "        for i,j in pairs:\n",
        "            g = str(gold_df.at[i, category_col])\n",
        "            p = str(pred_df.at[j, category_col])\n",
        "            if p == g:\n",
        "                TP[g] += 1\n",
        "            else:\n",
        "                FP[p] += 1\n",
        "                FN[g] += 1\n",
        "\n",
        "        for i in gold_df.index:\n",
        "            if i not in matched_gold:\n",
        "                g = str(gold_df.at[i, category_col])\n",
        "                FN[g] += 1\n",
        "\n",
        "        for j in pred_df.index:\n",
        "            if j not in matched_pred:\n",
        "                p = str(pred_df.at[j, category_col])\n",
        "                FP[p] += 1\n",
        "\n",
        "        metrics = {}\n",
        "        micro_tp = micro_fp = micro_fn = 0\n",
        "\n",
        "        for c in cats:\n",
        "            tp, fp, fn = TP[c], FP[c], FN[c]\n",
        "            prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "            f1   = (2*prec*rec)/(prec+rec) if (prec+rec) > 0 else 0.0\n",
        "            metrics[c] = {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
        "            micro_tp += tp; micro_fp += fp; micro_fn += fn\n",
        "\n",
        "        micro_prec = micro_tp / (micro_tp + micro_fp) if (micro_tp + micro_fp) > 0 else 0.0\n",
        "        micro_rec  = micro_tp / (micro_tp + micro_fn) if (micro_tp + micro_fn) > 0 else 0.0\n",
        "        micro_f1   = (2*micro_prec*micro_rec)/(micro_prec+micro_rec) if (micro_prec+micro_rec) > 0 else 0.0\n",
        "        macro_f1   = np.mean([m[\"f1\"] for m in metrics.values()]) if metrics else 0.0\n",
        "\n",
        "        return metrics, {\"micro_precision\": micro_prec, \"micro_recall\": micro_rec, \"micro_f1\": micro_f1, \"macro_f1\": macro_f1}\n",
        "\n",
        "    def calculate_corrected_f1_scores(self, fuzzy_threshold=0.92):\n",
        "        print(\"\\nCALCULATING INSTANCE-ALIGNED F1 METRICS\")\n",
        "        print(\"-\" * 44)\n",
        "        print(f\"Assumptions: sentence-level alignment (exact, then fuzzy ≥ {fuzzy_threshold}); category = 'Category_Framework'.\")\n",
        "\n",
        "        manual_df = self.datasets.get('manual', pd.DataFrame())\n",
        "        if manual_df.empty or 'Sentence_Context' not in manual_df or 'Category_Framework' not in manual_df:\n",
        "            print(\"F1 unavailable: manual annotations missing/invalid.\")\n",
        "            self.comparison_results['f1_analysis'] = None\n",
        "            return None, None\n",
        "\n",
        "        out = {}\n",
        "\n",
        "        def eval_one(pred_df, pred_name):\n",
        "            if pred_df.empty or 'Sentence_Context' not in pred_df or 'Category_Framework' not in pred_df:\n",
        "                print(f\"{pred_name}: dataset missing required columns.\")\n",
        "                return None\n",
        "            pairs, gdf, pdf = self._pair_rows_by_sentence(manual_df, pred_df, fuzzy_threshold=fuzzy_threshold)\n",
        "            metrics, overall = self._compute_f1_from_pairs(gdf, pdf, pairs)\n",
        "            print(f\"{pred_name}: pairs={len(pairs)}  micro-F1={overall['micro_f1']:.3f}  macro-F1={overall['macro_f1']:.3f}\")\n",
        "            return {\"pairs\": len(pairs), \"category_metrics\": metrics, \"overall\": overall}\n",
        "\n",
        "        rb = self.datasets.get('rule_based', pd.DataFrame())\n",
        "        nl = self.datasets.get('nlp', pd.DataFrame())\n",
        "\n",
        "        out['rule_based_vs_manual'] = eval_one(rb, \"Rule-Based vs Manual\")\n",
        "        out['nlp_vs_manual'] = eval_one(nl, \"NLP vs Manual\")\n",
        "\n",
        "        self.comparison_results['f1_analysis'] = out\n",
        "        primary = out['rule_based_vs_manual']['overall']['micro_f1'] if out.get('rule_based_vs_manual') else None\n",
        "        return out, primary\n",
        "\n",
        "    # ---------- Save / Export ----------\n",
        "\n",
        "    def save_comprehensive_results(self, output_path=\"comprehensive_linguistic_analysis_corrected.csv\"):\n",
        "        print(\"\\nSAVING COMPREHENSIVE RESULTS …\")\n",
        "        frames = []\n",
        "        for name, df in self.datasets.items():\n",
        "            if df is None or df.empty:\n",
        "                continue\n",
        "            d = df.copy()\n",
        "            for col, default in [\n",
        "                ('Original_Dataset', name),\n",
        "                ('Instance_ID', None),\n",
        "                ('Sentence_Context', None),\n",
        "                ('Category_Framework', None),\n",
        "                ('Comparator_Type', None)\n",
        "            ]:\n",
        "                if col not in d.columns:\n",
        "                    d[col] = default\n",
        "\n",
        "            if d['Instance_ID'].isna().any() or (not d['Instance_ID'].astype(str).is_unique):\n",
        "                d = self._ensure_ids(d, name)\n",
        "\n",
        "            base = ['Instance_ID','Original_Dataset','Sentence_Context','Category_Framework','Comparator_Type']\n",
        "            others = [c for c in d.columns if c not in base]\n",
        "            d = d[base + others]\n",
        "            frames.append(d)\n",
        "\n",
        "        if not frames:\n",
        "            print(\"No data to save.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        combined = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "        # Stable sort: Manual → Restrictive → Less-Restrictive PG → BNC\n",
        "        order = {\n",
        "            'Manual_CloseReading': 1,\n",
        "            'Restrictive_Dubliners': 2,\n",
        "            'NLP_LessRestrictive_PG': 3,\n",
        "            'BNC_Baseline': 4\n",
        "        }\n",
        "        combined['__order__'] = combined['Original_Dataset'].map(order).fillna(99).astype(int)\n",
        "\n",
        "        def _id_numeric_tail(x):\n",
        "            m = re.search(r'(\\d+)$', str(x))\n",
        "            return int(m.group(1)) if m else 0\n",
        "\n",
        "        combined = combined.sort_values(\n",
        "            by=['__order__','Original_Dataset','Instance_ID'],\n",
        "            key=lambda s: s.map(_id_numeric_tail) if s.name == 'Instance_ID' else s\n",
        "        ).drop(columns='__order__')\n",
        "\n",
        "        combined.to_csv(output_path, index=False)\n",
        "        print(f\"Saved: {output_path}\")\n",
        "        print(\"Integrity:\",\n",
        "              \"missing Instance_ID =\", combined['Instance_ID'].isna().sum(),\n",
        "              \"| missing Original_Dataset =\", combined['Original_Dataset'].isna().sum(),\n",
        "              \"| rows =\", len(combined))\n",
        "        print(\"Environment (for reproducibility):\", self.env_info)\n",
        "        return combined\n",
        "\n",
        "\n",
        "# ========= RUN THE PIPELINE (with your filenames) =========\n",
        "# manual_path = \"All Similes - Dubliners cont.csv\"           # close reading (manual)\n",
        "# rule_based_path = \"dubliners_corrected_extraction.csv\"     # restrictive\n",
        "# nlp_path = \"dubliners_nlp_basic_extraction.csv\"            # less-restrictive PG Dubliners\n",
        "# bnc_processed_path = \"bnc_processed_similes.csv\"           # BNC baseline\n",
        "\n",
        "# Example of how to use with uploaded files:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# manual_file_content = uploaded['All Similes - Dubliners cont.csv'] if 'All Similes - Dubliners cont.csv' in uploaded else None\n",
        "# rule_based_file_content = uploaded['dubliners_corrected_extraction.csv'] if 'dubliners_corrected_extraction.csv' in uploaded else None\n",
        "# nlp_file_content = uploaded['dubliners_nlp_basic_extraction.csv'] if 'dubliners_nlp_basic_extraction.csv' in uploaded else None\n",
        "# bnc_file_content = uploaded['bnc_processed_similes.csv'] if 'bnc_processed_similes.csv' in uploaded else None\n",
        "\n",
        "# Temporarily using existing files for demonstration\n",
        "manual_file_content = \"/content/All Similes - Dubliners cont.csv\"\n",
        "rule_based_file_content = \"/content/dubliners_rulebased_extraction.csv\"\n",
        "nlp_file_content = \"/content/dubliners_nlp_less_restrictive_extraction.csv\"\n",
        "bnc_file_content = \"/content/bnc_processed_similes.csv\"\n",
        "\n",
        "comparator = ComprehensiveLinguisticComparator()\n",
        "comparator.load_datasets(manual_file_content, rule_based_file_content, nlp_file_content, bnc_file_content)\n",
        "comparator.perform_comprehensive_linguistic_analysis()\n",
        "f1_analysis, primary_f1 = comparator.calculate_corrected_f1_scores()  # instance-aligned F1\n",
        "results_df = comparator.save_comprehensive_results(\"comprehensive_linguistic_analysis_corrected.csv\")\n",
        "\n",
        "print(\"\\nPIPELINE COMPLETED.\")\n"
      ],
      "metadata": {
        "id": "kn9hj4I8zAEU",
        "outputId": "6bc8f368-6f2a-4cb8-fe33-3ddf253fbc3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS (FIXED)\n",
            "===========================================================================\n",
            "Dataset 1: Manual Annotations (Ground Truth - Close Reading)\n",
            "Dataset 2: Rule-Based Extraction (Restrictive - Domain-Informed)\n",
            "Dataset 3: NLP Extraction (Less-Restrictive - PG Dubliners)\n",
            "Dataset 4: BNC Baseline Corpus (Standard English Reference)\n",
            "===========================================================================\n",
            "spaCy pipeline loaded: en_core_web_sm\n",
            "Environment: {'python': '3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]', 'pandas': '2.2.2', 'numpy': '2.0.2', 'spacy': '3.8.7', 'textblob': 'n/a'}\n",
            "\n",
            "LOADING DATASETS WITH FIXED ID HANDLING & EXPLICIT LABELS\n",
            "----------------------------------------------------------------------\n",
            "Loading manual annotations…\n",
            "Loading rule-based (restrictive)…\n",
            "Loading NLP (less-restrictive PG)…\n",
            "Loading BNC baseline…\n",
            "Standardizing column names & adding Dataset_Source…\n",
            "Standardization complete.\n",
            "Harmonizing Category_Framework labels…\n",
            "Category harmonization complete.\n",
            "      manual: rows= 184  missing_IDs=0  missing_Original_Dataset=0\n",
            "  rule_based: rows= 218  missing_IDs=0  missing_Original_Dataset=0\n",
            "         nlp: rows= 330  missing_IDs=0  missing_Original_Dataset=0\n",
            "         bnc: rows= 200  missing_IDs=0  missing_Original_Dataset=0\n",
            "Total instances: 932\n",
            "\n",
            "PERFORMING LINGUISTIC ANALYSIS\n",
            "-----------------------------------\n",
            "Finished linguistic analysis for manual.\n",
            "Finished linguistic analysis for rule_based.\n",
            "Finished linguistic analysis for nlp.\n",
            "Finished linguistic analysis for bnc.\n",
            "All datasets processed.\n",
            "\n",
            "CALCULATING INSTANCE-ALIGNED F1 METRICS\n",
            "--------------------------------------------\n",
            "Assumptions: sentence-level alignment (exact, then fuzzy ≥ 0.92); category = 'Category_Framework'.\n",
            "Rule-Based vs Manual: pairs=110  micro-F1=0.343  macro-F1=0.178\n",
            "NLP vs Manual: pairs=127  micro-F1=0.292  macro-F1=0.059\n",
            "\n",
            "SAVING COMPREHENSIVE RESULTS …\n",
            "Saved: comprehensive_linguistic_analysis_corrected.csv\n",
            "Integrity: missing Instance_ID = 0 | missing Original_Dataset = 0 | rows = 932\n",
            "Environment (for reproducibility): {'python': '3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]', 'pandas': '2.2.2', 'numpy': '2.0.2', 'spacy': '3.8.7', 'textblob': 'n/a'}\n",
            "\n",
            "PIPELINE COMPLETED.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Statistical Significance Testing\n",
        "# 7.1 Multi-Group Comparative Analysis\n",
        "The statistical analysis distinguishes between Joyce Manual, Joyce Restrictive, Joyce Less-Restrictive, and BNC subsets to provide granular assessment of methodological differences.\n",
        "\n",
        "# 7.2 Robust Statistical Framework\n",
        "Implementation includes:\n",
        "\n",
        "Four-way chi-square analysis for categorical distribution testing\n",
        "Newcombe-Wilson confidence intervals for two-proportion comparisons\n",
        "Binomial testing against BNC reference proportions\n",
        "Welch t-tests and Mann-Whitney U tests for continuous feature assessment\n",
        "\n",
        "# 7.3 Topic Modeling Integration\n",
        "Latent Dirichlet Allocation provides thematic analysis across all dataset subsets, revealing content-based distinctions complementing statistical findings."
      ],
      "metadata": {
        "id": "ft6Meu_eeMDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ROBUST STATISTICAL SIGNIFICANCE + TOPIC MODELLING (Joyce subsets vs BNC)\n",
        "# =============================================================================\n",
        "\n",
        "import os, json, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from math import asin, sqrt\n",
        "from scipy.stats import chi2_contingency, mannwhitneyu, ttest_ind, binomtest, norm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "try:\n",
        "    from statsmodels.stats.proportion import proportions_ztest, confint_proportions_2indep\n",
        "    _HAS_STATSMODELS = True\n",
        "except Exception:\n",
        "    _HAS_STATSMODELS = False\n",
        "\n",
        "# ---------- Setup ----------\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "out_dir = os.path.join(\"analysis_outputs\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "print(\"\\nROBUST STATISTICAL ANALYSIS (Joyce subsets vs BNC)\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# --- Sanity: results_df must exist from Cell 1 ---\n",
        "if 'results_df' not in globals() or results_df is None or results_df.empty:\n",
        "    raise RuntimeError(\"results_df not found or empty. Run Cell 1 first.\")\n",
        "\n",
        "# --- Group labels (from Cell 1 'Original_Dataset') ---\n",
        "LABELS = {\n",
        "    \"Manual_CloseReading\":      \"Joyce_Manual\",\n",
        "    \"Restrictive_Dubliners\":    \"Joyce_Restrictive\",\n",
        "    \"NLP_LessRestrictive_PG\":   \"Joyce_LessRestrictive\",\n",
        "    \"BNC_Baseline\":             \"BNC\"\n",
        "}\n",
        "\n",
        "df = results_df.copy()\n",
        "if \"Original_Dataset\" not in df.columns:\n",
        "    raise RuntimeError(\"results_df is missing 'Original_Dataset'.\")\n",
        "if \"Category_Framework\" not in df.columns:\n",
        "    raise RuntimeError(\"results_df is missing 'Category_Framework'.\")\n",
        "\n",
        "df[\"__Group__\"] = df[\"Original_Dataset\"].map(LABELS).fillna(df[\"Original_Dataset\"])\n",
        "\n",
        "# --- Category whitelist (Joycean_Quasi was merged → keep unified 'Quasi_Similes') ---\n",
        "KNOWN_CATEGORIES = {\n",
        "    'Standard',\n",
        "    'Quasi_Similes',          # unified quasi-simile label\n",
        "    'Joycean_Quasi_Fuzzy',\n",
        "    'Joycean_Framed',\n",
        "    'Joycean_Silent',\n",
        "    'Uncategorized'\n",
        "}\n",
        "\n",
        "bad_mask = ~df[\"Category_Framework\"].isin(KNOWN_CATEGORIES)\n",
        "if bad_mask.any():\n",
        "    dropped = int(bad_mask.sum())\n",
        "    print(f\"[WARN] Dropping {dropped} rows with unexpected Category_Framework values: \"\n",
        "          f\"{sorted(df.loc[bad_mask, 'Category_Framework'].astype(str).unique())}\")\n",
        "    df = df.loc[~bad_mask].copy()\n",
        "\n",
        "# --- Quick distribution sanity table (post-harmonisation) ---\n",
        "dist = (df.pivot_table(index=\"Category_Framework\",\n",
        "                       columns=\"Original_Dataset\",\n",
        "                       values=\"Instance_ID\",\n",
        "                       aggfunc=\"count\", fill_value=0)\n",
        "          .assign(Total=lambda x: x.sum(1))\n",
        "          .sort_values(\"Total\", ascending=False))\n",
        "print(\"\\nCategory distribution after harmonisation (counts):\")\n",
        "print(dist)\n",
        "\n",
        "# --- Split groups ---\n",
        "groups = {\n",
        "    \"Joyce_Manual\":          df[df[\"__Group__\"]==\"Joyce_Manual\"],\n",
        "    \"Joyce_Restrictive\":     df[df[\"__Group__\"]==\"Joyce_Restrictive\"],\n",
        "    \"Joyce_LessRestrictive\": df[df[\"__Group__\"]==\"Joyce_LessRestrictive\"],\n",
        "    \"BNC\":                   df[df[\"__Group__\"]==\"BNC\"]\n",
        "}\n",
        "for gname, gdf in groups.items():\n",
        "    print(f\"{gname:22s}: {len(gdf)} rows\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def p_adjust_bh(p):\n",
        "    \"\"\"Benjamini–Hochberg FDR for a 1D array-like of p-values.\"\"\"\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    n = p.size\n",
        "    order = np.argsort(p)\n",
        "    ranked = np.empty(n, dtype=float)\n",
        "    cummin = 1.0\n",
        "    for i, idx in enumerate(order[::-1], start=1):\n",
        "        rank = n - i + 1\n",
        "        val = p[idx] * n / rank\n",
        "        cummin = min(cummin, val)\n",
        "        ranked[idx] = cummin\n",
        "    return np.minimum(ranked, 1.0)\n",
        "\n",
        "def cramers_v(chi2, n, r, c):\n",
        "    \"\"\"Cramér's V for r x c table.\"\"\"\n",
        "    if n <= 0 or min(r, c) <= 1:\n",
        "        return np.nan\n",
        "    return sqrt(chi2 / (n * (min(r, c) - 1)))\n",
        "\n",
        "def cohens_h(p1, p2):\n",
        "    \"\"\"Cohen's h for proportions.\"\"\"\n",
        "    p1 = min(max(float(p1), 0.0), 1.0)\n",
        "    p2 = min(max(float(p2), 0.0), 1.0)\n",
        "    return 2 * (asin(sqrt(p1)) - asin(sqrt(p2)))\n",
        "\n",
        "def hedges_g(a, b):\n",
        "    \"\"\"Hedges' g (small-sample corrected Cohen's d).\"\"\"\n",
        "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
        "    na, nb = len(a), len(b)\n",
        "    if na < 2 or nb < 2:\n",
        "        return np.nan\n",
        "    s1 = np.var(a, ddof=1); s2 = np.var(b, ddof=1)\n",
        "    pooled = ((na-1)*s1 + (nb-1)*s2) / (na+nb-2) if (na+nb-2) > 0 else np.nan\n",
        "    if not np.isfinite(pooled) or pooled <= 0:\n",
        "        return np.nan\n",
        "    d = (np.mean(a) - np.mean(b)) / np.sqrt(pooled)\n",
        "    J = 1 - (3 / (4*(na+nb) - 9)) if (na+nb) > 2 else 1.0\n",
        "    return d * J\n",
        "\n",
        "def cliffs_delta_from_u(a, b):\n",
        "    \"\"\"Cliff's delta via Mann–Whitney U (orientation A > B).\"\"\"\n",
        "    a = pd.Series(a).dropna().to_numpy()\n",
        "    b = pd.Series(b).dropna().to_numpy()\n",
        "    if len(a) == 0 or len(b) == 0:\n",
        "        return np.nan\n",
        "    u_ab, _ = mannwhitneyu(a, b, alternative=\"greater\")\n",
        "    return (2 * u_ab) / (len(a)*len(b)) - 1\n",
        "\n",
        "# ---------- 1) 4-way Chi-square on Category_Framework ----------\n",
        "cats = sorted(KNOWN_CATEGORIES)\n",
        "contingency_4way = pd.DataFrame(\n",
        "    {\n",
        "        \"Joyce_Manual\":          [groups[\"Joyce_Manual\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "        \"Joyce_Restrictive\":     [groups[\"Joyce_Restrictive\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "        \"Joyce_LessRestrictive\": [groups[\"Joyce_LessRestrictive\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "        \"BNC\":                   [groups[\"BNC\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in cats],\n",
        "    },\n",
        "    index=cats\n",
        ")\n",
        "\n",
        "sim_used = False\n",
        "try:\n",
        "    chi2_4, p_4, dof_4, exp_4 = chi2_contingency(contingency_4way, correction=False)\n",
        "    if (np.asarray(exp_4) < 5).any():\n",
        "        try:\n",
        "            chi2_4, p_4, dof_4, exp_4 = chi2_contingency(\n",
        "                contingency_4way, correction=False, simulate_pval=True, num_simulation=5000\n",
        "            )\n",
        "            sim_used = True\n",
        "        except TypeError:\n",
        "            pass\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"chi2_contingency failed: {e}\")\n",
        "\n",
        "N_total = contingency_4way.values.sum()\n",
        "V_4 = cramers_v(chi2_4, N_total, *contingency_4way.shape)\n",
        "\n",
        "print(\"\\n4-way Chi-square on Category_Framework (Joyce subsets vs BNC):\")\n",
        "print(f\"χ² = {chi2_4:.4f} | df = {dof_4} | p = {p_4:.6f} | Cramér’s V = {V_4:.3f} | Monte-Carlo={sim_used}\")\n",
        "\n",
        "# Save contingency + expected + standardized residuals + per-cell p-values (FDR)\n",
        "path_cont_4 = os.path.join(out_dir, f\"chi2_contingency_by_subset_{ts}.csv\")\n",
        "path_exp_4  = os.path.join(out_dir, f\"chi2_expected_by_subset_{ts}.csv\")\n",
        "contingency_4way.to_csv(path_cont_4)\n",
        "exp_df_4 = pd.DataFrame(exp_4, index=cats, columns=contingency_4way.columns)\n",
        "exp_df_4.to_csv(path_exp_4)\n",
        "\n",
        "pearson_z = (contingency_4way - exp_df_4) / np.sqrt(exp_df_4.replace(0, np.nan))\n",
        "cell_pvals = pearson_z.applymap(lambda z: 2*norm.sf(abs(z)) if pd.notnull(z) else np.nan)\n",
        "\n",
        "flat = cell_pvals.values.flatten()\n",
        "mask = np.isfinite(flat)\n",
        "adj = np.full_like(flat, np.nan, dtype=float)\n",
        "if mask.any():\n",
        "    adj[mask] = p_adjust_bh(flat[mask])\n",
        "cell_pvals_adj = pd.DataFrame(adj.reshape(cell_pvals.shape), index=cell_pvals.index, columns=cell_pvals.columns)\n",
        "\n",
        "path_resid_z = os.path.join(out_dir, f\"chi2_pearson_z_by_subset_{ts}.csv\")\n",
        "path_resid_p = os.path.join(out_dir, f\"chi2_cell_p_by_subset_{ts}.csv\")\n",
        "path_resid_padj = os.path.join(out_dir, f\"chi2_cell_padj_BH_by_subset_{ts}.csv\")\n",
        "pearson_z.to_csv(path_resid_z)\n",
        "cell_pvals.to_csv(path_resid_p)\n",
        "cell_pvals_adj.to_csv(path_resid_padj)\n",
        "\n",
        "# Print the strongest drivers (optional but helpful)\n",
        "absz = pearson_z.abs().stack().sort_values(ascending=False)\n",
        "print(\"\\nTop 10 standardized residuals (|z|):\")\n",
        "for (cat, grp), z in absz.head(10).items():\n",
        "    obs = contingency_4way.loc[cat, grp]\n",
        "    exp = exp_df_4.loc[cat, grp]\n",
        "    print(f\"  {grp:22s} | {cat:20s}  z={z:6.2f}  obs={obs} exp={exp:.1f}\")\n",
        "\n",
        "# ---------- 2) Two-proportion tests (each Joyce subset vs BNC) ----------\n",
        "print(\"\\nTwo-proportion tests (Newcombe–Wilson) for each Joyce subset vs BNC:\")\n",
        "two_prop_rows = []\n",
        "bnc_total = len(groups[\"BNC\"])\n",
        "bnc_counts = groups[\"BNC\"][\"Category_Framework\"].value_counts()\n",
        "\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "    subset_total = len(groups[subset])\n",
        "    subset_counts = groups[subset][\"Category_Framework\"].value_counts()\n",
        "    for cat in cats:\n",
        "        cA = subset_counts.get(cat,0); nA = subset_total\n",
        "        cB = bnc_counts.get(cat,0);    nB = bnc_total\n",
        "        pA = cA/nA if nA>0 else 0.0\n",
        "        pB = cB/nB if nB>0 else 0.0\n",
        "        row = {\"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset, \"Category\":cat,\n",
        "               \"count_A\":cA, \"n_A\":nA, \"prop_A\":pA, \"count_B\":cB, \"n_B\":nB, \"prop_B\":pB,\n",
        "               \"cohens_h\": float(cohens_h(pA, pB))}\n",
        "        # Skip boundary cases where both groups are 0 or 1 for this category\n",
        "        if (cA == 0 and cB == 0) or (cA == nA and cB == nB) or (nA == 0 or nB == 0):\n",
        "            row.update({\"z\": np.nan, \"p_value\": 1.0, \"CI_low\": np.nan, \"CI_up\": np.nan})\n",
        "            print(f\"  {subset:22s} | {cat:20s} (both groups at boundary → skip) h={row['cohens_h']:.3f}\")\n",
        "            two_prop_rows.append(row); continue\n",
        "        if _HAS_STATSMODELS:\n",
        "            try:\n",
        "                z, pz = proportions_ztest(np.array([cA,cB]), np.array([nA,nB]))\n",
        "            except Exception:\n",
        "                # Haldane–Anscombe continuity correction fallback\n",
        "                pA_ha = (cA + 0.5) / (nA + 1)\n",
        "                pB_ha = (cB + 0.5) / (nB + 1)\n",
        "                se = np.sqrt(pA_ha*(1-pA_ha)/(nA+1) + pB_ha*(1-pB_ha)/(nB+1))\n",
        "                z = (pA_ha - pB_ha) / se if se>0 else np.nan\n",
        "                pz = 2*norm.sf(abs(z)) if np.isfinite(z) else np.nan\n",
        "            ci_low, ci_up = (np.nan, np.nan)\n",
        "            try:\n",
        "                ci_low, ci_up = confint_proportions_2indep(cA, nA, cB, nB, method=\"newcombe\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            row.update({\"z\":float(z), \"p_value\":float(pz), \"CI_low\":float(ci_low), \"CI_up\":float(ci_up)})\n",
        "            print(f\"  {subset:22s} | {cat:20s} z={z:6.3f} p={pz:.6g} CI[{ci_low:.3f},{ci_up:.3f}] h={row['cohens_h']:.3f}\")\n",
        "        else:\n",
        "            row.update({\"z\":np.nan, \"p_value\":np.nan, \"CI_low\":np.nan, \"CI_up\":np.nan})\n",
        "            print(f\"  {subset:22s} | {cat:20s} (statsmodels unavailable → skipping z/CI) h={row['cohens_h']:.3f}\")\n",
        "        two_prop_rows.append(row)\n",
        "\n",
        "two_prop_df = pd.DataFrame(two_prop_rows)\n",
        "if \"p_value\" in two_prop_df.columns:\n",
        "    two_prop_df[\"p_adj_BH\"] = p_adjust_bh(two_prop_df[\"p_value\"].fillna(1.0).to_numpy())\n",
        "path_two_prop = os.path.join(out_dir, f\"two_prop_newcombe_by_subset_{ts}.csv\")\n",
        "two_prop_df.to_csv(path_two_prop, index=False)\n",
        "\n",
        "# ---------- 3) Binomial tests (subset vs BNC reference proportion) ----------\n",
        "print(\"\\nBinomial tests (each Joyce subset vs BNC category proportion):\")\n",
        "binom_rows = []\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "    nA = len(groups[subset])\n",
        "    subset_counts = groups[subset][\"Category_Framework\"].value_counts()\n",
        "    for cat in cats:\n",
        "        cA = subset_counts.get(cat,0)\n",
        "        cB = bnc_counts.get(cat,0); nB = bnc_total\n",
        "        p_ref = (cB/nB) if nB>0 else 0.0\n",
        "        if nA>0 and 0 < p_ref < 1:\n",
        "            bt = binomtest(cA, n=nA, p=p_ref)\n",
        "            pv = bt.pvalue\n",
        "            binom_rows.append({\"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset, \"Category\":cat,\n",
        "                               \"count_A\":cA, \"n_A\":nA, \"p_ref_BNC\":p_ref, \"p_value\":pv})\n",
        "            print(f\"  {subset:22s} | {cat:20s} {cA}/{nA} vs p_ref={p_ref:.4f} p={pv:.6g}\")\n",
        "        else:\n",
        "            binom_rows.append({\"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset, \"Category\":cat,\n",
        "                               \"count_A\":cA, \"n_A\":nA, \"p_ref_BNC\":p_ref, \"p_value\":np.nan})\n",
        "\n",
        "binom_df = pd.DataFrame(binom_rows)\n",
        "if \"p_value\" in binom_df.columns:\n",
        "    binom_df[\"p_adj_BH\"] = p_adjust_bh(binom_df[\"p_value\"].fillna(1.0).to_numpy())\n",
        "path_binom = os.path.join(out_dir, f\"binomial_tests_by_subset_{ts}.csv\")\n",
        "binom_df.to_csv(path_binom, index=False)\n",
        "\n",
        "# ---------- 4) Continuous features (subset vs BNC) ----------\n",
        "print(\"\\nContinuous features (Welch t + Mann–Whitney U) each Joyce subset vs BNC:\")\n",
        "continuous_feats = [\"Sentence_Length\",\"Pre_Post_Ratio\",\"Sentiment_Polarity\",\"Sentiment_Subjectivity\"]\n",
        "cont_rows = []\n",
        "\n",
        "for feat in continuous_feats:\n",
        "    for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "        A = pd.to_numeric(groups[subset].get(feat, pd.Series(dtype=float)), errors=\"coerce\").dropna()\n",
        "        B = pd.to_numeric(groups[\"BNC\"].get(feat, pd.Series(dtype=float)), errors=\"coerce\").dropna()\n",
        "        row = {\"Feature\":feat, \"Comparison\":f\"{subset}_vs_BNC\", \"Subset\":subset,\n",
        "               \"A_n\":int(A.shape[0]), \"B_n\":int(B.shape[0])}\n",
        "        if len(A)>10 and len(B)>10:\n",
        "            t,p_t = ttest_ind(A,B,equal_var=False)\n",
        "            u,p_u = mannwhitneyu(A,B,alternative=\"two-sided\")\n",
        "            row.update({\n",
        "                \"A_mean\":float(A.mean()), \"A_median\":float(A.median()),\n",
        "                \"B_mean\":float(B.mean()), \"B_median\":float(B.median()),\n",
        "                \"t_stat\":float(t), \"t_pvalue\":float(p_t),\n",
        "                \"U_stat\":float(u), \"U_pvalue\":float(p_u),\n",
        "                \"hedges_g\": float(hedges_g(A, B)),\n",
        "                \"cliffs_delta\": float(cliffs_delta_from_u(A, B))\n",
        "            })\n",
        "            print(f\"  {feat:22s} | {subset:22s} t={t:7.3f} p={p_t:.6g} | U={u:9.1f} p={p_u:.6g} | g={row['hedges_g']:.3f} δ={row['cliffs_delta']:.3f}\")\n",
        "        cont_rows.append(row)\n",
        "\n",
        "cont_df = pd.DataFrame(cont_rows)\n",
        "if \"t_pvalue\" in cont_df.columns:\n",
        "    cont_df[\"t_padj_BH\"] = p_adjust_bh(cont_df[\"t_pvalue\"].fillna(1.0).to_numpy())\n",
        "if \"U_pvalue\" in cont_df.columns:\n",
        "    cont_df[\"U_padj_BH\"] = p_adjust_bh(cont_df[\"U_pvalue\"].fillna(1.0).to_numpy())\n",
        "\n",
        "path_cont = os.path.join(out_dir, f\"continuous_tests_by_subset_{ts}.csv\")\n",
        "cont_df.to_csv(path_cont, index=False)\n",
        "\n",
        "# ---------- 5) Topic modelling (per subset + BNC) ----------\n",
        "print(\"\\nTOPIC MODELLING (per subset + BNC)\")\n",
        "\n",
        "def prepare_corpus(gdf):\n",
        "    \"\"\"Prefer Lemmatized_Text from Cell 1; fallback to Sentence_Context.\"\"\"\n",
        "    if \"Lemmatized_Text\" in gdf.columns and gdf[\"Lemmatized_Text\"].notna().any():\n",
        "        texts = gdf[\"Lemmatized_Text\"].fillna(\"\").astype(str).tolist()\n",
        "    else:\n",
        "        texts = gdf[\"Sentence_Context\"].fillna(\"\").astype(str).tolist() if \"Sentence_Context\" in gdf.columns else []\n",
        "    return [t for t in texts if t.strip()]\n",
        "\n",
        "def lda_topics(corpus, n_topics=5, n_top_words=10, max_df=0.85, min_df=2, max_features=5000, random_state=RANDOM_STATE):\n",
        "    if not corpus:\n",
        "        return None, None, None\n",
        "    vectorizer = CountVectorizer(\n",
        "        max_df=max_df, min_df=min_df, max_features=max_features,\n",
        "        stop_words=\"english\",\n",
        "        token_pattern=r\"(?u)\\b[^\\W\\d_][^\\W\\d_]+\\b\"\n",
        "    )\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    lda = LatentDirichletAllocation(\n",
        "        n_components=n_topics, random_state=random_state, learning_method=\"batch\"\n",
        "    )\n",
        "    lda.fit(X)\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for comp in lda.components_:\n",
        "        top_idx = comp.argsort()[:-n_top_words-1:-1]\n",
        "        topics.append([terms[i] for i in top_idx])\n",
        "    doc_topic = lda.transform(X)  # rows sum ~1\n",
        "    return topics, doc_topic, terms\n",
        "\n",
        "topics_summary = {\"params\":{\"n_topics\":5,\"n_top_words\":10,\"vectorizer\":\"CountVectorizer\"}, \"groups\":{}}\n",
        "topic_rows = []\n",
        "topicmix_rows = []\n",
        "\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\",\"BNC\"]:\n",
        "    corpus = prepare_corpus(groups[subset])\n",
        "    if corpus:\n",
        "        tpcs, doc_topic, terms = lda_topics(corpus, n_topics=5, n_top_words=10)\n",
        "        topics_summary[\"groups\"][subset] = tpcs if tpcs is not None else []\n",
        "        if tpcs:\n",
        "            for i, words in enumerate(tpcs, 1):\n",
        "                topic_rows.append({\"Group\":subset, \"Topic\":i, \"Top_Words\":\", \".join(words)})\n",
        "            topic_means = doc_topic.mean(axis=0) if doc_topic is not None else None\n",
        "            if topic_means is not None:\n",
        "                for i, val in enumerate(topic_means, 1):\n",
        "                    topicmix_rows.append({\"Group\":subset, \"Topic\":i, \"Mean_Weight\":float(val)})\n",
        "        print(f\"  Topics generated for {subset}: {len(tpcs) if tpcs else 0}\")\n",
        "    else:\n",
        "        topics_summary[\"groups\"][subset] = []\n",
        "        print(f\"  Not enough text for {subset}\")\n",
        "\n",
        "topics_json_path = os.path.join(out_dir, f\"lda_topics_by_subset_{ts}.json\")\n",
        "with open(topics_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(topics_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "topics_csv = pd.DataFrame(topic_rows, columns=[\"Group\",\"Topic\",\"Top_Words\"])\n",
        "topics_csv_path = os.path.join(out_dir, f\"lda_topics_by_subset_{ts}.csv\")\n",
        "topics_csv.to_csv(topics_csv_path, index=False)\n",
        "\n",
        "topicmix_csv = pd.DataFrame(topicmix_rows, columns=[\"Group\",\"Topic\",\"Mean_Weight\"])\n",
        "topicmix_csv_path = os.path.join(out_dir, f\"lda_topic_mix_by_subset_{ts}.csv\")\n",
        "topicmix_csv.to_csv(topicmix_csv_path, index=False)\n",
        "\n",
        "# ---------- 6) Optional robustness: 3-way χ² (drop NLP group) ----------\n",
        "try:\n",
        "    cont_3 = contingency_4way.drop(columns=[\"Joyce_LessRestrictive\"])\n",
        "    chi2_3, p_3, dof_3, _ = chi2_contingency(cont_3, correction=False)\n",
        "    V_3 = cramers_v(chi2_3, cont_3.values.sum(), *cont_3.shape)\n",
        "    print(f\"\\n3-way χ² (Manual/Restrictive/BNC): χ²={chi2_3:.2f} df={dof_3} p={p_3:.3g} V={V_3:.3f}\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------- 7) Master summary JSON ----------\n",
        "master = {\n",
        "    \"generated_at\": ts,\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "    \"note\": \"By-subset outputs (Manual / Restrictive / Less-Restrictive vs BNC) with unified Quasi_Similes label and FDR corrections.\",\n",
        "    \"chi_square_4way\": {\n",
        "        \"chi2\": float(chi2_4), \"dof\": int(dof_4), \"p_value\": float(p_4),\n",
        "        \"cramers_v\": float(V_4), \"monte_carlo\": bool(sim_used),\n",
        "        \"N\": int(N_total)\n",
        "    },\n",
        "    \"files\": {\n",
        "        \"chi2_contingency_by_subset_csv\": path_cont_4,\n",
        "        \"chi2_expected_by_subset_csv\": path_exp_4,\n",
        "        \"chi2_pearson_z_by_subset_csv\": path_resid_z,\n",
        "        \"chi2_cell_p_by_subset_csv\": path_resid_p,\n",
        "        \"chi2_cell_padj_BH_by_subset_csv\": path_resid_padj,\n",
        "        \"two_prop_newcombe_by_subset_csv\": path_two_prop,\n",
        "        \"binomial_tests_by_subset_csv\": path_binom,\n",
        "        \"continuous_tests_by_subset_csv\": path_cont,\n",
        "        \"lda_topics_by_subset_json\": topics_json_path,\n",
        "        \"lda_topics_by_subset_csv\": topics_csv_path,\n",
        "        \"lda_topic_mix_by_subset_csv\": topicmix_csv_path\n",
        "    }\n",
        "}\n",
        "master_path = os.path.join(out_dir, f\"stats_and_topics_summary_by_subset_{ts}.json\")\n",
        "with open(master_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(master, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nSAVED OUTPUTS (by-subset)\")\n",
        "print(\" - 4-way contingency:\", master[\"files\"][\"chi2_contingency_by_subset_csv\"])\n",
        "print(\" - 4-way expected:\", master[\"files\"][\"chi2_expected_by_subset_csv\"])\n",
        "print(\" - 4-way Pearson z:\", master[\"files\"][\"chi2_pearson_z_by_subset_csv\"])\n",
        "print(\" - 4-way cell p-values:\", master[\"files\"][\"chi2_cell_p_by_subset_csv\"])\n",
        "print(\" - 4-way cell p-values (BH):\", master[\"files\"][\"chi2_cell_padj_BH_by_subset_csv\"])\n",
        "print(\" - Two-proportion (subset vs BNC):\", master[\"files\"][\"two_prop_newcombe_by_subset_csv\"])\n",
        "print(\" - Binomial (subset vs BNC):\", master[\"files\"][\"binomial_tests_by_subset_csv\"])\n",
        "print(\" - Continuous tests (subset vs BNC):\", master[\"files\"][\"continuous_tests_by_subset_csv\"])\n",
        "print(\" - Topics JSON (per subset):\", master[\"files\"][\"lda_topics_by_subset_json\"])\n",
        "print(\" - Topics CSV (per subset):\", master[\"files\"][\"lda_topics_by_subset_csv\"])\n",
        "print(\" - Topic mix CSV (per subset):\", master[\"files\"][\"lda_topic_mix_by_subset_csv\"])\n",
        "print(\" - Master summary JSON:\", master_path)\n",
        "print(\"\\nDONE.\")\n"
      ],
      "metadata": {
        "id": "bFsJ5blk0D6b",
        "outputId": "36314386-0410-4617-96d4-6be0aed075b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROBUST STATISTICAL ANALYSIS (Joyce subsets vs BNC)\n",
            "===========================================================================\n",
            "\n",
            "Category distribution after harmonisation (counts):\n",
            "Original_Dataset     BNC_Baseline  Manual_CloseReading  \\\n",
            "Category_Framework                                       \n",
            "Standard                      118                   93   \n",
            "Quasi_Similes                  82                   53   \n",
            "Joycean_Quasi_Fuzzy             0                   13   \n",
            "Joycean_Framed                  0                   18   \n",
            "Joycean_Silent                  0                    6   \n",
            "Uncategorized                   0                    1   \n",
            "\n",
            "Original_Dataset     NLP_LessRestrictive_PG  Restrictive_Dubliners  Total  \n",
            "Category_Framework                                                         \n",
            "Standard                                330                    150    691  \n",
            "Quasi_Similes                             0                     47    182  \n",
            "Joycean_Quasi_Fuzzy                       0                     14     27  \n",
            "Joycean_Framed                            0                      4     22  \n",
            "Joycean_Silent                            0                      3      9  \n",
            "Uncategorized                             0                      0      1  \n",
            "Joyce_Manual          : 184 rows\n",
            "Joyce_Restrictive     : 218 rows\n",
            "Joyce_LessRestrictive : 330 rows\n",
            "BNC                   : 200 rows\n",
            "\n",
            "4-way Chi-square on Category_Framework (Joyce subsets vs BNC):\n",
            "χ² = 281.8806 | df = 15 | p = 0.000000 | Cramér’s V = 0.318 | Monte-Carlo=False\n",
            "\n",
            "Top 10 standardized residuals (|z|):\n",
            "  Joyce_LessRestrictive  | Quasi_Similes         z=  8.03  obs=0 exp=64.4\n",
            "  BNC                    | Quasi_Similes         z=  6.87  obs=82 exp=39.1\n",
            "  Joyce_Manual           | Joycean_Framed        z=  6.55  obs=18 exp=4.3\n",
            "  Joyce_LessRestrictive  | Standard              z=  5.46  obs=330 exp=244.7\n",
            "  Joyce_Manual           | Standard              z=  3.72  obs=93 exp=136.4\n",
            "  Joyce_Manual           | Joycean_Quasi_Fuzzy   z=  3.32  obs=13 exp=5.3\n",
            "  Joyce_Manual           | Joycean_Silent        z=  3.17  obs=6 exp=1.8\n",
            "  Joyce_LessRestrictive  | Joycean_Quasi_Fuzzy   z=  3.09  obs=0 exp=9.6\n",
            "  Joyce_Restrictive      | Joycean_Quasi_Fuzzy   z=  3.06  obs=14 exp=6.3\n",
            "  Joyce_Manual           | Quasi_Similes         z=  2.85  obs=53 exp=35.9\n",
            "\n",
            "Two-proportion tests (Newcombe–Wilson) for each Joyce subset vs BNC:\n",
            "  Joyce_Manual           | Joycean_Framed       z= 4.531 p=5.87825e-06 CI[0.058,0.149] h=0.636\n",
            "  Joyce_Manual           | Joycean_Quasi_Fuzzy  z= 3.824 p=0.000131123 CI[0.036,0.117] h=0.538\n",
            "  Joyce_Manual           | Joycean_Silent       z= 2.574 p=0.0100543 CI[0.007,0.069] h=0.363\n",
            "  Joyce_Manual           | Quasi_Similes        z=-2.501 p=0.0124016 CI[-0.214,-0.026] h=-0.257\n",
            "  Joyce_Manual           | Standard             z=-1.664 p=0.0961402 CI[-0.182,0.015] h=-0.170\n",
            "  Joyce_Manual           | Uncategorized        z= 1.044 p=0.296517 CI[-0.014,0.030] h=0.148\n",
            "  Joyce_Restrictive      | Joycean_Framed       z= 1.925 p=0.0542438 CI[-0.004,0.046] h=0.272\n",
            "  Joyce_Restrictive      | Joycean_Quasi_Fuzzy  z= 3.645 p=0.00026695 CI[0.032,0.105] h=0.512\n",
            "  Joyce_Restrictive      | Joycean_Silent       z= 1.665 p=0.0959149 CI[-0.007,0.040] h=0.235\n",
            "  Joyce_Restrictive      | Quasi_Similes        z=-4.298 p=1.72149e-05 CI[-0.279,-0.106] h=-0.424\n",
            "  Joyce_Restrictive      | Standard             z= 2.088 p=0.0367809 CI[0.006,0.188] h=0.205\n",
            "  Joyce_Restrictive      | Uncategorized        (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Joycean_Framed       (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Joycean_Quasi_Fuzzy  (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Joycean_Silent       (both groups at boundary → skip) h=0.000\n",
            "  Joyce_LessRestrictive  | Quasi_Similes        z=-12.652 p=1.09523e-36 CI[-0.479,-0.343] h=-1.390\n",
            "  Joyce_LessRestrictive  | Standard             z=12.652 p=1.09523e-36 CI[0.343,0.479] h=1.390\n",
            "  Joyce_LessRestrictive  | Uncategorized        (both groups at boundary → skip) h=0.000\n",
            "\n",
            "Binomial tests (each Joyce subset vs BNC category proportion):\n",
            "  Joyce_Manual           | Quasi_Similes        53/184 vs p_ref=0.4100 p=0.00070983\n",
            "  Joyce_Manual           | Standard             93/184 vs p_ref=0.5900 p=0.0242733\n",
            "  Joyce_Restrictive      | Quasi_Similes        47/218 vs p_ref=0.4100 p=1.9121e-09\n",
            "  Joyce_Restrictive      | Standard             150/218 vs p_ref=0.5900 p=0.00303334\n",
            "  Joyce_LessRestrictive  | Quasi_Similes        0/330 vs p_ref=0.4100 p=3.46102e-76\n",
            "  Joyce_LessRestrictive  | Standard             330/330 vs p_ref=0.5900 p=3.46102e-76\n",
            "\n",
            "Continuous features (Welch t + Mann–Whitney U) each Joyce subset vs BNC:\n",
            "  Sentence_Length        | Joyce_Manual           t=  2.804 p=0.00541318 | U=  20123.5 p=0.112632 | g=0.293 δ=0.094\n",
            "  Sentence_Length        | Joyce_Restrictive      t=  1.806 p=0.0716264 | U=  23076.0 p=0.301001 | g=0.175 δ=0.059\n",
            "  Sentence_Length        | Joyce_LessRestrictive  t=  1.731 p=0.0841489 | U=  34228.5 p=0.472192 | g=0.145 δ=0.037\n",
            "  Pre_Post_Ratio         | Joyce_Manual           t=  1.309 p=0.191305 | U=  20310.0 p=0.0786362 | g=0.133 δ=0.104\n",
            "  Pre_Post_Ratio         | Joyce_Restrictive      t=  0.251 p=0.802209 | U=  21925.0 p=0.919566 | g=0.025 δ=0.006\n",
            "  Pre_Post_Ratio         | Joyce_LessRestrictive  t=  0.540 p=0.589421 | U=  33247.5 p=0.657815 | g=0.049 δ=0.023\n",
            "  Sentiment_Polarity     | Joyce_Manual           t= -0.825 p=0.40996 | U=  17385.5 p=0.343227 | g=-0.084 δ=-0.055\n",
            "  Sentiment_Polarity     | Joyce_Restrictive      t= -0.634 p=0.526346 | U=  20806.5 p=0.414288 | g=-0.062 δ=-0.046\n",
            "  Sentiment_Polarity     | Joyce_LessRestrictive  t=  0.510 p=0.610387 | U=  33618.5 p=0.714242 | g=0.046 δ=0.019\n",
            "  Sentiment_Subjectivity | Joyce_Manual           t= -0.308 p=0.757975 | U=  18113.5 p=0.790903 | g=-0.031 δ=-0.016\n",
            "  Sentiment_Subjectivity | Joyce_Restrictive      t= -0.374 p=0.708807 | U=  21360.5 p=0.719938 | g=-0.036 δ=-0.020\n",
            "  Sentiment_Subjectivity | Joyce_LessRestrictive  t= -0.664 p=0.507057 | U=  31990.0 p=0.551874 | g=-0.059 δ=-0.031\n",
            "\n",
            "TOPIC MODELLING (per subset + BNC)\n",
            "  Topics generated for Joyce_Manual: 5\n",
            "  Topics generated for Joyce_Restrictive: 5\n",
            "  Topics generated for Joyce_LessRestrictive: 5\n",
            "  Topics generated for BNC: 5\n",
            "\n",
            "3-way χ² (Manual/Restrictive/BNC): χ²=69.65 df=10 p=5.18e-11 V=0.241\n",
            "\n",
            "SAVED OUTPUTS (by-subset)\n",
            " - 4-way contingency: analysis_outputs/chi2_contingency_by_subset_20250824_204131.csv\n",
            " - 4-way expected: analysis_outputs/chi2_expected_by_subset_20250824_204131.csv\n",
            " - 4-way Pearson z: analysis_outputs/chi2_pearson_z_by_subset_20250824_204131.csv\n",
            " - 4-way cell p-values: analysis_outputs/chi2_cell_p_by_subset_20250824_204131.csv\n",
            " - 4-way cell p-values (BH): analysis_outputs/chi2_cell_padj_BH_by_subset_20250824_204131.csv\n",
            " - Two-proportion (subset vs BNC): analysis_outputs/two_prop_newcombe_by_subset_20250824_204131.csv\n",
            " - Binomial (subset vs BNC): analysis_outputs/binomial_tests_by_subset_20250824_204131.csv\n",
            " - Continuous tests (subset vs BNC): analysis_outputs/continuous_tests_by_subset_20250824_204131.csv\n",
            " - Topics JSON (per subset): analysis_outputs/lda_topics_by_subset_20250824_204131.json\n",
            " - Topics CSV (per subset): analysis_outputs/lda_topics_by_subset_20250824_204131.csv\n",
            " - Topic mix CSV (per subset): analysis_outputs/lda_topic_mix_by_subset_20250824_204131.csv\n",
            " - Master summary JSON: analysis_outputs/stats_and_topics_summary_by_subset_20250824_204131.json\n",
            "\n",
            "DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Academic Reporting and Documentation\n",
        "\n",
        "# 8.1 Professional Report Generation\n",
        "The HTML report generator creates comprehensive academic documentation suitable for:\n",
        "\n",
        "Peer review and publication supplementary materials\n",
        "Research documentation and reproducibility\n",
        "Academic presentation and dissemination\n",
        "\n",
        "# 8.2 Results Integration\n",
        "The report synthesizes all analytical components including performance metrics, statistical significance testing, topic modeling results, and comprehensive dataset summaries.\n",
        "\n",
        "# 8.3 Academic Standards\n",
        "The output maintains academic formatting standards with proper typography, professional styling, and structured organization suitable for scholarly communication."
      ],
      "metadata": {
        "id": "RwoglydbeaCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ACADEMIC HTML REPORT GENERATOR — UPDATED (data-driven, harmonised labels)\n",
        "# Generates a comprehensive, reproducible HTML report from Cells 1–2 outputs\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "print(\"GENERATING ACADEMIC HTML REPORT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Timestamp for report\n",
        "now = datetime.now()\n",
        "report_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "report_date = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def latest_file(pattern, base=\"analysis_outputs\"):\n",
        "    files = glob.glob(os.path.join(base, pattern))\n",
        "    return max(files, key=os.path.getctime) if files else None\n",
        "\n",
        "def safe_read_csv(path):\n",
        "    if not path or not os.path.exists(path):\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load CSV {path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def safe_read_json(path):\n",
        "    if not path or not os.path.exists(path):\n",
        "        return {}\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load JSON {path}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def df_for_html(df, index=False, max_rows=None):\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "    d = df.copy()\n",
        "    if max_rows is not None and len(d) > max_rows:\n",
        "        d = d.head(max_rows)\n",
        "        d.__truncated__ = True\n",
        "    return d\n",
        "\n",
        "def create_table_html(df, title=\"\", max_rows=20, index=False):\n",
        "    \"\"\"Create HTML table with styling; auto-handle empty and truncation note.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return f\"<p><em>No data available for {title}</em></p>\"\n",
        "    d = df.copy()\n",
        "    trunc = False\n",
        "    if max_rows and len(d) > max_rows:\n",
        "        d = d.head(max_rows)\n",
        "        trunc = True\n",
        "    # If the index is meaningful (named or not default RangeIndex), show it as a column\n",
        "    if index:\n",
        "        d = d.reset_index()\n",
        "    table_html = d.to_html(classes='analysis-table', escape=False, index=False)\n",
        "    note = f\"<p class='truncated-note'><em>Showing first {max_rows} of {len(df)} rows</em></p>\" if trunc else \"\"\n",
        "    return f\"\"\"\n",
        "    <div class=\"table-container\">\n",
        "        <h4>{title}</h4>\n",
        "        <div class=\"table-wrapper\">{table_html}</div>\n",
        "        {note}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# ---------- Pull in analysis outputs from Cell 2 ----------\n",
        "out_dir = \"analysis_outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "paths = {\n",
        "    \"chi2_cont\": latest_file(\"chi2_contingency_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_exp\": latest_file(\"chi2_expected_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_z\": latest_file(\"chi2_pearson_z_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_p\": latest_file(\"chi2_cell_p_by_subset_*.csv\", out_dir),\n",
        "    \"chi2_padj\": latest_file(\"chi2_cell_padj_BH_by_subset_*.csv\", out_dir),\n",
        "    \"two_prop\": latest_file(\"two_prop_newcombe_by_subset_*.csv\", out_dir),\n",
        "    \"binom\": latest_file(\"binomial_tests_by_subset_*.csv\", out_dir),\n",
        "    \"cont\": latest_file(\"continuous_tests_by_subset_*.csv\", out_dir),\n",
        "    \"topics\": latest_file(\"lda_topics_by_subset_*.csv\", out_dir),\n",
        "    \"topicmix\": latest_file(\"lda_topic_mix_by_subset_*.csv\", out_dir),\n",
        "    \"hl\": latest_file(\"continuous_HL_shifts_*.csv\", out_dir),\n",
        "    \"master\": latest_file(\"stats_and_topics_summary_by_subset_*.json\", out_dir),\n",
        "}\n",
        "\n",
        "chi2_cont_df = safe_read_csv(paths[\"chi2_cont\"])\n",
        "chi2_exp_df  = safe_read_csv(paths[\"chi2_exp\"])\n",
        "chi2_z_df    = safe_read_csv(paths[\"chi2_z\"])\n",
        "chi2_p_df    = safe_read_csv(paths[\"chi2_p\"])\n",
        "chi2_padj_df = safe_read_csv(paths[\"chi2_padj\"])\n",
        "two_prop_df  = safe_read_csv(paths[\"two_prop\"])\n",
        "binom_df     = safe_read_csv(paths[\"binom\"])\n",
        "cont_df      = safe_read_csv(paths[\"cont\"])\n",
        "topics_df    = safe_read_csv(paths[\"topics\"])\n",
        "topicmix_df  = safe_read_csv(paths[\"topicmix\"])\n",
        "hl_df        = safe_read_csv(paths[\"hl\"])\n",
        "master_json  = safe_read_json(paths[\"master\"])\n",
        "\n",
        "# ---------- Summaries from Cell 1 (results_df, f1_analysis, comparator.env_info) ----------\n",
        "def create_summary_stats_html():\n",
        "    if 'results_df' not in globals() or results_df is None or results_df.empty:\n",
        "        return \"<p><em>No results data available</em></p>\"\n",
        "    dcounts = results_df['Original_Dataset'].value_counts()\n",
        "    ccounts = results_df['Category_Framework'].value_counts()\n",
        "\n",
        "    items_d = \"\".join([f\"<li><strong>{k}:</strong> {int(v):,} instances</li>\" for k,v in dcounts.items()])\n",
        "    items_c = \"\".join([f\"<li><strong>{k}:</strong> {int(v):,} ({(v/len(results_df))*100:.1f}%)</li>\" for k,v in ccounts.items()])\n",
        "\n",
        "    return f\"\"\"\n",
        "    <div class=\"summary-stats\">\n",
        "        <div class=\"stat-group\">\n",
        "            <h4>Dataset Distribution</h4>\n",
        "            <ul>{items_d}</ul>\n",
        "            <p><strong>Total Instances:</strong> {len(results_df):,}</p>\n",
        "        </div>\n",
        "        <div class=\"stat-group\">\n",
        "            <h4>Category Distribution</h4>\n",
        "            <ul>{items_c}</ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def f1_summary_html():\n",
        "    # Try multiple sources\n",
        "    micro_rb = macro_rb = micro_nlp = macro_nlp = None\n",
        "    pairs_rb = pairs_nlp = None\n",
        "    if 'f1_analysis' in globals() and isinstance(f1_analysis, dict):\n",
        "        rb = f1_analysis.get('rule_based_vs_manual') or f1_analysis.get('Rule-Based vs Manual')\n",
        "        nl = f1_analysis.get('nlp_vs_manual') or f1_analysis.get('NLP vs Manual')\n",
        "        if rb and rb.get(\"overall\"):\n",
        "            micro_rb = rb[\"overall\"].get(\"micro_f1\")\n",
        "            macro_rb = rb[\"overall\"].get(\"macro_f1\")\n",
        "            pairs_rb = rb.get(\"pairs\")\n",
        "        if nl and nl.get(\"overall\"):\n",
        "            micro_nlp = nl[\"overall\"].get(\"micro_f1\")\n",
        "            macro_nlp = nl[\"overall\"].get(\"macro_f1\")\n",
        "            pairs_nlp = nl.get(\"pairs\")\n",
        "    elif 'comparator' in globals():\n",
        "        try:\n",
        "            fa = comparator.comparison_results.get('f1_analysis', {})\n",
        "            rb = fa.get('rule_based_vs_manual')\n",
        "            nl = fa.get('nlp_vs_manual')\n",
        "            if rb and rb.get(\"overall\"):\n",
        "                micro_rb = rb[\"overall\"].get(\"micro_f1\")\n",
        "                macro_rb = rb[\"overall\"].get(\"macro_f1\")\n",
        "                pairs_rb = rb.get(\"pairs\")\n",
        "            if nl and nl.get(\"overall\"):\n",
        "                micro_nlp = nl[\"overall\"].get(\"micro_f1\")\n",
        "                macro_nlp = nl[\"overall\"].get(\"macro_f1\")\n",
        "                pairs_nlp = nl.get(\"pairs\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if micro_rb is None and micro_nlp is None:\n",
        "        return \"<p><em>F1 metrics unavailable (run Cell 1 in this session).</em></p>\"\n",
        "\n",
        "    lines = []\n",
        "    if micro_rb is not None:\n",
        "        lines.append(f\"• Rule-Based vs Manual: micro-F1 = {micro_rb:.3f}, macro-F1 = {macro_rb:.3f}{' (pairs=' + str(pairs_rb) + ')' if pairs_rb else ''}\")\n",
        "    if micro_nlp is not None:\n",
        "        lines.append(f\"• NLP vs Manual: micro-F1 = {micro_nlp:.3f}, macro-F1 = {macro_nlp:.3f}{' (pairs=' + str(pairs_nlp) + ')' if pairs_nlp else ''}\")\n",
        "\n",
        "    return f\"\"\"\n",
        "    <div class=\"highlight\">\n",
        "      <strong>F1 Summary:</strong><br>\n",
        "      {'<br>'.join(lines)}\n",
        "      <br>• Total instances processed: {len(results_df):,} (across all datasets)\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def env_html():\n",
        "    info = {}\n",
        "    if 'comparator' in globals():\n",
        "        try:\n",
        "            info = comparator.env_info\n",
        "        except Exception:\n",
        "            info = {}\n",
        "    if not info:\n",
        "        return \"\"\n",
        "    items = \"\".join([f\"<li><strong>{k}:</strong> {v}</li>\" for k,v in info.items()])\n",
        "    return f\"\"\"\n",
        "    <div class=\"methodology\">\n",
        "      <strong>Environment:</strong>\n",
        "      <ul>{items}</ul>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# ---------- Build analytic highlight snippets ----------\n",
        "def chi2_summary_html():\n",
        "    if not master_json:\n",
        "        return \"\"\n",
        "    chi = master_json.get(\"chi_square_4way\", {})\n",
        "    chi_line = (f\"χ² = {chi.get('chi2', float('nan')):.3f} | df = {chi.get('dof', 0)} | \"\n",
        "                f\"p = {chi.get('p_value', float('nan')):.3g} | Cramér’s V = {chi.get('cramers_v', float('nan')):.3f} \"\n",
        "                f\"| Monte-Carlo={chi.get('monte_carlo', False)} | N={chi.get('N', 0)}\")\n",
        "    return f\"<p>{chi_line}</p>\"\n",
        "\n",
        "def top_residuals_html(k=10):\n",
        "    if chi2_z_df.empty or chi2_cont_df.empty or chi2_exp_df.empty:\n",
        "        return \"\"\n",
        "    # Melt z\n",
        "    z_long = chi2_z_df.copy()\n",
        "    z_long = z_long.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in z_long.columns else z_long\n",
        "    z_long = z_long.melt(id_vars=[c for c in [\"Category_Framework\"] if c in z_long.columns],\n",
        "                         var_name=\"Group\", value_name=\"z\")\n",
        "    z_long[\"abs_z\"] = z_long[\"z\"].abs()\n",
        "    # Get obs/exp\n",
        "    cont = chi2_cont_df.copy()\n",
        "    cont = cont.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in cont.columns else cont\n",
        "    exp = chi2_exp_df.copy()\n",
        "    exp = exp.rename(columns={\"Unnamed: 0\":\"Category_Framework\"}) if \"Unnamed: 0\" in exp.columns else exp\n",
        "    # Align to long\n",
        "    obs_long = cont.melt(id_vars=[c for c in [\"Category_Framework\"] if c in cont.columns],\n",
        "                         var_name=\"Group\", value_name=\"obs\")\n",
        "    exp_long = exp.melt(id_vars=[c for c in [\"Category_Framework\"] if c in exp.columns],\n",
        "                        var_name=\"Group\", value_name=\"exp\")\n",
        "    m = (z_long.merge(obs_long, on=[\"Category_Framework\",\"Group\"], how=\"left\")\n",
        "               .merge(exp_long, on=[\"Category_Framework\",\"Group\"], how=\"left\"))\n",
        "    m = m.sort_values(\"abs_z\", ascending=False).head(k)\n",
        "    return create_table_html(m, f\"Top {k} standardized residuals (|z|)\", max_rows=k)\n",
        "\n",
        "def sig_two_prop_html(alpha=0.05, top=15):\n",
        "    if two_prop_df.empty or \"p_adj_BH\" not in two_prop_df.columns:\n",
        "        return \"\"\n",
        "    df = two_prop_df.copy()\n",
        "    # Keep only significant rows, order by adjusted p then effect size magnitude\n",
        "    df[\"|h|\"] = df.get(\"cohens_h\", 0).abs()\n",
        "    sig = df[df[\"p_adj_BH\"] < alpha].sort_values([\"p_adj_BH\",\"|h|\"])\n",
        "    cols = [c for c in [\"Subset\",\"Category\",\"prop_A\",\"prop_B\",\"z\",\"p_value\",\"p_adj_BH\",\"cohens_h\"] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Two-proportion (BH<{alpha}) — top {top}\", max_rows=top)\n",
        "\n",
        "def sig_binom_html(alpha=0.05, top=15):\n",
        "    if binom_df.empty or \"p_adj_BH\" not in binom_df.columns:\n",
        "        return \"\"\n",
        "    df = binom_df.copy()\n",
        "    sig = df[df[\"p_adj_BH\"] < alpha].sort_values(\"p_adj_BH\")\n",
        "    cols = [c for c in [\"Subset\",\"Category\",\"count_A\",\"n_A\",\"p_ref_BNC\",\"p_value\",\"p_adj_BH\"] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Binomial vs BNC (BH<{alpha}) — top {top}\", max_rows=top)\n",
        "\n",
        "def sig_continuous_html(alpha=0.05):\n",
        "    if cont_df.empty:\n",
        "        return \"\"\n",
        "    d = cont_df.copy()\n",
        "    # Prefer Mann–Whitney U adjusted p; fall back to Welch t\n",
        "    if \"U_padj_BH\" in d.columns:\n",
        "        sig = d[d[\"U_padj_BH\"] < alpha].sort_values(\"U_padj_BH\")\n",
        "        label = \"Mann–Whitney U (BH)\"\n",
        "        padj_col = \"U_padj_BH\"\n",
        "        p_col = \"U_pvalue\"\n",
        "    elif \"t_padj_BH\" in d.columns:\n",
        "        sig = d[d[\"t_padj_BH\"] < alpha].sort_values(\"t_padj_BH\")\n",
        "        label = \"Welch t (BH)\"\n",
        "        padj_col = \"t_padj_BH\"\n",
        "        p_col = \"t_pvalue\"\n",
        "    else:\n",
        "        return \"\"\n",
        "    if sig.empty:\n",
        "        return \"<p><em>No continuous feature differences were significant after FDR correction.</em></p>\"\n",
        "    cols = [c for c in [\"Feature\",\"Subset\",\"A_n\",\"B_n\",\"A_mean\",\"B_mean\",\"A_median\",\"B_median\",\n",
        "                        \"t_stat\",\"t_pvalue\",\"U_stat\",\"U_pvalue\",\"hedges_g\",\"cliffs_delta\", padj_col] if c in sig.columns]\n",
        "    return create_table_html(sig[cols], f\"Continuous features — significant by {label}\", max_rows=15)\n",
        "\n",
        "# ---------- Build the HTML ----------\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/>\n",
        "<title>Joyce Simile Research: Comprehensive Linguistic Analysis Report</title>\n",
        "<style>\n",
        "    body {{\n",
        "        font-family: 'Times New Roman', serif;\n",
        "        line-height: 1.6;\n",
        "        margin: 0;\n",
        "        padding: 20px;\n",
        "        background-color: #f9f9f9;\n",
        "        color: #333;\n",
        "    }}\n",
        "    .container {{\n",
        "        max-width: 1200px;\n",
        "        margin: 0 auto;\n",
        "        background: white;\n",
        "        padding: 30px;\n",
        "        border-radius: 8px;\n",
        "        box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
        "    }}\n",
        "    .header {{\n",
        "        text-align: center;\n",
        "        border-bottom: 3px solid #2c3e50;\n",
        "        padding-bottom: 20px;\n",
        "        margin-bottom: 30px;\n",
        "    }}\n",
        "    .header h1 {{ color: #2c3e50; margin: 0; font-size: 2.2em; font-weight: bold; }}\n",
        "    .header .subtitle {{ color: #7f8c8d; font-size: 1.1em; margin: 10px 0 5px 0; font-style: italic; }}\n",
        "    .header .timestamp {{ color: #95a5a6; font-size: 0.9em; }}\n",
        "    .section {{\n",
        "        margin: 30px 0;\n",
        "        padding: 20px;\n",
        "        border-left: 4px solid #3498db;\n",
        "        background-color: #f8f9fa;\n",
        "    }}\n",
        "    .section h2 {{\n",
        "        color: #2c3e50;\n",
        "        margin-top: 0; border-bottom: 2px solid #ecf0f1; padding-bottom: 10px;\n",
        "    }}\n",
        "    .section h3 {{ color: #34495e; margin-top: 25px; }}\n",
        "    .section h4 {{ color: #5d6d7e; margin-top: 20px; margin-bottom: 10px; }}\n",
        "    .analysis-table {{ width: 100%; border-collapse: collapse; margin: 15px 0; font-size: 0.9em; }}\n",
        "    .analysis-table th {{ background:#34495e; color:#fff; padding:12px 8px; text-align:left; font-weight:bold; }}\n",
        "    .analysis-table td {{ padding:10px 8px; border-bottom:1px solid #ddd; }}\n",
        "    .analysis-table tr:nth-child(even) {{ background-color:#f2f2f2; }}\n",
        "    .analysis-table tr:hover {{ background-color:#e8f4fd; }}\n",
        "    .summary-stats {{ display:grid; grid-template-columns:1fr 1fr; gap:30px; margin:20px 0; }}\n",
        "    .stat-group {{ background:#fff; padding:20px; border-radius:6px; border:1px solid #e1e8ed; }}\n",
        "    .stat-group h4 {{ margin-top:0; color:#2c3e50; border-bottom:1px solid #ecf0f1; padding-bottom:8px; }}\n",
        "    .stat-group ul {{ list-style-type:none; padding:0; }}\n",
        "    .stat-group li {{ padding:5px 0; border-bottom:1px solid #f8f9fa; }}\n",
        "    .highlight {{ background:#d1ecf1; padding:15px; border-left:4px solid #17a2b8; margin:15px 0; }}\n",
        "    .methodology {{ background:#f8f9fa; padding:15px; border-radius:5px; margin:15px 0; font-style:italic; }}\n",
        "    .table-container {{ margin:20px 0; }}\n",
        "    .table-wrapper {{ overflow-x:auto; }}\n",
        "    .truncated-note {{ color:#6c757d; font-size:0.9em; margin-top:5px; }}\n",
        "    .footer {{ text-align:center; margin-top:40px; padding-top:20px; border-top:2px solid #ecf0f1; color:#7f8c8d; font-size:0.9em; }}\n",
        "    @media (max-width: 768px) {{\n",
        "        .summary-stats {{ grid-template-columns:1fr; }}\n",
        "        .container {{ padding:15px; }}\n",
        "        .analysis-table {{ font-size:0.85em; }}\n",
        "    }}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container\">\n",
        "    <div class=\"header\">\n",
        "        <h1>Joyce Simile Research</h1>\n",
        "        <div class=\"subtitle\">Comprehensive Linguistic Analysis Report</div>\n",
        "        <div class=\"subtitle\">Manual Annotations vs Extraction Methods vs BNC Baseline</div>\n",
        "        <div class=\"timestamp\">Generated on {report_timestamp}</div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Executive Summary</h2>\n",
        "        <p>This report compiles the results of a comparative analysis of simile usage in <em>Dubliners</em>, spanning manual expert annotations, a restrictive rule-based extractor, a less-restrictive NLP extractor, and a BNC baseline. Categories were harmonised with <strong>Quasi_Similes</strong> as the unified label for the Joyce/BNC quasi-simile phenomenon.</p>\n",
        "        {chi2_summary_html()}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Dataset Overview</h2>\n",
        "        <p>Four datasets were analysed, representing complementary identification approaches and a baseline:</p>\n",
        "        {create_summary_stats_html()}\n",
        "        {env_html()}\n",
        "        <div class=\"methodology\">\n",
        "            <strong>Methodology:</strong> Features include token counts, pre/post-comparator ratio, POS distribution, syntactic complexity, TextBlob sentiment (exploratory), and comparative structure flags. Instance-aligned F1 (exact+fuzzy sentence matching) was used to evaluate extractors against manual ground truth.\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Performance Metrics</h2>\n",
        "        <h3>Instance-Aligned F1 Scores</h3>\n",
        "        {f1_summary_html()}\n",
        "    </div>\n",
        "\n",
        "    <div class=\"section\">\n",
        "        <h2>Statistical Results</h2>\n",
        "        <h3>4-way Category Distribution</h3>\n",
        "\"\"\"\n",
        "\n",
        "# 4-way chi-square tables\n",
        "if not chi2_cont_df.empty:\n",
        "    html_content += create_table_html(chi2_cont_df, \"Observed counts by group × category\", max_rows=10, index=True)\n",
        "if not chi2_exp_df.empty:\n",
        "    html_content += create_table_html(chi2_exp_df, \"Expected counts under independence\", max_rows=10, index=True)\n",
        "if not chi2_z_df.empty:\n",
        "    html_content += create_table_html(chi2_z_df, \"Pearson standardized residuals (z)\", max_rows=10, index=True)\n",
        "    html_content += top_residuals_html(k=10)\n",
        "if not chi2_padj_df.empty:\n",
        "    html_content += create_table_html(chi2_padj_df, \"Per-cell p-values (BH-adjusted)\", max_rows=10, index=True)\n",
        "\n",
        "# Two-proportion\n",
        "if not two_prop_df.empty:\n",
        "    html_content += \"\"\"\n",
        "        <h3>Two-Proportion Tests (Joyce subsets vs BNC)</h3>\n",
        "        <p>Newcombe confidence intervals and Cohen’s h. BH correction applied across tests.</p>\n",
        "    \"\"\"\n",
        "    # Ensure adj col exists (defensive)\n",
        "    if \"p_adj_BH\" not in two_prop_df.columns and \"p_value\" in two_prop_df.columns:\n",
        "        pv = two_prop_df[\"p_value\"].fillna(1.0).to_numpy()\n",
        "        # simple BH\n",
        "        order = np.argsort(pv)\n",
        "        ranked = np.empty_like(pv, dtype=float)\n",
        "        cummin = 1.0\n",
        "        n = pv.size\n",
        "        for i, idx in enumerate(order[::-1], start=1):\n",
        "            rank = n - i + 1\n",
        "            val = pv[idx] * n / rank\n",
        "            cummin = min(cummin, val)\n",
        "            ranked[idx] = min(cummin, 1.0)\n",
        "        two_prop_df[\"p_adj_BH\"] = ranked\n",
        "    html_content += create_table_html(two_prop_df, \"All two-proportion results\", max_rows=15)\n",
        "    html_content += sig_two_prop_html(alpha=0.05, top=15)\n",
        "\n",
        "# Binomial\n",
        "if not binom_df.empty:\n",
        "    if \"p_adj_BH\" not in binom_df.columns and \"p_value\" in binom_df.columns:\n",
        "        pv = binom_df[\"p_value\"].fillna(1.0).to_numpy()\n",
        "        order = np.argsort(pv)\n",
        "        ranked = np.empty_like(pv, dtype=float)\n",
        "        cummin = 1.0\n",
        "        n = pv.size\n",
        "        for i, idx in enumerate(order[::-1], start=1):\n",
        "            rank = n - i + 1\n",
        "            val = pv[idx] * n / rank\n",
        "            cummin = min(cummin, val)\n",
        "            ranked[idx] = min(cummin, 1.0)\n",
        "        binom_df[\"p_adj_BH\"] = ranked\n",
        "    html_content += \"\"\"\n",
        "        <h3>Binomial Tests (Joyce subsets vs BNC reference proportion)</h3>\n",
        "        <p>One-sample tests per category per subset; BH correction across tests.</p>\n",
        "    \"\"\"\n",
        "    html_content += create_table_html(binom_df, \"All binomial results\", max_rows=12)\n",
        "    html_content += sig_binom_html(alpha=0.05, top=12)\n",
        "\n",
        "# Continuous\n",
        "if not cont_df.empty:\n",
        "    html_content += \"\"\"\n",
        "        <h3>Continuous Features</h3>\n",
        "        <p>Welch t and Mann–Whitney U with effect sizes (Hedges’ g, Cliff’s δ). BH correction applied.</p>\n",
        "    \"\"\"\n",
        "    html_content += create_table_html(cont_df, \"Continuous feature comparisons\", max_rows=12)\n",
        "    html_content += sig_continuous_html(alpha=0.05)\n",
        "if not hl_df.empty:\n",
        "    html_content += create_table_html(hl_df, \"Hodges–Lehmann location shifts (A − B) with bootstrap CIs\", max_rows=12)\n",
        "\n",
        "# Topics\n",
        "html_content += \"\"\"\n",
        "    </div>\n",
        "    <div class=\"section\">\n",
        "        <h2>Topic Modelling</h2>\n",
        "        <p>Per-subset LDA topics (CountVectorizer; 5 topics × 10 words). Topic mixture weights summarised by mean.</p>\n",
        "\"\"\"\n",
        "if not topics_df.empty:\n",
        "    html_content += create_table_html(topics_df, \"Top words per topic × group\", max_rows=20)\n",
        "if not topicmix_df.empty:\n",
        "    html_content += create_table_html(topicmix_df, \"Mean topic weights per group\", max_rows=20)\n",
        "\n",
        "# Sample of comprehensive results\n",
        "if 'results_df' in globals() and isinstance(results_df, pd.DataFrame) and not results_df.empty:\n",
        "    sample_cols = [c for c in ['Instance_ID','Original_Dataset','Category_Framework','Comparator_Type',\n",
        "                               'Sentence_Length','Sentiment_Polarity','Pre_Post_Ratio'] if c in results_df.columns]\n",
        "    sample_results = results_df.head(25)[sample_cols].round(3)\n",
        "    html_content += f\"\"\"\n",
        "    </div>\n",
        "    <div class=\"section\">\n",
        "        <h2>Comprehensive Results — Sample</h2>\n",
        "        <p>Representative snippet of the full dataset (first 25 rows):</p>\n",
        "        {create_table_html(sample_results, \"Sample of comprehensive analysis\", max_rows=25)}\n",
        "        <div class=\"methodology\">\n",
        "            The full dataset includes lemmatisation, POS distributions, syntactic complexity, and comparative structure features.\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# Files list\n",
        "files_list = []\n",
        "files_list.append(\"comprehensive_linguistic_analysis_corrected.csv\")\n",
        "for k, p in paths.items():\n",
        "    if p:\n",
        "        files_list.append(os.path.relpath(p))\n",
        "files_items = \"\".join([f\"<li><code>{f}</code></li>\" for f in sorted(set(files_list))])\n",
        "\n",
        "html_content += f\"\"\"\n",
        "    <div class=\"section\">\n",
        "        <h2>Files Generated</h2>\n",
        "        <ul>{files_items}</ul>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"footer\">\n",
        "        <p>Generated by Comprehensive Linguistic Analysis Pipeline</p>\n",
        "        <p>Joyce Simile Research Project • {report_timestamp}</p>\n",
        "        <p><em>Categories harmonised with unified <strong>Quasi_Similes</strong>; results computed using FDR corrections where applicable.</em></p>\n",
        "    </div>\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Save HTML report\n",
        "report_filename = f\"joyce_simile_analysis_report_{report_date}.html\"\n",
        "with open(report_filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(f\"✓ Academic HTML report generated: {report_filename}\")\n",
        "print(f\"✓ File size: {os.path.getsize(report_filename):,} bytes\")\n",
        "print(f\"✓ Report length: {len(html_content):,} characters\")\n",
        "print(\"\\nREPORT READY — open in a browser to view.\")\n"
      ],
      "metadata": {
        "id": "xMyOrRz-4kmz",
        "outputId": "64ae16fa-60eb-4516-828b-4788c5abf525",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATING ACADEMIC HTML REPORT\n",
            "==================================================\n",
            "✓ Academic HTML report generated: joyce_simile_analysis_report_20250824_210058.html\n",
            "✓ File size: 46,522 bytes\n",
            "✓ Report length: 46,491 characters\n",
            "\n",
            "REPORT READY — open in a browser to view.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Research Implications and Future Directions\n",
        "\n",
        "## 9.1 Computational Literary Analysis\n",
        "Using sentence-level **instance-aligned evaluation** (exact match, then fuzzy ≥ 0.92), the extractors partially recover the manual categories:\n",
        "- **Rule-Based vs Manual:** micro-F1 ≈ **0.343**, macro-F1 ≈ **0.178** (pairs matched: see Cell 1 output)\n",
        "- **NLP (Less-Restrictive) vs Manual:** micro-F1 ≈ **0.292**, macro-F1 ≈ **0.059**\n",
        "\n",
        "These scores indicate **meaningful but incomplete replication** of expert judgments—appropriate for unsupervised/heuristic extractors—and motivate targeted improvements (Section 9.4). Importantly, categorical distributions still differ strongly across corpora (**χ²≈281.88**, **Cramér’s V≈0.318**, *p*≪.001), so the **signal is primarily categorical**, not scalar.\n",
        "\n",
        "**Key distributional contrasts (harmonised labels):**\n",
        "- **BNC**: *Quasi_Similes* ≈ **41%** (82/200) vs **Joyce-Manual** ≈ **29%** (53/184).  \n",
        "- **Joyce-Manual** shows enrichments in **Joycean_Framed**, **Joycean_Quasi_Fuzzy**, and **Joycean_Silent**.\n",
        "- **NLP (Less-Restrictive)** is **all Standard** (330/330), by design—useful as a control but excluded in robustness checks where appropriate.\n",
        "\n",
        "## 9.2 Innovation Quantification\n",
        "Under the unified scheme:\n",
        "- **Joycean-specific categories** (Framed + Quasi_Fuzzy + Silent) account for **~20%** of **Joyce-Manual** (37/184).\n",
        "- **Quasi_Similes** appear **less** in **Joyce-Manual** (~29%) relative to **BNC** (~41%), with significant two-proportion differences (BH-corrected; moderate effects, e.g., |*h*|≈0.26–0.42 for Manual/Restrictive vs BNC).\n",
        "\n",
        "This suggests Joyce’s corpus **rebalances** the space of similes: fewer conventional quasi-similes than BNC and **more Joycean-specific forms**, evidencing stylistic innovation distinct from baseline English.\n",
        "\n",
        "## 9.3 Methodological Contributions\n",
        "- **Label harmonisation**: *Joycean_Quasi* merged into **Quasi_Similes** to align BNC and Joyce phenomena, avoiding double-counting and artificial χ² inflation.\n",
        "- **Robust inference**: 4-way χ² with per-cell residuals and **BH-FDR**; two-proportion tests with **Newcombe CIs** + **Cohen’s h**; binomial tests vs BNC reference; Welch-*t* & Mann–Whitney-*U* with **Hedges’ g** and **Cliff’s δ**; optional 3-way χ² excluding the degenerate NLP group.\n",
        "- **Instance-aligned evaluation**: sentence-pairing (exact+fuzzy) for extractor vs manual categories, a more faithful alternative to bag-of-counts F1.\n",
        "- **Reproducibility**: environment stamping; deterministic random seeds; saved CSV/JSON artifacts.\n",
        "\n",
        "## 9.4 Future Directions\n",
        "1. **Improve extractor recall** for Joycean categories  \n",
        "   - Add patterns for **punctuation-mediated** comparators (colon/semicolon/ellipsis) and **‘as … as’** spans.  \n",
        "   - Incorporate dependency features (governor-comparator-target) and discourse cues.\n",
        "\n",
        "2. **Supervised modelling**  \n",
        "   - Train a classifier on paired manual/extractor sentences (use current pairs as silver labels), with features from syntax, lemmata, and comparator spans.\n",
        "\n",
        "3. **Error analysis & ablations**  \n",
        "   - Per-category confusion and residuals to identify systematic misses; ablate comparator detection vs category mapping.\n",
        "\n",
        "4. **Topic–category linkage**  \n",
        "   - Relate LDA topic mixtures to categories (e.g., Framed vs Standard) and test with permutation or regression.\n",
        "\n",
        "5. **Generalisation**  \n",
        "   - Validate on other Joyce texts or contemporaries; check stability of the Joycean category enrichments.\n",
        "\n",
        "---\n",
        "\n",
        "## References and Data Sources\n",
        "\n",
        "**Primary Text**  \n",
        "- Joyce, James. *Dubliners*. Project Gutenberg.\n",
        "\n",
        "**Baseline Corpus**  \n",
        "- British National Corpus (BNC), used as a **standard English** reference. *Ensure appropriate licensing/access for the specific BNC source employed.*\n",
        "\n",
        "---\n",
        "\n",
        "## Computational Tools\n",
        "- **spaCy** (`en_core_web_sm`): tokenisation, POS, dependencies  \n",
        "- **scikit-learn**: CountVectorizer, LDA, utilities  \n",
        "- **SciPy**: χ², *t*-tests, Mann–Whitney U, binomial  \n",
        "- **statsmodels** (if available): two-proportion z-tests, Newcombe CIs  \n",
        "- **TextBlob**: exploratory sentiment  \n",
        "- **pandas / numpy**: data handling and numerics\n",
        "\n",
        "---\n",
        "\n",
        "## Research Framework (Summary)\n",
        "- **Evaluation**: instance-aligned F1 (micro/macro) via sentence pairing (exact + fuzzy ≥ 0.92).  \n",
        "- **Categorical inference**: 4-way χ² with per-cell residual z and BH-FDR; two-proportion tests (Newcombe CIs, Cohen’s h); binomial tests vs BNC proportions.  \n",
        "- **Continuous inference**: Welch-*t* and Mann–Whitney-*U* with **Hedges’ g**, **Cliff’s δ**, and **Hodges–Lehmann** shifts (bootstrap CIs).  \n",
        "- **Topics**: LDA per subset using **CountVectorizer** (5 topics × 10 words), with mean topic-mix summaries.  \n",
        "- **Harmonised taxonomy**: **Quasi_Similes** (unified), **Joycean_Framed**, **Joycean_Quasi_Fuzzy**, **Joycean_Silent**, **Standard**, **Uncategorized**.\n",
        "\n",
        "> **Terminology note:** *Quasi_Similes* is the unified tag covering both the BNC’s “Quasi_Similes” and Joyce’s former “Joycean_Quasi,” representing the same phenomenon.\n"
      ],
      "metadata": {
        "id": "C71bNmH3ejeg"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}