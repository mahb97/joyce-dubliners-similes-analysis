{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/joyce-dubliners-similes-analysis/blob/main/02_linguistic_analysis_and_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw07RNuhGhxA"
      },
      "source": [
        "# Joyce Simile Research: Comprehensive Linguistic Analysis and Comparison Framework\n",
        "\n",
        "# Abstract\n",
        "\n",
        "This notebook implements a comprehensive computational linguistic analysis framework for comparing simile extraction methodologies in James Joyce's Dubliners. The research examines the effectiveness of manual expert annotation versus algorithmic extraction methods, establishing benchmarks against British National Corpus baseline data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Comprehensive Linguistic Analysis Framework\n",
        "# 6.1 Multi-Dataset Integration\n",
        "The comprehensive analysis pipeline implements robust loading and standardization procedures to ensure data integrity across all four datasets while preserving original categorical frameworks.\n",
        "\n",
        "# 6.2 Advanced Linguistic Feature Extraction\n",
        "Utilizing spaCy and TextBlob, the framework extracts:\n",
        "\n",
        "Syntactic complexity measures through dependency parsing\n",
        "Comparative structural analysis identifying explicit and implicit comparison markers\n",
        "Sentiment and subjectivity scoring for emotional content assessment\n",
        "Pre/post-comparator ratios for structural balance analysis\n",
        "\n",
        "# 6.3 Performance Validation\n",
        "F1 score calculations provide quantitative validation of extraction methodologies against ground truth manual annotations, establishing computational linguistic benchmarks for literary text analysis."
      ],
      "metadata": {
        "id": "zXvVrXssd9qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import json\n",
        "import zipfile\n",
        "import hashlib\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional NLP libs\n",
        "try:\n",
        "    import spacy\n",
        "except Exception:\n",
        "    spacy = None\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "print(\"COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS (FIXED)\")\n",
        "print(\"=\" * 75)\n",
        "print(\"Dataset 1: Manual Annotations (Ground Truth - Close Reading)\")\n",
        "print(\"Dataset 2: Rule-Based Extraction (Restrictive - Domain-Informed)\")\n",
        "print(\"Dataset 3: NLP Extraction (Less-Restrictive - PG Dubliners)\")\n",
        "print(\"Dataset 4: BNC Baseline Corpus (Standard English Reference)\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# Initialize spaCy if available\n",
        "nlp = None\n",
        "if spacy is not None:\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        print(\"spaCy pipeline loaded: en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"spaCy model not found; attempting to download…\")\n",
        "        os.system(\"python -m spacy download en_core_web_sm\")\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "            print(\"spaCy pipeline loaded after download: en_core_web_sm\")\n",
        "        except Exception:\n",
        "            print(\"spaCy unavailable; analysis will use simplified methods.\")\n",
        "\n",
        "class ComprehensiveLinguisticComparator:\n",
        "    \"\"\"\n",
        "    Full pipeline preserved:\n",
        "      - robust loading & standardisation\n",
        "      - linguistic feature extraction (spaCy/TextBlob, simplified fallback)\n",
        "      - category harmonisation\n",
        "      - corrected F1 score approximations\n",
        "      - combined CSV export with stable ordering\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = nlp\n",
        "        self.datasets = {}\n",
        "        self.linguistic_features = {}\n",
        "        self.comparison_results = {}\n",
        "\n",
        "    # ---------- ID / Loading / Standardisation ----------\n",
        "\n",
        "    def _ensure_ids(self, df, dataset_name, prefix=None):\n",
        "        \"\"\"\n",
        "        Ensure a unique, non-null 'Instance_ID' string column exists.\n",
        "        If missing, non-unique, or contains NaNs, regenerate sequential IDs with a readable prefix.\n",
        "        \"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return pd.DataFrame(columns=['Instance_ID'])\n",
        "\n",
        "        short = (prefix or {\n",
        "            'manual': 'MAN',\n",
        "            'rule_based': 'RST',\n",
        "            'nlp': 'NLP',\n",
        "            'bnc': 'BNC'\n",
        "        }.get(dataset_name, dataset_name[:3].upper()))\n",
        "\n",
        "        candidates = ['Instance_ID', 'ID', 'id', 'sentence_id', 'Sentence_ID', 'Index', 'index']\n",
        "        chosen = next((c for c in candidates if c in df.columns), None)\n",
        "        if chosen and chosen != 'Instance_ID':\n",
        "            df = df.rename(columns={chosen: 'Instance_ID'})\n",
        "        elif not chosen:\n",
        "            df['Instance_ID'] = np.nan\n",
        "\n",
        "        # Normalize and test uniqueness\n",
        "        df['Instance_ID'] = df['Instance_ID'].astype(str).replace({'nan': np.nan, '': np.nan})\n",
        "        needs_regen = df['Instance_ID'].isna().any() or (not df['Instance_ID'].is_unique)\n",
        "        if needs_regen:\n",
        "            df['Instance_ID'] = [f\"{short}_{i+1:05d}\" for i in range(len(df))]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _load_manual_dataset_robust(self, manual_path):\n",
        "        \"\"\"Robust loader for manual annotations with long quoted Joycean sentences.\"\"\"\n",
        "        import csv\n",
        "        try:\n",
        "            df = pd.read_csv(\n",
        "                manual_path, encoding='cp1252', quotechar='\"',\n",
        "                quoting=csv.QUOTE_MINIMAL, skipinitialspace=True, engine='python'\n",
        "            )\n",
        "            if 'Sentence Context' in df.columns:\n",
        "                df = df[df['Sentence Context'].astype(str).str.lower() != 'sentence context'].copy()\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"  pandas (python engine) failed: {e}\")\n",
        "\n",
        "        # Fallback simpler read\n",
        "        try:\n",
        "            df = pd.read_csv(manual_path, encoding='cp1252')\n",
        "            if 'Sentence Context' in df.columns:\n",
        "                df = df[df['Sentence Context'].astype(str).str.lower() != 'sentence context'].copy()\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"  pandas (default) failed: {e}\")\n",
        "\n",
        "        print(\"  Manual annotations not found or failed to load.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def load_datasets(self, manual_path, rule_based_path, nlp_path, bnc_processed_path):\n",
        "        print(\"\\nLOADING DATASETS WITH FIXED ID HANDLING & EXPLICIT LABELS\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # Manual (close reading)\n",
        "        print(\"Loading manual annotations…\")\n",
        "        self.datasets['manual'] = self._load_manual_dataset_robust(manual_path)\n",
        "        self.datasets['manual'] = self._ensure_ids(self.datasets['manual'], 'manual', prefix='MAN')\n",
        "        if not self.datasets['manual'].empty:\n",
        "            self.datasets['manual']['Original_Dataset'] = 'Manual_CloseReading'\n",
        "\n",
        "        # Rule-based (restrictive)\n",
        "        print(\"Loading rule-based (restrictive)…\")\n",
        "        self.datasets['rule_based'] = pd.read_csv(rule_based_path) if os.path.exists(rule_based_path) else pd.DataFrame()\n",
        "        self.datasets['rule_based'] = self._ensure_ids(self.datasets['rule_based'], 'rule_based', prefix='RST')\n",
        "        if not self.datasets['rule_based'].empty:\n",
        "            self.datasets['rule_based']['Original_Dataset'] = 'Restrictive_Dubliners'\n",
        "\n",
        "        # NLP (less-restrictive PG)\n",
        "        print(\"Loading NLP (less-restrictive PG)…\")\n",
        "        self.datasets['nlp'] = pd.read_csv(nlp_path) if os.path.exists(nlp_path) else pd.DataFrame()\n",
        "        self.datasets['nlp'] = self._ensure_ids(self.datasets['nlp'], 'nlp', prefix='NLP')\n",
        "        if not self.datasets['nlp'].empty:\n",
        "            self.datasets['nlp']['Original_Dataset'] = 'NLP_LessRestrictive_PG'\n",
        "\n",
        "        # BNC\n",
        "        print(\"Loading BNC baseline…\")\n",
        "        self.datasets['bnc'] = pd.read_csv(bnc_processed_path, encoding='utf-8') if os.path.exists(bnc_processed_path) else pd.DataFrame()\n",
        "        self.datasets['bnc'] = self._ensure_ids(self.datasets['bnc'], 'bnc', prefix='BNC')\n",
        "        if not self.datasets['bnc'].empty:\n",
        "            self.datasets['bnc']['Original_Dataset'] = 'BNC_Baseline'\n",
        "\n",
        "        self._standardize_datasets()\n",
        "        self._standardize_categories()\n",
        "\n",
        "        for name, df in self.datasets.items():\n",
        "            print(f\"{name:>12}: rows={len(df):4d}  \"\n",
        "                  f\"missing_IDs={df['Instance_ID'].isna().sum() if 'Instance_ID' in df else 'N/A'}  \"\n",
        "                  f\"missing_Original_Dataset={df['Original_Dataset'].isna().sum() if 'Original_Dataset' in df else 'N/A'}\")\n",
        "        print(f\"Total instances: {sum(len(df) for df in self.datasets.values())}\")\n",
        "\n",
        "    def _standardize_datasets(self):\n",
        "        print(\"Standardizing column names & adding Dataset_Source…\")\n",
        "\n",
        "        # Manual\n",
        "        df = self.datasets.get('manual', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            ren = {\n",
        "                'Category (Framwrok)': 'Category_Framework',\n",
        "                'Comparator Type ': 'Comparator_Type',\n",
        "                'Sentence Context': 'Sentence_Context',\n",
        "                'Page No.': 'Page_Number'\n",
        "            }\n",
        "            df = df.rename(columns={k: v for k, v in ren.items() if k in df.columns})\n",
        "            df['Dataset_Source'] = 'Manual_Expert_Annotation'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['manual'] = df\n",
        "        else:\n",
        "            self.datasets['manual'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context','Page_Number',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # Rule-based\n",
        "        df = self.datasets.get('rule_based', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            df = df.rename(columns={\n",
        "                'Sentence Context': 'Sentence_Context',\n",
        "                'Comparator Type ': 'Comparator_Type',\n",
        "                'Category (Framwrok)': 'Category_Framework'\n",
        "            })\n",
        "            df['Dataset_Source'] = 'Rule_Based_Domain_Informed'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['rule_based'] = df\n",
        "        else:\n",
        "            self.datasets['rule_based'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # NLP (less-restrictive)\n",
        "        df = self.datasets.get('nlp', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            if 'Sentence_Context' not in df.columns:\n",
        "                for c in ['Sentence Context','text','sentence','context','content']:\n",
        "                    if c in df.columns:\n",
        "                        df = df.rename(columns={c: 'Sentence_Context'})\n",
        "                        break\n",
        "            if 'Comparator Type ' in df.columns:\n",
        "                df = df.rename(columns={'Comparator Type ': 'Comparator_Type'})\n",
        "            if 'Category (Framwrok)' in df.columns and 'Category_Framework' not in df.columns:\n",
        "                df = df.rename(columns={'Category (Framwrok)': 'Category_Framework'})\n",
        "            if 'Category_Framework' not in df.columns:\n",
        "                df['Category_Framework'] = 'NLP_Basic_Pattern'\n",
        "            df['Dataset_Source'] = 'NLP_General_Pattern_Recognition'\n",
        "            df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['nlp'] = df\n",
        "        else:\n",
        "            self.datasets['nlp'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Category_Framework','Comparator_Type','Sentence_Context',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        # BNC\n",
        "        df = self.datasets.get('bnc', pd.DataFrame())\n",
        "        if not df.empty:\n",
        "            if 'Category (Framework)' in df.columns and 'Category_Framework' not in df.columns:\n",
        "                df = df.rename(columns={'Category (Framework)':'Category_Framework'})\n",
        "            if 'Comparator Type' in df.columns and 'Comparator_Type' not in df.columns:\n",
        "                df = df.rename(columns={'Comparator Type':'Comparator_Type'})\n",
        "            if 'Sentence Context' in df.columns and 'Sentence_Context' not in df.columns:\n",
        "                df = df.rename(columns={'Sentence Context':'Sentence_Context'})\n",
        "            df['Dataset_Source'] = 'BNC_Standard_English_Baseline'\n",
        "            if 'Category_Framework' in df.columns:\n",
        "                df['Category_Framework'] = df['Category_Framework'].astype(str)\n",
        "            self.datasets['bnc'] = df\n",
        "        else:\n",
        "            self.datasets['bnc'] = pd.DataFrame(columns=[\n",
        "                'Instance_ID','Sentence_Context','Comparator_Type','Category_Framework',\n",
        "                'Dataset_Source','Original_Dataset'\n",
        "            ])\n",
        "\n",
        "        print(\"Standardization complete.\")\n",
        "\n",
        "    def _standardize_categories(self):\n",
        "        print(\"Harmonizing Category_Framework labels…\")\n",
        "        mapping = {\n",
        "            'NLP_Basic': 'Standard',\n",
        "            'NLP_Basic_Pattern': 'Standard',\n",
        "            'Standard_English_Usage': 'Standard',\n",
        "\n",
        "            'Standard': 'Standard',\n",
        "            'Joycean_Quasi': 'Joycean_Quasi',\n",
        "            'Joycean_Framed': 'Joycean_Framed',\n",
        "            'Joycean_Silent': 'Joycean_Silent',\n",
        "            'Joycean_Quasi_Fuzzy': 'Joycean_Quasi_Fuzzy',\n",
        "\n",
        "            'Quasi_Similes': 'Quasi_Similes',\n",
        "            'nan': 'Uncategorized', 'NaN': 'Uncategorized', '': 'Uncategorized'\n",
        "        }\n",
        "        for name, df in self.datasets.items():\n",
        "            if df.empty or 'Category_Framework' not in df.columns:\n",
        "                continue\n",
        "            df['Category_Framework'] = df['Category_Framework'].astype(str).map(mapping).fillna(df['Category_Framework'])\n",
        "            self.datasets[name] = df\n",
        "        print(\"Category harmonization complete.\")\n",
        "\n",
        "    # ---------- Linguistic analysis (spaCy/TextBlob; simplified fallback) ----------\n",
        "\n",
        "    def _find_comparator_position(self, doc, comparator_type):\n",
        "        comparator_type = str(comparator_type).lower().strip()\n",
        "        patterns = {\n",
        "            'like': ['like'],\n",
        "            'as if': ['as','if'],\n",
        "            'as': ['as'],\n",
        "            'seemed': ['seemed','seem','seems'],\n",
        "            'colon': [':'],\n",
        "            'semicolon': [';'],\n",
        "            'ellipsis': ['...', '…'],\n",
        "            'en dash': ['—','–','-'],\n",
        "            'resembl': ['resemble','resembled','resembling']\n",
        "        }\n",
        "        for i, token in enumerate(doc):\n",
        "            t = token.text.lower()\n",
        "            if t == comparator_type:\n",
        "                return i\n",
        "            if comparator_type in patterns and t in patterns[comparator_type]:\n",
        "                return i\n",
        "        return None\n",
        "\n",
        "    def _analyze_comparative_structure(self, doc, comparator_type):\n",
        "        structure = {\n",
        "            'has_explicit_comparator': False,\n",
        "            'comparator_type': str(comparator_type).strip() or \"Unknown\",\n",
        "            'comparative_adjectives': [],\n",
        "            'superlative_adjectives': [],\n",
        "            'modal_verbs': [],\n",
        "            'epistemic_markers': []\n",
        "        }\n",
        "        for token in doc:\n",
        "            if token.text.lower() in ['like','as','than','似']:\n",
        "                structure['has_explicit_comparator'] = True\n",
        "            if token.tag_ in ['JJR','RBR']:\n",
        "                structure['comparative_adjectives'].append(token.text)\n",
        "            elif token.tag_ in ['JJS','RBS']:\n",
        "                structure['superlative_adjectives'].append(token.text)\n",
        "            if token.pos_ == 'AUX' and token.text.lower() in ['might','could','would','should','may']:\n",
        "                structure['modal_verbs'].append(token.text)\n",
        "            if token.text.lower() in ['perhaps','maybe','possibly','apparently','seemingly']:\n",
        "                structure['epistemic_markers'].append(token.text)\n",
        "        return structure\n",
        "\n",
        "    def _calculate_syntactic_complexity(self, doc):\n",
        "        def depth(tok, d=0):\n",
        "            if not list(tok.children):\n",
        "                return d\n",
        "            return max(depth(ch, d+1) for ch in tok.children)\n",
        "        roots = [t for t in doc if t.head == t]\n",
        "        if not roots:\n",
        "            return 0\n",
        "        try:\n",
        "            return max(depth(r) for r in roots)\n",
        "        except Exception:\n",
        "            return np.nan\n",
        "\n",
        "    def perform_comprehensive_linguistic_analysis(self):\n",
        "        print(\"\\nPERFORMING LINGUISTIC ANALYSIS\")\n",
        "        print(\"-\" * 35)\n",
        "        if self.nlp is None:\n",
        "            print(\"spaCy unavailable → simplified analysis.\")\n",
        "            return self._perform_simplified_analysis()\n",
        "\n",
        "        for name, df in list(self.datasets.items()):\n",
        "            if df.empty:\n",
        "                print(f\"Skipping empty dataset: {name}\")\n",
        "                continue\n",
        "\n",
        "            # Initialize feature containers\n",
        "            n = len(df)\n",
        "            feats = {\n",
        "                'Total_Tokens': [None]*n,\n",
        "                'Pre_Comparator_Tokens': [None]*n,\n",
        "                'Post_Comparator_Tokens': [None]*n,\n",
        "                'Pre_Post_Ratio': [None]*n,\n",
        "                'Lemmatized_Text': [None]*n,\n",
        "                'POS_Tags': [None]*n,\n",
        "                'POS_Distribution': [None]*n,\n",
        "                'Sentiment_Polarity': [None]*n,\n",
        "                'Sentiment_Subjectivity': [None]*n,\n",
        "                'Comparative_Structure': [None]*n,\n",
        "                'Syntactic_Complexity': [None]*n,\n",
        "                'Sentence_Length': [None]*n,\n",
        "                'Adjective_Count': [None]*n,\n",
        "                'Verb_Count': [None]*n,\n",
        "                'Noun_Count': [None]*n,\n",
        "                'Figurative_Density': [None]*n\n",
        "            }\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                sent = str(row.get('Sentence_Context', '') or '').strip()\n",
        "                comp = row.get('Comparator_Type', '')\n",
        "                if not sent:\n",
        "                    continue\n",
        "                try:\n",
        "                    doc = self.nlp(sent)\n",
        "                    tokens = [t for t in doc if not t.is_space and not t.is_punct]\n",
        "                    total = len(tokens)\n",
        "                    pos = self._find_comparator_position(doc, comp)\n",
        "                    if pos is not None:\n",
        "                        pre, post = pos, total - pos - 1\n",
        "                        ratio = pre / post if post > 0 else 0\n",
        "                    else:\n",
        "                        pre = total // 2\n",
        "                        post = total - pre\n",
        "                        ratio = pre / post if post > 0 else np.nan\n",
        "\n",
        "                    lemmas = [t.lemma_.lower() for t in doc if not t.is_space and not t.is_punct and not t.is_stop]\n",
        "                    pos_tags = [t.pos_ for t in doc if not t.is_space]\n",
        "                    pos_dist = Counter(pos_tags)\n",
        "\n",
        "                    blob = TextBlob(sent)\n",
        "                    pol, subj = blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "\n",
        "                    comp_struct = self._analyze_comparative_structure(doc, comp)\n",
        "                    complexity = self._calculate_syntactic_complexity(doc)\n",
        "                    slen = len(sent.split())\n",
        "                    adj = sum(1 for t in doc if t.pos_ == 'ADJ')\n",
        "                    vrb = sum(1 for t in doc if t.pos_ == 'VERB')\n",
        "                    nou = sum(1 for t in doc if t.pos_ == 'NOUN')\n",
        "                    figurative_markers = ['like','as','似','such','seem','appear']\n",
        "                    fdens = sum(1 for t in doc if t.text.lower() in figurative_markers) / total if total else 0\n",
        "\n",
        "                    loc = df.index.get_loc(idx)\n",
        "                    feats['Total_Tokens'][loc] = total\n",
        "                    feats['Pre_Comparator_Tokens'][loc] = pre\n",
        "                    feats['Post_Comparator_Tokens'][loc] = post\n",
        "                    feats['Pre_Post_Ratio'][loc] = ratio\n",
        "                    feats['Lemmatized_Text'][loc] = ' '.join(lemmas)\n",
        "                    feats['POS_Tags'][loc] = '; '.join(pos_tags)\n",
        "                    feats['POS_Distribution'][loc] = dict(pos_dist)\n",
        "                    feats['Sentiment_Polarity'][loc] = pol\n",
        "                    feats['Sentiment_Subjectivity'][loc] = subj\n",
        "                    feats['Comparative_Structure'][loc] = comp_struct\n",
        "                    feats['Syntactic_Complexity'][loc] = complexity\n",
        "                    feats['Sentence_Length'][loc] = slen\n",
        "                    feats['Adjective_Count'][loc] = adj\n",
        "                    feats['Verb_Count'][loc] = vrb\n",
        "                    feats['Noun_Count'][loc] = nou\n",
        "                    feats['Figurative_Density'][loc] = fdens\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error in {name} row {idx}: {e}\")\n",
        "\n",
        "            # Serialize complex columns for CSV\n",
        "            df['POS_Distribution'] = [json.dumps(x) if isinstance(x, dict) else None for x in feats['POS_Distribution']]\n",
        "            df['Comparative_Structure'] = [json.dumps(x) if isinstance(x, dict) else None for x in feats['Comparative_Structure']]\n",
        "            for k, v in feats.items():\n",
        "                if k in ['POS_Distribution','Comparative_Structure']:\n",
        "                    continue\n",
        "                df[k] = v\n",
        "\n",
        "            self.linguistic_features[name] = feats\n",
        "            self.datasets[name] = df\n",
        "            print(f\"Finished linguistic analysis for {name}.\")\n",
        "\n",
        "        print(\"All datasets processed.\")\n",
        "\n",
        "    def _perform_simplified_analysis(self):\n",
        "        for name, df in list(self.datasets.items()):\n",
        "            if df.empty or 'Sentence_Context' not in df.columns:\n",
        "                continue\n",
        "            n = len(df)\n",
        "            df['Total_Tokens'] = [None]*n\n",
        "            df['Pre_Comparator_Tokens'] = [None]*n\n",
        "            df['Post_Comparator_Tokens'] = [None]*n\n",
        "            df['Pre_Post_Ratio'] = [np.nan]*n\n",
        "            df['Sentiment_Polarity'] = [np.nan]*n\n",
        "            df['Sentiment_Subjectivity'] = [np.nan]*n\n",
        "            df['Sentence_Length'] = [None]*n\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                sent = str(row.get('Sentence_Context','') or '').strip()\n",
        "                if not sent:\n",
        "                    continue\n",
        "                tokens = sent.split()\n",
        "                total = len(tokens)\n",
        "                df.loc[idx, 'Total_Tokens'] = total\n",
        "                df.loc[idx, 'Sentence_Length'] = total\n",
        "                try:\n",
        "                    blob = TextBlob(sent)\n",
        "                    df.loc[idx, 'Sentiment_Polarity'] = blob.sentiment.polarity\n",
        "                    df.loc[idx, 'Sentiment_Subjectivity'] = blob.sentiment.subjectivity\n",
        "                except Exception:\n",
        "                    pass\n",
        "                comp = row.get('Comparator_Type','')\n",
        "                pos = -1\n",
        "                if str(comp).strip():\n",
        "                    try:\n",
        "                        m = re.search(r'\\b' + re.escape(str(comp).strip()) + r'\\b', sent, re.IGNORECASE)\n",
        "                        if m:\n",
        "                            pre_text = sent[:m.start()]\n",
        "                            pos = len(pre_text.split())\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                if total > 0 and pos != -1:\n",
        "                    pre, post = pos, total - pos - 1\n",
        "                    df.loc[idx, 'Pre_Comparator_Tokens'] = pre\n",
        "                    df.loc[idx, 'Post_Comparator_Tokens'] = post\n",
        "                    df.loc[idx, 'Pre_Post_Ratio'] = (pre / post) if post > 0 else np.nan\n",
        "            self.datasets[name] = df\n",
        "        print(\"Simplified analysis complete.\")\n",
        "\n",
        "    # ---------- F1 metrics (as in your original) ----------\n",
        "\n",
        "    def calculate_corrected_f1_scores(self):\n",
        "        print(\"\\nCALCULATING CORRECTED F1 PERFORMANCE METRICS\")\n",
        "        print(\"-\" * 44)\n",
        "\n",
        "        manual_df = self.datasets.get('manual', pd.DataFrame())\n",
        "        rule_based_df = self.datasets.get('rule_based', pd.DataFrame())\n",
        "        nlp_df = self.datasets.get('nlp', pd.DataFrame())\n",
        "\n",
        "        f1_analysis = {}\n",
        "\n",
        "        if manual_df.empty or 'Category_Framework' not in manual_df.columns:\n",
        "            print(\"F1 calculation unavailable: manual annotations missing/invalid.\")\n",
        "            self.comparison_results['f1_analysis'] = None\n",
        "            return None, None\n",
        "\n",
        "        if not rule_based_df.empty and 'Category_Framework' in rule_based_df.columns:\n",
        "            print(\"\\nEvaluating Rule-Based (Domain-Informed) vs Manual Annotations:\")\n",
        "            category_metrics_rule, overall_f1_rule = self._calculate_f1_metrics(\n",
        "                manual_df, rule_based_df, 'Rule_Based_Domain_Informed'\n",
        "            )\n",
        "            f1_analysis['rule_based_vs_manual'] = {\n",
        "                'category_metrics': category_metrics_rule,\n",
        "                'overall_f1': overall_f1_rule\n",
        "            }\n",
        "            print(f\"Overall F1 (Rule-Based vs Manual): {overall_f1_rule:.3f}\")\n",
        "        else:\n",
        "            print(\"Rule-Based evaluation unavailable.\")\n",
        "\n",
        "        if not nlp_df.empty and 'Category_Framework' in nlp_df.columns:\n",
        "            print(\"\\nEvaluating NLP (General Pattern Recognition) vs Manual Annotations:\")\n",
        "            category_metrics_nlp, overall_f1_nlp = self._calculate_f1_metrics(\n",
        "                manual_df, nlp_df, 'NLP_General_Pattern'\n",
        "            )\n",
        "            f1_analysis['nlp_vs_manual'] = {\n",
        "                'category_metrics': category_metrics_nlp,\n",
        "                'overall_f1': overall_f1_nlp\n",
        "            }\n",
        "            print(f\"Overall F1 (NLP vs Manual): {overall_f1_nlp:.3f}\")\n",
        "        else:\n",
        "            print(\"NLP evaluation unavailable.\")\n",
        "\n",
        "        self.comparison_results['f1_analysis'] = f1_analysis\n",
        "        primary_f1 = f1_analysis.get('rule_based_vs_manual', {}).get('overall_f1', None)\n",
        "        return f1_analysis, primary_f1\n",
        "\n",
        "    def _calculate_f1_metrics(self, ground_truth_df, prediction_df, prediction_name):\n",
        "        truth_categories = ground_truth_df['Category_Framework'].astype(str).value_counts()\n",
        "        pred_categories = prediction_df['Category_Framework'].astype(str).value_counts()\n",
        "\n",
        "        all_categories = sorted(set(truth_categories.index) | set(pred_categories.index))\n",
        "        category_metrics = {}\n",
        "\n",
        "        total_truth = len(ground_truth_df)\n",
        "        total_pred = len(prediction_df)\n",
        "\n",
        "        for category in all_categories:\n",
        "            truth_count = truth_categories.get(category, 0)\n",
        "            pred_count = pred_categories.get(category, 0)\n",
        "\n",
        "            precision = min(truth_count / pred_count, 1.0) if pred_count > 0 else 0.0\n",
        "            recall = min(pred_count / truth_count, 1.0) if truth_count > 0 else 0.0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "            category_metrics[category] = {\n",
        "                f'{prediction_name}_count': pred_count,\n",
        "                'manual_count': truth_count,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_score': f1\n",
        "            }\n",
        "\n",
        "            print(f\"  {category}: {prediction_name}: {pred_count}, Manual: {truth_count}, \"\n",
        "                  f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
        "\n",
        "        overall_precision = min(total_truth / total_pred, 1.0) if total_pred > 0 else 0.0\n",
        "        overall_recall = min(total_pred / total_truth, 1.0) if total_truth > 0 else 0.0\n",
        "        overall_f1 = (2 * overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0\n",
        "\n",
        "        return category_metrics, overall_f1\n",
        "\n",
        "    # ---------- Save / Export ----------\n",
        "\n",
        "    def save_comprehensive_results(self, output_path=\"comprehensive_linguistic_analysis_corrected.csv\"):\n",
        "        print(\"\\nSAVING COMPREHENSIVE RESULTS …\")\n",
        "        frames = []\n",
        "        for name, df in self.datasets.items():\n",
        "            if df is None or df.empty:\n",
        "                continue\n",
        "            d = df.copy()\n",
        "            for col, default in [\n",
        "                ('Original_Dataset', name),\n",
        "                ('Instance_ID', None),\n",
        "                ('Sentence_Context', None),\n",
        "                ('Category_Framework', None),\n",
        "                ('Comparator_Type', None)\n",
        "            ]:\n",
        "                if col not in d.columns:\n",
        "                    d[col] = default\n",
        "\n",
        "            if d['Instance_ID'].isna().any() or (not d['Instance_ID'].astype(str).is_unique):\n",
        "                d = self._ensure_ids(d, name)\n",
        "\n",
        "            base = ['Instance_ID','Original_Dataset','Sentence_Context','Category_Framework','Comparator_Type']\n",
        "            others = [c for c in d.columns if c not in base]\n",
        "            d = d[base + others]\n",
        "            frames.append(d)\n",
        "\n",
        "        if not frames:\n",
        "            print(\"No data to save.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        combined = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "        # Stable sort: Manual → Restrictive → Less‑Restrictive PG → BNC\n",
        "        order = {\n",
        "            'Manual_CloseReading': 1,\n",
        "            'Restrictive_Dubliners': 2,\n",
        "            'NLP_LessRestrictive_PG': 3,\n",
        "            'BNC_Baseline': 4\n",
        "        }\n",
        "        combined['__order__'] = combined['Original_Dataset'].map(order).fillna(99).astype(int)\n",
        "\n",
        "        def _id_numeric_tail(x):\n",
        "            m = re.search(r'(\\d+)$', str(x))\n",
        "            return int(m.group(1)) if m else 0\n",
        "\n",
        "        combined = combined.sort_values(\n",
        "            by=['__order__','Original_Dataset','Instance_ID'],\n",
        "            key=lambda s: s.map(_id_numeric_tail) if s.name == 'Instance_ID' else s\n",
        "        ).drop(columns='__order__')\n",
        "\n",
        "        combined.to_csv(output_path, index=False)\n",
        "        print(f\"Saved: {output_path}\")\n",
        "        print(\"Integrity:\",\n",
        "              \"missing Instance_ID =\", combined['Instance_ID'].isna().sum(),\n",
        "              \"| missing Original_Dataset =\", combined['Original_Dataset'].isna().sum(),\n",
        "              \"| rows =\", len(combined))\n",
        "        return combined\n",
        "\n",
        "\n",
        "# ========= RUN THE PIPELINE (with your filenames) =========\n",
        "manual_path = \"All Similes - Dubliners cont.csv\"           # close reading (manual)\n",
        "rule_based_path = \"dubliners_corrected_extraction.csv\"    # restrictive\n",
        "nlp_path = \"dubliners_nlp_basic_extraction.csv\"           # less-restrictive PG Dubliners\n",
        "bnc_processed_path = \"bnc_processed_similes.csv\"          # BNC baseline\n",
        "\n",
        "comparator = ComprehensiveLinguisticComparator()\n",
        "comparator.load_datasets(manual_path, rule_based_path, nlp_path, bnc_processed_path)\n",
        "comparator.perform_comprehensive_linguistic_analysis()\n",
        "f1_analysis, primary_f1 = comparator.calculate_corrected_f1_scores()\n",
        "results_df = comparator.save_comprehensive_results(\"comprehensive_linguistic_analysis_corrected.csv\")\n",
        "\n",
        "print(\"\\nPIPELINE COMPLETED.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCwl9PwLS_96",
        "outputId": "f0c41525-b3c4-438f-dc83-80ecf35a6437"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPREHENSIVE LINGUISTIC COMPARISON OF FOUR SIMILE DATASETS (FIXED)\n",
            "===========================================================================\n",
            "Dataset 1: Manual Annotations (Ground Truth - Close Reading)\n",
            "Dataset 2: Rule-Based Extraction (Restrictive - Domain-Informed)\n",
            "Dataset 3: NLP Extraction (Less-Restrictive - PG Dubliners)\n",
            "Dataset 4: BNC Baseline Corpus (Standard English Reference)\n",
            "===========================================================================\n",
            "spaCy pipeline loaded: en_core_web_sm\n",
            "\n",
            "LOADING DATASETS WITH FIXED ID HANDLING & EXPLICIT LABELS\n",
            "----------------------------------------------------------------------\n",
            "Loading manual annotations…\n",
            "Loading rule-based (restrictive)…\n",
            "Loading NLP (less-restrictive PG)…\n",
            "Loading BNC baseline…\n",
            "Standardizing column names & adding Dataset_Source…\n",
            "Standardization complete.\n",
            "Harmonizing Category_Framework labels…\n",
            "Category harmonization complete.\n",
            "      manual: rows= 184  missing_IDs=0  missing_Original_Dataset=0\n",
            "  rule_based: rows= 218  missing_IDs=0  missing_Original_Dataset=0\n",
            "         nlp: rows= 103  missing_IDs=0  missing_Original_Dataset=0\n",
            "         bnc: rows= 200  missing_IDs=0  missing_Original_Dataset=0\n",
            "Total instances: 705\n",
            "\n",
            "PERFORMING LINGUISTIC ANALYSIS\n",
            "-----------------------------------\n",
            "Finished linguistic analysis for manual.\n",
            "Finished linguistic analysis for rule_based.\n",
            "Finished linguistic analysis for nlp.\n",
            "Finished linguistic analysis for bnc.\n",
            "All datasets processed.\n",
            "\n",
            "CALCULATING CORRECTED F1 PERFORMANCE METRICS\n",
            "--------------------------------------------\n",
            "\n",
            "Evaluating Rule-Based (Domain-Informed) vs Manual Annotations:\n",
            "  Joycean_Framed: Rule_Based_Domain_Informed: 4, Manual: 18, Precision: 1.000, Recall: 0.222, F1: 0.364\n",
            "  Joycean_Quasi: Rule_Based_Domain_Informed: 47, Manual: 53, Precision: 1.000, Recall: 0.887, F1: 0.940\n",
            "  Joycean_Quasi_Fuzzy: Rule_Based_Domain_Informed: 14, Manual: 13, Precision: 0.929, Recall: 1.000, F1: 0.963\n",
            "  Joycean_Silent: Rule_Based_Domain_Informed: 3, Manual: 6, Precision: 1.000, Recall: 0.500, F1: 0.667\n",
            "  Standard: Rule_Based_Domain_Informed: 150, Manual: 93, Precision: 0.620, Recall: 1.000, F1: 0.765\n",
            "  Uncategorized: Rule_Based_Domain_Informed: 0, Manual: 1, Precision: 0.000, Recall: 0.000, F1: 0.000\n",
            "Overall F1 (Rule-Based vs Manual): 0.915\n",
            "\n",
            "Evaluating NLP (General Pattern Recognition) vs Manual Annotations:\n",
            "  Joycean_Framed: NLP_General_Pattern: 0, Manual: 18, Precision: 0.000, Recall: 0.000, F1: 0.000\n",
            "  Joycean_Quasi: NLP_General_Pattern: 0, Manual: 53, Precision: 0.000, Recall: 0.000, F1: 0.000\n",
            "  Joycean_Quasi_Fuzzy: NLP_General_Pattern: 0, Manual: 13, Precision: 0.000, Recall: 0.000, F1: 0.000\n",
            "  Joycean_Silent: NLP_General_Pattern: 0, Manual: 6, Precision: 0.000, Recall: 0.000, F1: 0.000\n",
            "  Standard: NLP_General_Pattern: 103, Manual: 93, Precision: 0.903, Recall: 1.000, F1: 0.949\n",
            "  Uncategorized: NLP_General_Pattern: 0, Manual: 1, Precision: 0.000, Recall: 0.000, F1: 0.000\n",
            "Overall F1 (NLP vs Manual): 0.718\n",
            "\n",
            "SAVING COMPREHENSIVE RESULTS …\n",
            "Saved: comprehensive_linguistic_analysis_corrected.csv\n",
            "Integrity: missing Instance_ID = 0 | missing Original_Dataset = 0 | rows = 705\n",
            "\n",
            "PIPELINE COMPLETED.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Statistical Significance Testing\n",
        "# 7.1 Multi-Group Comparative Analysis\n",
        "The statistical analysis distinguishes between Joyce Manual, Joyce Restrictive, Joyce Less-Restrictive, and BNC subsets to provide granular assessment of methodological differences.\n",
        "\n",
        "# 7.2 Robust Statistical Framework\n",
        "Implementation includes:\n",
        "\n",
        "Four-way chi-square analysis for categorical distribution testing\n",
        "Newcombe-Wilson confidence intervals for two-proportion comparisons\n",
        "Binomial testing against BNC reference proportions\n",
        "Welch t-tests and Mann-Whitney U tests for continuous feature assessment\n",
        "\n",
        "# 7.3 Topic Modeling Integration\n",
        "Latent Dirichlet Allocation provides thematic analysis across all dataset subsets, revealing content-based distinctions complementing statistical findings."
      ],
      "metadata": {
        "id": "ft6Meu_eeMDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ROBUST STATISTICAL SIGNIFICANCE + TOPIC MODELLING (Joyce subsets vs BNC)\n",
        "# - Distinguishes: Joyce Manual, Joyce Restrictive, Joyce Less-Restrictive PG, and BNC\n",
        "# - Saves multi-group and per-subset outputs to analysis_outputs/\n",
        "# =============================================================================\n",
        "\n",
        "import os, json, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.stats import chi2_contingency, mannwhitneyu, ttest_ind, binomtest\n",
        "try:\n",
        "    from statsmodels.stats.proportion import proportions_ztest, confint_proportions_2indep\n",
        "    _HAS_STATSMODELS = True\n",
        "except Exception:\n",
        "    _HAS_STATSMODELS = False\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "out_dir = os.path.join(\"analysis_outputs\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "print(\"\\nROBUST STATISTICAL ANALYSIS (Joyce subsets vs BNC)\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# --- Sanity: results_df must exist from the previous comprehensive cell ---\n",
        "if 'results_df' not in globals() or results_df is None or results_df.empty:\n",
        "    raise RuntimeError(\"results_df not found or empty. Run the comprehensive analysis cell first.\")\n",
        "\n",
        "# --- Define groups explicitly ---\n",
        "LABELS = {\n",
        "    \"Manual_CloseReading\":      \"Joyce_Manual\",\n",
        "    \"Restrictive_Dubliners\":    \"Joyce_Restrictive\",\n",
        "    \"NLP_LessRestrictive_PG\":   \"Joyce_LessRestrictive\",\n",
        "    \"BNC_Baseline\":             \"BNC\"\n",
        "}\n",
        "\n",
        "df = results_df.copy()\n",
        "if \"Original_Dataset\" not in df.columns:\n",
        "    raise RuntimeError(\"results_df is missing 'Original_Dataset' column.\")\n",
        "\n",
        "df[\"__Group__\"] = df[\"Original_Dataset\"].map(LABELS).fillna(df[\"Original_Dataset\"])\n",
        "\n",
        "# --- Split groups ---\n",
        "groups = {\n",
        "    \"Joyce_Manual\":         df[df[\"__Group__\"]==\"Joyce_Manual\"],\n",
        "    \"Joyce_Restrictive\":    df[df[\"__Group__\"]==\"Joyce_Restrictive\"],\n",
        "    \"Joyce_LessRestrictive\":df[df[\"__Group__\"]==\"Joyce_LessRestrictive\"],\n",
        "    \"BNC\":                  df[df[\"__Group__\"]==\"BNC\"]\n",
        "}\n",
        "\n",
        "for gname, gdf in groups.items():\n",
        "    print(f\"{gname:22s}: {len(gdf)} rows\")\n",
        "\n",
        "# ---------- 1) 4-way Chi-square on Category_Framework ----------\n",
        "print(\"\\n4-way Chi-square on Category_Framework (Joyce subsets vs BNC):\")\n",
        "cats = set()\n",
        "for gdf in groups.values():\n",
        "    if \"Category_Framework\" in gdf.columns:\n",
        "        cats |= set(gdf[\"Category_Framework\"].dropna().astype(str).unique())\n",
        "categories = sorted(cats)\n",
        "\n",
        "contingency_4way = pd.DataFrame(\n",
        "    {\n",
        "        \"Joyce_Manual\":          [groups[\"Joyce_Manual\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in categories],\n",
        "        \"Joyce_Restrictive\":     [groups[\"Joyce_Restrictive\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in categories],\n",
        "        \"Joyce_LessRestrictive\": [groups[\"Joyce_LessRestrictive\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in categories],\n",
        "        \"BNC\":                   [groups[\"BNC\"][\"Category_Framework\"].value_counts().get(cat,0) for cat in categories],\n",
        "    },\n",
        "    index=categories\n",
        ")\n",
        "\n",
        "chi2_4, p_4, dof_4, exp_4 = chi2_contingency(contingency_4way)\n",
        "print(f\"χ² = {chi2_4:.4f} | df = {dof_4} | p = {p_4:.6f}\")\n",
        "\n",
        "# Save 4-way contingency + expected + standardized residuals\n",
        "path_cont_4 = os.path.join(out_dir, f\"chi2_contingency_by_subset_{ts}.csv\")\n",
        "path_exp_4  = os.path.join(out_dir, f\"chi2_expected_by_subset_{ts}.csv\")\n",
        "contingency_4way.to_csv(path_cont_4)\n",
        "\n",
        "exp_df_4 = pd.DataFrame(exp_4, index=categories, columns=contingency_4way.columns)\n",
        "exp_df_4.to_csv(path_exp_4)\n",
        "\n",
        "std_resid_4 = (contingency_4way - exp_df_4) / np.sqrt(exp_df_4.replace(0, np.nan))\n",
        "path_resid_4 = os.path.join(out_dir, f\"chi2_std_residuals_by_subset_{ts}.csv\")\n",
        "std_resid_4.to_csv(path_resid_4)\n",
        "\n",
        "# ---------- 2) Two-proportion tests (each Joyce subset vs BNC) ----------\n",
        "print(\"\\nTwo-proportion tests (Newcombe–Wilson) for each Joyce subset vs BNC:\")\n",
        "two_prop_rows = []\n",
        "bnc_total = len(groups[\"BNC\"])\n",
        "bnc_counts = groups[\"BNC\"][\"Category_Framework\"].value_counts()\n",
        "\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "    subset_total = len(groups[subset])\n",
        "    subset_counts = groups[subset][\"Category_Framework\"].value_counts()\n",
        "    for cat in categories:\n",
        "        cA = subset_counts.get(cat,0); nA = subset_total\n",
        "        cB = bnc_counts.get(cat,0);    nB = bnc_total\n",
        "        row = {\"Comparison\":\"%s_vs_BNC\" % subset, \"Subset\":subset, \"Category\":cat,\n",
        "               \"count_A\":cA, \"n_A\":nA, \"count_B\":cB, \"n_B\":nB}\n",
        "        if _HAS_STATSMODELS and nA>0 and nB>0:\n",
        "            z, pz = proportions_ztest(np.array([cA,cB]), np.array([nA,nB]))\n",
        "            ci_low, ci_up = confint_proportions_2indep(cA, nA, cB, nB, method=\"newcombe\")\n",
        "            row.update({\"z\":float(z), \"p_value\":float(pz), \"CI_low\":float(ci_low), \"CI_up\":float(ci_up)})\n",
        "            print(f\"  {subset:22s} | {cat:20s} z={z:6.3f} p={pz:.6g} CI[{ci_low:.3f},{ci_up:.3f}]\")\n",
        "        else:\n",
        "            row.update({\"z\":np.nan, \"p_value\":np.nan, \"CI_low\":np.nan, \"CI_up\":np.nan})\n",
        "            if not _HAS_STATSMODELS:\n",
        "                print(f\"  {subset:22s} | {cat:20s} (statsmodels unavailable → skipping z/CI)\")\n",
        "        two_prop_rows.append(row)\n",
        "\n",
        "two_prop_df = pd.DataFrame(two_prop_rows)\n",
        "path_two_prop = os.path.join(out_dir, f\"two_prop_newcombe_by_subset_{ts}.csv\")\n",
        "two_prop_df.to_csv(path_two_prop, index=False)\n",
        "\n",
        "# ---------- 3) Binomial tests (each Joyce subset vs BNC proportion) ----------\n",
        "print(\"\\nBinomial tests (each Joyce subset vs BNC category proportion):\")\n",
        "binom_rows = []\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "    nA = len(groups[subset])\n",
        "    for cat in categories:\n",
        "        cA = groups[subset][\"Category_Framework\"].value_counts().get(cat,0)\n",
        "        cB = bnc_counts.get(cat,0); nB = bnc_total\n",
        "        p_ref = (cB/nB) if nB>0 else 0.0\n",
        "        if nA>0 and p_ref>0:\n",
        "            bt = binomtest(cA, n=nA, p=p_ref)\n",
        "            print(f\"  {subset:22s} | {cat:20s} {cA}/{nA} vs p_ref={p_ref:.4f} p={bt.pvalue:.6g}\")\n",
        "            binom_rows.append({\"Comparison\":\"%s_vs_BNC\" % subset, \"Subset\":subset, \"Category\":cat,\n",
        "                               \"count_A\":cA, \"n_A\":nA, \"p_ref_BNC\":p_ref, \"p_value\":bt.pvalue})\n",
        "        else:\n",
        "            binom_rows.append({\"Comparison\":\"%s_vs_BNC\" % subset, \"Subset\":subset, \"Category\":cat,\n",
        "                               \"count_A\":cA, \"n_A\":nA, \"p_ref_BNC\":p_ref, \"p_value\":np.nan})\n",
        "\n",
        "binom_df = pd.DataFrame(binom_rows)\n",
        "path_binom = os.path.join(out_dir, f\"binomial_tests_by_subset_{ts}.csv\")\n",
        "binom_df.to_csv(path_binom, index=False)\n",
        "\n",
        "# ---------- 4) Continuous features (subset vs BNC) ----------\n",
        "print(\"\\nContinuous features (Welch t + Mann–Whitney U), each Joyce subset vs BNC:\")\n",
        "continuous_feats = [\"Sentence_Length\",\"Pre_Post_Ratio\",\"Sentiment_Polarity\",\"Sentiment_Subjectivity\"]\n",
        "cont_rows = []\n",
        "\n",
        "for feat in continuous_feats:\n",
        "    for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\"]:\n",
        "        A = pd.to_numeric(groups[subset][feat], errors=\"coerce\").dropna() if feat in groups[subset].columns else pd.Series(dtype=float)\n",
        "        B = pd.to_numeric(groups[\"BNC\"][feat], errors=\"coerce\").dropna()     if feat in groups[\"BNC\"].columns else pd.Series(dtype=float)\n",
        "        if len(A)>10 and len(B)>10:\n",
        "            t,p_t = ttest_ind(A,B,equal_var=False)\n",
        "            u,p_u = mannwhitneyu(A,B,alternative=\"two-sided\")\n",
        "            cont_rows.append({\n",
        "                \"Feature\":feat, \"Comparison\":\"%s_vs_BNC\" % subset, \"Subset\":subset,\n",
        "                \"A_n\":len(A), \"A_mean\":float(np.mean(A)), \"A_median\":float(np.median(A)),\n",
        "                \"B_n\":len(B), \"B_mean\":float(np.mean(B)), \"B_median\":float(np.median(B)),\n",
        "                \"t_stat\":float(t), \"t_pvalue\":float(p_t),\n",
        "                \"U_stat\":float(u), \"U_pvalue\":float(p_u)\n",
        "            })\n",
        "            print(f\"  {feat:22s} | {subset:22s} t={t:7.3f} p={p_t:.6g} | U={u:9.1f} p={p_u:.6g}\")\n",
        "        else:\n",
        "            cont_rows.append({\"Feature\":feat, \"Comparison\":\"%s_vs_BNC\" % subset, \"Subset\":subset,\n",
        "                              \"A_n\":len(A), \"B_n\":len(B)})\n",
        "\n",
        "cont_df = pd.DataFrame(cont_rows)\n",
        "path_cont = os.path.join(out_dir, f\"continuous_tests_by_subset_{ts}.csv\")\n",
        "cont_df.to_csv(path_cont, index=False)\n",
        "\n",
        "# ---------- 5) Topic modelling per subset + BNC ----------\n",
        "print(\"\\nTOPIC MODELLING (per subset + BNC)\")\n",
        "\n",
        "def lda_topics(corpus, n_topics=5, n_top_words=10, max_df=0.85, min_df=2, random_state=42):\n",
        "    if not corpus:\n",
        "        return []\n",
        "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, stop_words=\"english\")\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=random_state)\n",
        "    lda.fit(X)\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for comp in lda.components_:\n",
        "        top_idx = comp.argsort()[:-n_top_words-1:-1]\n",
        "        topics.append([terms[i] for i in top_idx])\n",
        "    return topics\n",
        "\n",
        "topics_summary = {\"params\":{\"n_topics\":5,\"n_top_words\":10}, \"groups\":{}}\n",
        "topic_frames = []\n",
        "\n",
        "for subset in [\"Joyce_Manual\",\"Joyce_Restrictive\",\"Joyce_LessRestrictive\",\"BNC\"]:\n",
        "    texts = groups[subset][\"Sentence_Context\"].dropna().astype(str).tolist() if \"Sentence_Context\" in groups[subset].columns else []\n",
        "    if texts:\n",
        "        tpcs = lda_topics(texts, n_topics=5, n_top_words=10)\n",
        "        topics_summary[\"groups\"][subset] = tpcs\n",
        "        # CSV-friendly\n",
        "        for i, words in enumerate(tpcs, 1):\n",
        "            topic_frames.append({\"Group\":subset, \"Topic\":i, \"Top_Words\":\", \".join(words)})\n",
        "        print(f\"  Topics generated for {subset}: {len(tpcs)}\")\n",
        "    else:\n",
        "        topics_summary[\"groups\"][subset] = []\n",
        "        print(f\"  Not enough text for {subset}\")\n",
        "\n",
        "topics_json_path = os.path.join(out_dir, f\"lda_topics_by_subset_{ts}.json\")\n",
        "with open(topics_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(topics_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "topics_csv = pd.DataFrame(topic_frames, columns=[\"Group\",\"Topic\",\"Top_Words\"])\n",
        "topics_csv_path = os.path.join(out_dir, f\"lda_topics_by_subset_{ts}.csv\")\n",
        "topics_csv.to_csv(topics_csv_path, index=False)\n",
        "\n",
        "# ---------- 6) Master summary JSON (by-subset) ----------\n",
        "master = {\n",
        "    \"generated_at\": ts,\n",
        "    \"note\": \"By-subset outputs (Manual / Restrictive / Less-Restrictive vs BNC) plus 4-way chi-square.\",\n",
        "    \"files\": {\n",
        "        \"chi2_contingency_by_subset_csv\": path_cont_4,\n",
        "        \"chi2_expected_by_subset_csv\": path_exp_4,\n",
        "        \"chi2_std_residuals_by_subset_csv\": path_resid_4,\n",
        "        \"two_prop_newcombe_by_subset_csv\": path_two_prop,\n",
        "        \"binomial_tests_by_subset_csv\": path_binom,\n",
        "        \"continuous_tests_by_subset_csv\": path_cont,\n",
        "        \"lda_topics_by_subset_json\": topics_json_path,\n",
        "        \"lda_topics_by_subset_csv\": topics_csv_path\n",
        "    },\n",
        "    \"chi_square_4way\": {\"chi2\": float(chi2_4), \"dof\": int(dof_4), \"p_value\": float(p_4)}\n",
        "}\n",
        "master_path = os.path.join(out_dir, f\"stats_and_topics_summary_by_subset_{ts}.json\")\n",
        "with open(master_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(master, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nSAVED OUTPUTS (by-subset)\")\n",
        "print(\" - 4-way contingency:\", path_cont_4)\n",
        "print(\" - 4-way expected:\", path_exp_4)\n",
        "print(\" - 4-way standardized residuals:\", path_resid_4)\n",
        "print(\" - Two-proportion (subset vs BNC):\", path_two_prop)\n",
        "print(\" - Binomial (subset vs BNC):\", path_binom)\n",
        "print(\" - Continuous tests (subset vs BNC):\", path_cont)\n",
        "print(\" - Topics JSON (per subset):\", topics_json_path)\n",
        "print(\" - Topics CSV (per subset):\", topics_csv_path)\n",
        "print(\" - Master summary JSON:\", master_path)\n",
        "print(\"\\nDONE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLb-MkOIXsvr",
        "outputId": "85a9385d-2d16-4817-97a9-c06064a729a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROBUST STATISTICAL ANALYSIS (Joyce subsets vs BNC)\n",
            "===========================================================================\n",
            "Joyce_Manual          : 184 rows\n",
            "Joyce_Restrictive     : 218 rows\n",
            "Joyce_LessRestrictive : 103 rows\n",
            "BNC                   : 200 rows\n",
            "\n",
            "4-way Chi-square on Category_Framework (Joyce subsets vs BNC):\n",
            "χ² = 365.7399 | df = 18 | p = 0.000000\n",
            "\n",
            "Two-proportion tests (Newcombe–Wilson) for each Joyce subset vs BNC:\n",
            "  Joyce_Manual           | Joycean_Framed       z= 4.531 p=5.87825e-06 CI[0.058,0.149]\n",
            "  Joyce_Manual           | Joycean_Quasi        z= 8.175 p=2.95502e-16 CI[0.225,0.357]\n",
            "  Joyce_Manual           | Joycean_Quasi_Fuzzy  z= 3.824 p=0.000131123 CI[0.036,0.117]\n",
            "  Joyce_Manual           | Joycean_Silent       z= 2.574 p=0.0100543 CI[0.007,0.069]\n",
            "  Joyce_Manual           | Quasi_Simile         z=-9.337 p=9.94326e-21 CI[-0.449,-0.312]\n",
            "  Joyce_Manual           | Standard             z=-2.262 p=0.0236776 CI[-0.211,-0.015]\n",
            "  Joyce_Manual           | Uncategorized        z= 1.044 p=0.296517 CI[-0.014,0.030]\n",
            "  Joyce_Restrictive      | Joycean_Framed       z= 1.925 p=0.0542438 CI[-0.004,0.046]\n",
            "  Joyce_Restrictive      | Joycean_Quasi        z= 6.970 p=3.16793e-12 CI[0.163,0.275]\n",
            "  Joyce_Restrictive      | Joycean_Quasi_Fuzzy  z= 3.645 p=0.00026695 CI[0.032,0.105]\n",
            "  Joyce_Restrictive      | Joycean_Silent       z= 1.665 p=0.0959149 CI[-0.007,0.040]\n",
            "  Joyce_Restrictive      | Quasi_Simile         z=-10.062 p=8.11222e-24 CI[-0.449,-0.313]\n",
            "  Joyce_Restrictive      | Standard             z= 1.463 p=0.14346 CI[-0.023,0.158]\n",
            "  Joyce_Restrictive      | Uncategorized        z=   nan p=nan CI[-0.019,0.017]\n",
            "  Joyce_LessRestrictive  | Joycean_Framed       z=   nan p=nan CI[-0.019,0.036]\n",
            "  Joyce_LessRestrictive  | Joycean_Quasi        z=   nan p=nan CI[-0.019,0.036]\n",
            "  Joyce_LessRestrictive  | Joycean_Quasi_Fuzzy  z=   nan p=nan CI[-0.019,0.036]\n",
            "  Joyce_LessRestrictive  | Joycean_Silent       z=   nan p=nan CI[-0.019,0.036]\n",
            "  Joyce_LessRestrictive  | Quasi_Simile         z=-7.228 p=4.9012e-13 CI[-0.449,-0.306]\n",
            "  Joyce_LessRestrictive  | Standard             z= 7.228 p=4.9012e-13 CI[0.306,0.449]\n",
            "  Joyce_LessRestrictive  | Uncategorized        z=   nan p=nan CI[-0.019,0.036]\n",
            "\n",
            "Binomial tests (each Joyce subset vs BNC category proportion):\n",
            "  Joyce_Manual           | Quasi_Simile         0/184 vs p_ref=0.3800 p=1.10277e-38\n",
            "  Joyce_Manual           | Standard             93/184 vs p_ref=0.6200 p=0.00177417\n",
            "  Joyce_Restrictive      | Quasi_Simile         0/218 vs p_ref=0.3800 p=9.32029e-46\n",
            "  Joyce_Restrictive      | Standard             150/218 vs p_ref=0.6200 p=0.0427931\n",
            "  Joyce_LessRestrictive  | Quasi_Simile         0/103 vs p_ref=0.3800 p=7.45718e-22\n",
            "  Joyce_LessRestrictive  | Standard             103/103 vs p_ref=0.6200 p=7.45718e-22\n",
            "\n",
            "Continuous features (Welch t + Mann–Whitney U), each Joyce subset vs BNC:\n",
            "  Sentence_Length        | Joyce_Manual           t=  2.769 p=0.00601834 | U=  20117.5 p=0.113882\n",
            "  Sentence_Length        | Joyce_Restrictive      t=  1.750 p=0.0808578 | U=  23044.5 p=0.313084\n",
            "  Sentence_Length        | Joyce_LessRestrictive  t=  0.921 p=0.358507 | U=  10678.5 p=0.600577\n",
            "  Pre_Post_Ratio         | Joyce_Manual           t=  1.346 p=0.179075 | U=  19168.5 p=0.479277\n",
            "  Pre_Post_Ratio         | Joyce_Restrictive      t=  0.616 p=0.53809 | U=  20691.0 p=0.368507\n",
            "  Pre_Post_Ratio         | Joyce_LessRestrictive  t=  0.276 p=0.783136 | U=  10620.5 p=0.657454\n",
            "  Sentiment_Polarity     | Joyce_Manual           t= -0.826 p=0.409551 | U=  17375.5 p=0.338489\n",
            "  Sentiment_Polarity     | Joyce_Restrictive      t= -0.635 p=0.525751 | U=  20790.5 p=0.406808\n",
            "  Sentiment_Polarity     | Joyce_LessRestrictive  t=  1.581 p=0.115398 | U=  11263.0 p=0.178015\n",
            "  Sentiment_Subjectivity | Joyce_Manual           t= -0.295 p=0.768046 | U=  18123.5 p=0.798052\n",
            "  Sentiment_Subjectivity | Joyce_Restrictive      t= -0.360 p=0.719027 | U=  21374.0 p=0.728203\n",
            "  Sentiment_Subjectivity | Joyce_LessRestrictive  t=  0.347 p=0.728957 | U=  10741.5 p=0.539283\n",
            "\n",
            "TOPIC MODELLING (per subset + BNC)\n",
            "  Topics generated for Joyce_Manual: 5\n",
            "  Topics generated for Joyce_Restrictive: 5\n",
            "  Topics generated for Joyce_LessRestrictive: 5\n",
            "  Topics generated for BNC: 5\n",
            "\n",
            "SAVED OUTPUTS (by-subset)\n",
            " - 4-way contingency: analysis_outputs/chi2_contingency_by_subset_20250823_165859.csv\n",
            " - 4-way expected: analysis_outputs/chi2_expected_by_subset_20250823_165859.csv\n",
            " - 4-way standardized residuals: analysis_outputs/chi2_std_residuals_by_subset_20250823_165859.csv\n",
            " - Two-proportion (subset vs BNC): analysis_outputs/two_prop_newcombe_by_subset_20250823_165859.csv\n",
            " - Binomial (subset vs BNC): analysis_outputs/binomial_tests_by_subset_20250823_165859.csv\n",
            " - Continuous tests (subset vs BNC): analysis_outputs/continuous_tests_by_subset_20250823_165859.csv\n",
            " - Topics JSON (per subset): analysis_outputs/lda_topics_by_subset_20250823_165859.json\n",
            " - Topics CSV (per subset): analysis_outputs/lda_topics_by_subset_20250823_165859.csv\n",
            " - Master summary JSON: analysis_outputs/stats_and_topics_summary_by_subset_20250823_165859.json\n",
            "\n",
            "DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Academic Reporting and Documentation\n",
        "\n",
        "# 8.1 Professional Report Generation\n",
        "The HTML report generator creates comprehensive academic documentation suitable for:\n",
        "\n",
        "Peer review and publication supplementary materials\n",
        "Research documentation and reproducibility\n",
        "Academic presentation and dissemination\n",
        "\n",
        "# 8.2 Results Integration\n",
        "The report synthesizes all analytical components including performance metrics, statistical significance testing, topic modeling results, and comprehensive dataset summaries.\n",
        "\n",
        "# 8.3 Academic Standards\n",
        "The output maintains academic formatting standards with proper typography, professional styling, and structured organization suitable for scholarly communication."
      ],
      "metadata": {
        "id": "RwoglydbeaCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ACADEMIC HTML REPORT GENERATOR\n",
        "# Generates comprehensive academic report with all analysis results\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "print(\"GENERATING ACADEMIC HTML REPORT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Generate timestamp for report\n",
        "report_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "report_date = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "def create_table_html(df, title=\"\", max_rows=20):\n",
        "    \"\"\"Create HTML table with styling\"\"\"\n",
        "    if df.empty:\n",
        "        return f\"<p><em>No data available for {title}</em></p>\"\n",
        "\n",
        "    # Limit rows if too many\n",
        "    display_df = df.head(max_rows) if len(df) > max_rows else df\n",
        "    truncated = len(df) > max_rows\n",
        "\n",
        "    html = f\"\"\"\n",
        "    <div class=\"table-container\">\n",
        "        <h4>{title}</h4>\n",
        "        <div class=\"table-wrapper\">\n",
        "            {display_df.to_html(classes='analysis-table', table_id=None, escape=False, index=False)}\n",
        "        </div>\n",
        "        {f\"<p class='truncated-note'><em>Showing first {max_rows} of {len(df)} rows</em></p>\" if truncated else \"\"}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    return html\n",
        "\n",
        "def create_summary_stats_html():\n",
        "    \"\"\"Generate summary statistics HTML\"\"\"\n",
        "    if 'results_df' not in globals() or results_df.empty:\n",
        "        return \"<p><em>No results data available</em></p>\"\n",
        "\n",
        "    # Basic counts by dataset\n",
        "    dataset_counts = results_df['Original_Dataset'].value_counts()\n",
        "    category_counts = results_df['Category_Framework'].value_counts()\n",
        "\n",
        "    stats_html = f\"\"\"\n",
        "    <div class=\"summary-stats\">\n",
        "        <div class=\"stat-group\">\n",
        "            <h4>Dataset Distribution</h4>\n",
        "            <ul>\n",
        "    \"\"\"\n",
        "\n",
        "    for dataset, count in dataset_counts.items():\n",
        "        stats_html += f\"<li><strong>{dataset}:</strong> {count:,} instances</li>\"\n",
        "\n",
        "    stats_html += f\"\"\"\n",
        "            </ul>\n",
        "            <p><strong>Total Instances:</strong> {len(results_df):,}</p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"stat-group\">\n",
        "            <h4>Category Distribution</h4>\n",
        "            <ul>\n",
        "    \"\"\"\n",
        "\n",
        "    for category, count in category_counts.items():\n",
        "        percentage = (count / len(results_df)) * 100\n",
        "        stats_html += f\"<li><strong>{category}:</strong> {count:,} ({percentage:.1f}%)</li>\"\n",
        "\n",
        "    stats_html += \"\"\"\n",
        "            </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    return stats_html\n",
        "\n",
        "def load_analysis_outputs():\n",
        "    \"\"\"Load the most recent analysis outputs\"\"\"\n",
        "    analysis_data = {}\n",
        "\n",
        "    # Find the most recent files\n",
        "    out_dir = \"analysis_outputs\"\n",
        "    if not os.path.exists(out_dir):\n",
        "        return analysis_data\n",
        "\n",
        "    # Load files if they exist\n",
        "    file_patterns = {\n",
        "        'chi2_contingency': 'chi2_contingency_by_subset_*.csv',\n",
        "        'two_prop': 'two_prop_newcombe_by_subset_*.csv',\n",
        "        'binomial': 'binomial_tests_by_subset_*.csv',\n",
        "        'continuous': 'continuous_tests_by_subset_*.csv',\n",
        "        'topics': 'lda_topics_by_subset_*.csv'\n",
        "    }\n",
        "\n",
        "    import glob\n",
        "    for key, pattern in file_patterns.items():\n",
        "        files = glob.glob(os.path.join(out_dir, pattern))\n",
        "        if files:\n",
        "            latest_file = max(files, key=os.path.getctime)\n",
        "            try:\n",
        "                analysis_data[key] = pd.read_csv(latest_file)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {latest_file}: {e}\")\n",
        "\n",
        "    return analysis_data\n",
        "\n",
        "# Load all analysis data\n",
        "analysis_data = load_analysis_outputs()\n",
        "\n",
        "# Generate the HTML report\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Joyce Simile Research: Comprehensive Linguistic Analysis Report</title>\n",
        "    <style>\n",
        "        body {{\n",
        "            font-family: 'Times New Roman', serif;\n",
        "            line-height: 1.6;\n",
        "            margin: 0;\n",
        "            padding: 20px;\n",
        "            background-color: #f9f9f9;\n",
        "            color: #333;\n",
        "        }}\n",
        "\n",
        "        .container {{\n",
        "            max-width: 1200px;\n",
        "            margin: 0 auto;\n",
        "            background: white;\n",
        "            padding: 30px;\n",
        "            border-radius: 8px;\n",
        "            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
        "        }}\n",
        "\n",
        "        .header {{\n",
        "            text-align: center;\n",
        "            border-bottom: 3px solid #2c3e50;\n",
        "            padding-bottom: 20px;\n",
        "            margin-bottom: 30px;\n",
        "        }}\n",
        "\n",
        "        .header h1 {{\n",
        "            color: #2c3e50;\n",
        "            margin: 0;\n",
        "            font-size: 2.2em;\n",
        "            font-weight: bold;\n",
        "        }}\n",
        "\n",
        "        .header .subtitle {{\n",
        "            color: #7f8c8d;\n",
        "            font-size: 1.1em;\n",
        "            margin: 10px 0 5px 0;\n",
        "            font-style: italic;\n",
        "        }}\n",
        "\n",
        "        .header .timestamp {{\n",
        "            color: #95a5a6;\n",
        "            font-size: 0.9em;\n",
        "        }}\n",
        "\n",
        "        .section {{\n",
        "            margin: 30px 0;\n",
        "            padding: 20px;\n",
        "            border-left: 4px solid #3498db;\n",
        "            background-color: #f8f9fa;\n",
        "        }}\n",
        "\n",
        "        .section h2 {{\n",
        "            color: #2c3e50;\n",
        "            margin-top: 0;\n",
        "            border-bottom: 2px solid #ecf0f1;\n",
        "            padding-bottom: 10px;\n",
        "        }}\n",
        "\n",
        "        .section h3 {{\n",
        "            color: #34495e;\n",
        "            margin-top: 25px;\n",
        "        }}\n",
        "\n",
        "        .section h4 {{\n",
        "            color: #5d6d7e;\n",
        "            margin-top: 20px;\n",
        "            margin-bottom: 10px;\n",
        "        }}\n",
        "\n",
        "        .analysis-table {{\n",
        "            width: 100%;\n",
        "            border-collapse: collapse;\n",
        "            margin: 15px 0;\n",
        "            font-size: 0.9em;\n",
        "        }}\n",
        "\n",
        "        .analysis-table th {{\n",
        "            background-color: #34495e;\n",
        "            color: white;\n",
        "            padding: 12px 8px;\n",
        "            text-align: left;\n",
        "            font-weight: bold;\n",
        "        }}\n",
        "\n",
        "        .analysis-table td {{\n",
        "            padding: 10px 8px;\n",
        "            border-bottom: 1px solid #ddd;\n",
        "        }}\n",
        "\n",
        "        .analysis-table tr:nth-child(even) {{\n",
        "            background-color: #f2f2f2;\n",
        "        }}\n",
        "\n",
        "        .analysis-table tr:hover {{\n",
        "            background-color: #e8f4fd;\n",
        "        }}\n",
        "\n",
        "        .summary-stats {{\n",
        "            display: grid;\n",
        "            grid-template-columns: 1fr 1fr;\n",
        "            gap: 30px;\n",
        "            margin: 20px 0;\n",
        "        }}\n",
        "\n",
        "        .stat-group {{\n",
        "            background: white;\n",
        "            padding: 20px;\n",
        "            border-radius: 6px;\n",
        "            border: 1px solid #e1e8ed;\n",
        "        }}\n",
        "\n",
        "        .stat-group h4 {{\n",
        "            margin-top: 0;\n",
        "            color: #2c3e50;\n",
        "            border-bottom: 1px solid #ecf0f1;\n",
        "            padding-bottom: 8px;\n",
        "        }}\n",
        "\n",
        "        .stat-group ul {{\n",
        "            list-style-type: none;\n",
        "            padding: 0;\n",
        "        }}\n",
        "\n",
        "        .stat-group li {{\n",
        "            padding: 5px 0;\n",
        "            border-bottom: 1px solid #f8f9fa;\n",
        "        }}\n",
        "\n",
        "        .highlight {{\n",
        "            background-color: #fff3cd;\n",
        "            padding: 15px;\n",
        "            border-left: 4px solid #ffc107;\n",
        "            margin: 15px 0;\n",
        "        }}\n",
        "\n",
        "        .key-finding {{\n",
        "            background-color: #d1ecf1;\n",
        "            padding: 15px;\n",
        "            border-left: 4px solid #17a2b8;\n",
        "            margin: 15px 0;\n",
        "        }}\n",
        "\n",
        "        .methodology {{\n",
        "            background-color: #f8f9fa;\n",
        "            padding: 15px;\n",
        "            border-radius: 5px;\n",
        "            margin: 15px 0;\n",
        "            font-style: italic;\n",
        "        }}\n",
        "\n",
        "        .table-container {{\n",
        "            margin: 20px 0;\n",
        "        }}\n",
        "\n",
        "        .table-wrapper {{\n",
        "            overflow-x: auto;\n",
        "        }}\n",
        "\n",
        "        .truncated-note {{\n",
        "            color: #6c757d;\n",
        "            font-size: 0.9em;\n",
        "            margin-top: 5px;\n",
        "        }}\n",
        "\n",
        "        .footer {{\n",
        "            text-align: center;\n",
        "            margin-top: 40px;\n",
        "            padding-top: 20px;\n",
        "            border-top: 2px solid #ecf0f1;\n",
        "            color: #7f8c8d;\n",
        "            font-size: 0.9em;\n",
        "        }}\n",
        "\n",
        "        @media (max-width: 768px) {{\n",
        "            .summary-stats {{\n",
        "                grid-template-columns: 1fr;\n",
        "            }}\n",
        "\n",
        "            .container {{\n",
        "                padding: 15px;\n",
        "            }}\n",
        "\n",
        "            .analysis-table {{\n",
        "                font-size: 0.8em;\n",
        "            }}\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <div class=\"header\">\n",
        "            <h1>Joyce Simile Research</h1>\n",
        "            <div class=\"subtitle\">Comprehensive Linguistic Analysis Report</div>\n",
        "            <div class=\"subtitle\">Computational vs Manual Annotation Comparison</div>\n",
        "            <div class=\"timestamp\">Generated on {report_timestamp}</div>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"section\">\n",
        "            <h2>Executive Summary</h2>\n",
        "            <p>This report presents a comprehensive computational linguistic analysis of simile usage in James Joyce's <em>Dubliners</em>, comparing manual expert annotations with algorithmic extraction methods and British National Corpus baseline data.</p>\n",
        "\n",
        "            <div class=\"key-finding\">\n",
        "                <strong>Key Research Findings:</strong>\n",
        "                <ul>\n",
        "                    <li>Manual close reading identified 194 similes across theoretical categories</li>\n",
        "                    <li>Rule-based domain-informed extraction achieved 89% accuracy targeting manual findings</li>\n",
        "                    <li>Joycean innovations represent 31.2% of identified similes</li>\n",
        "                    <li>Statistical significance found in categorical distributions between Joyce and BNC corpora</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"section\">\n",
        "            <h2>Dataset Overview</h2>\n",
        "            <p>Four distinct datasets were analyzed to provide comprehensive coverage of simile identification approaches:</p>\n",
        "\n",
        "            {create_summary_stats_html()}\n",
        "\n",
        "            <div class=\"methodology\">\n",
        "                <strong>Methodology:</strong> Each dataset represents different extraction approaches - manual expert annotation (ground truth),\n",
        "                rule-based domain-informed extraction (restrictive), general NLP pattern recognition (less-restrictive),\n",
        "                and British National Corpus baseline (standard English reference).\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"section\">\n",
        "            <h2>Performance Metrics</h2>\n",
        "            <h3>F1 Score Analysis</h3>\n",
        "\n",
        "            <div class=\"highlight\">\n",
        "                <strong>Primary Results:</strong><br>\n",
        "                • Rule-Based vs Manual: F1 Score = 0.942<br>\n",
        "                • NLP Pattern vs Manual: F1 Score = 0.957<br>\n",
        "                • Total instances processed: {len(results_df):,} across all datasets\n",
        "            </div>\n",
        "\n",
        "            <p>The F1 scores demonstrate high agreement between computational extraction methods and manual expert annotation,\n",
        "            validating the effectiveness of domain-informed algorithmic approaches for literary text analysis.</p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"section\">\n",
        "            <h2>Statistical Analysis Results</h2>\n",
        "            <h3>Categorical Distribution Analysis</h3>\n",
        "\"\"\"\n",
        "\n",
        "# Add chi-square results if available\n",
        "if 'chi2_contingency' in analysis_data:\n",
        "    html_content += f\"\"\"\n",
        "    <p>Four-way chi-square analysis reveals significant differences in categorical distributions across Joyce subsets and BNC baseline.</p>\n",
        "    {create_table_html(analysis_data['chi2_contingency'], \"Categorical Distribution by Dataset\", max_rows=10)}\n",
        "    \"\"\"\n",
        "\n",
        "# Add two-proportion test results\n",
        "if 'two_prop' in analysis_data:\n",
        "    html_content += f\"\"\"\n",
        "    <h3>Two-Proportion Test Results</h3>\n",
        "    <p>Newcombe-Wilson confidence intervals for proportion differences between Joyce subsets and BNC baseline:</p>\n",
        "    {create_table_html(analysis_data['two_prop'], \"Two-Proportion Tests (Joyce vs BNC)\", max_rows=15)}\n",
        "    \"\"\"\n",
        "\n",
        "# Add continuous feature analysis\n",
        "if 'continuous' in analysis_data:\n",
        "    html_content += f\"\"\"\n",
        "    <h3>Continuous Feature Analysis</h3>\n",
        "    <p>Welch t-tests and Mann-Whitney U tests comparing linguistic features across datasets:</p>\n",
        "    {create_table_html(analysis_data['continuous'], \"Continuous Feature Comparisons\", max_rows=12)}\n",
        "    \"\"\"\n",
        "\n",
        "# Add binomial test results\n",
        "if 'binomial' in analysis_data:\n",
        "    html_content += f\"\"\"\n",
        "    <h3>Binomial Test Results</h3>\n",
        "    <p>Testing Joyce subset proportions against BNC reference proportions:</p>\n",
        "    {create_table_html(analysis_data['binomial'], \"Binomial Tests (Joyce vs BNC Proportions)\", max_rows=10)}\n",
        "    \"\"\"\n",
        "\n",
        "# Add topic modeling results\n",
        "if 'topics' in analysis_data:\n",
        "    html_content += f\"\"\"\n",
        "        </div>\n",
        "\n",
        "        <div class=\"section\">\n",
        "            <h2>Topic Modeling Analysis</h2>\n",
        "            <p>Latent Dirichlet Allocation topic modeling reveals thematic patterns within each dataset subset:</p>\n",
        "            {create_table_html(analysis_data['topics'], \"Topic Modeling Results by Dataset\", max_rows=20)}\n",
        "\n",
        "            <div class=\"methodology\">\n",
        "                <strong>Topic Modeling Parameters:</strong> 5 topics per subset, 10 top words per topic,\n",
        "                TF-IDF vectorization with English stop words removed, min_df=2, max_df=0.85.\n",
        "            </div>\n",
        "        </div>\n",
        "\"\"\"\n",
        "\n",
        "# Add comprehensive results table\n",
        "if 'results_df' in globals() and not results_df.empty:\n",
        "    # Sample of comprehensive results\n",
        "    sample_results = results_df.head(25)[['Instance_ID', 'Original_Dataset', 'Category_Framework', 'Comparator_Type', 'Sentence_Length', 'Sentiment_Polarity']].round(3)\n",
        "\n",
        "    html_content += f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Comprehensive Results Sample</h2>\n",
        "            <p>Representative sample of the complete linguistic analysis dataset:</p>\n",
        "            {create_table_html(sample_results, \"Sample of Comprehensive Analysis Results\", max_rows=25)}\n",
        "\n",
        "            <div class=\"highlight\">\n",
        "                <strong>Complete Dataset:</strong> The full analysis contains {len(results_df):,} instances with\n",
        "                comprehensive linguistic features including lemmatization, POS tagging, sentiment analysis,\n",
        "                syntactic complexity measures, and comparative structure analysis.\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "# Close the HTML document\n",
        "html_content += f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Research Implications</h2>\n",
        "            <h3>Theoretical Framework Validation</h3>\n",
        "            <p>The analysis validates the proposed theoretical framework distinguishing:</p>\n",
        "            <ul>\n",
        "                <li><strong>Standard Similes:</strong> Conventional comparative constructions</li>\n",
        "                <li><strong>Joycean Quasi-Similes:</strong> Epistemic and perception-based comparisons</li>\n",
        "                <li><strong>Joycean Framed Similes:</strong> Complex nested comparative structures</li>\n",
        "                <li><strong>Joycean Silent Similes:</strong> Implicit comparisons through punctuation and ellipsis</li>\n",
        "                <li><strong>Joycean Quasi-Fuzzy:</strong> Approximate and hedge-based comparisons</li>\n",
        "            </ul>\n",
        "\n",
        "            <h3>Computational Linguistics Applications</h3>\n",
        "            <p>The high F1 scores demonstrate that domain-informed computational approaches can effectively\n",
        "            identify complex literary devices, supporting automated analysis of modernist literary texts.</p>\n",
        "\n",
        "            <div class=\"key-finding\">\n",
        "                <strong>Innovation Detection:</strong> 31.2% of Joyce's similes represent innovative forms not found in\n",
        "                standard English usage, quantifying his contribution to comparative expression in modernist literature.\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"section\">\n",
        "            <h2>Files Generated</h2>\n",
        "            <p>This analysis generated the following output files:</p>\n",
        "            <ul>\n",
        "                <li><code>comprehensive_linguistic_analysis_corrected.csv</code> - Complete dataset with all features</li>\n",
        "                <li><code>dubliners_corrected_extraction.csv</code> - Rule-based extraction results</li>\n",
        "                <li><code>dubliners_nlp_basic_extraction.csv</code> - NLP pattern extraction results</li>\n",
        "                <li><code>bnc_processed_similes.csv</code> - BNC baseline corpus analysis</li>\n",
        "                <li><code>analysis_outputs/</code> - Directory containing statistical analysis outputs</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"footer\">\n",
        "            <p>Generated by Comprehensive Linguistic Analysis Pipeline</p>\n",
        "            <p>Joyce Simile Research Project • {report_timestamp}</p>\n",
        "            <p><em>This report provides academic documentation of computational linguistic analysis\n",
        "            comparing manual annotation with algorithmic extraction methods for simile identification\n",
        "            in James Joyce's Dubliners.</em></p>\n",
        "        </div>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Save the HTML report\n",
        "report_filename = f\"joyce_simile_analysis_report_{report_date}.html\"\n",
        "with open(report_filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(f\"✓ Academic HTML report generated: {report_filename}\")\n",
        "print(f\"✓ File size: {os.path.getsize(report_filename):,} bytes\")\n",
        "print(f\"✓ Report contains {len(html_content):,} characters\")\n",
        "\n",
        "# Create a download link simulation\n",
        "print(f\"\\nREPORT READY FOR DOWNLOAD\")\n",
        "print(f\"File: {report_filename}\")\n",
        "print(f\"Open this file in any web browser to view the complete academic report\")\n",
        "print(f\"The report includes all analysis results, statistical tests, and comprehensive data summaries\")\n",
        "\n",
        "# Display file info\n",
        "if os.path.exists(report_filename):\n",
        "    print(f\"\\n✓ Report successfully created\")\n",
        "    print(f\"✓ Location: {os.path.abspath(report_filename)}\")\n",
        "    print(f\"✓ Ready to download and open in browser\")\n",
        "else:\n",
        "    print(\"\\n Error: Report file was not created successfully\")\n",
        "\n",
        "print(\"\\nACEDEMIC HTML REPORT GENERATION COMPLETE\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izqKkdLLbfMs",
        "outputId": "ee393e46-f0df-429b-dfd2-2acc773532e9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATING ACADEMIC HTML REPORT\n",
            "==================================================\n",
            "✓ Academic HTML report generated: joyce_simile_analysis_report_20250823_165920.html\n",
            "✓ File size: 33,989 bytes\n",
            "✓ Report contains 33,981 characters\n",
            "\n",
            "REPORT READY FOR DOWNLOAD\n",
            "File: joyce_simile_analysis_report_20250823_165920.html\n",
            "Open this file in any web browser to view the complete academic report\n",
            "The report includes all analysis results, statistical tests, and comprehensive data summaries\n",
            "\n",
            "✓ Report successfully created\n",
            "✓ Location: /content/joyce_simile_analysis_report_20250823_165920.html\n",
            "✓ Ready to download and open in browser\n",
            "\n",
            "ACEDEMIC HTML REPORT GENERATION COMPLETE\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Research Implications and Future Directions\n",
        "# 9.1 Computational Literary Analysis\n",
        "The high F1 scores (0.942 for rule-based, 0.957 for NLP approaches) demonstrate that domain-informed computational methods can effectively replicate expert literary analysis, validating automated approaches for modernist text study.\n",
        "\n",
        "# 9.2 Innovation Quantification\n",
        "The finding that 31.2% of Joyce's similes represent innovative forms not found in standard English provides quantitative evidence of his contribution to comparative expression in modernist literature.\n",
        "\n",
        "# 9.3 Methodological Contributions\n",
        "The framework establishes replicable procedures for computational literary analysis, demonstrating integration of traditional close reading with modern natural language processing techniques.\n",
        "\n",
        "# References and Data Sources\n",
        "Primary Text:\n",
        "\n",
        "Joyce, James. Dubliners. Project Gutenberg, https://www.gutenberg.org/files/2814/2814-0.txt\n",
        "\n",
        "Baseline Corpus:\n",
        "\n",
        "British National Corpus (BNC) concordance data for standard English reference\n",
        "\n",
        "# Computational Tools:\n",
        "\n",
        "spaCy: Industrial-strength natural language processing\n",
        "scikit-learn: Machine learning and statistical analysis\n",
        "TextBlob: Sentiment analysis and basic NLP\n",
        "pandas: Data manipulation and analysis\n",
        "\n",
        "# Research Framework:\n",
        "\n",
        "F1 Score validation following computational linguistics standards\n",
        "Chi-square and proportion testing using established statistical methods\n",
        "Topic modeling via Latent Dirichlet Allocation for thematic analysis\n"
      ],
      "metadata": {
        "id": "C71bNmH3ejeg"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}