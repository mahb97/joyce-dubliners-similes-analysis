{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/joyce-dubliners-similes-analysis/blob/main/01_data_processing_linguistic_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c68DDLz_cncb"
      },
      "source": [
        "# Computational Analysis of Simile Structures in Joyce's Dubliners and the BNC\n",
        "## Notebook 1: Data Processing and preparation for Linguistic Analysis\n",
        "\n",
        "**As part of the MA Dissertion in Digital Humanities at UCL, 2025**\n",
        "\n",
        "From the Research Dialogue *Language Games* in HOLO 3, text by Nora N Khan & Peli Grietzer: *\"To spend time obsessing over language, whether written, inscribed, coded, encoded, or spoken, is to feel acutely aware of its limits: the places that language fails, the territories of experiences it outlines. The way acceptable language shifts with one’s time. To spend time in computational spaces is to run into constant frustration with languages elisions, its elusiveness,  its – for lack of better words – ineffable qualities, its indeterminacy. That pesky language is ever expressed in relation, tied to experience becoming real, fixed for only a moment when printed. As authors have written here, computation and code have fairly low tolerance for elisions and variance in meaning, for what can be changed on a dime. Language always gestures to places beyond it, of spiritual, transcendent, mystic experiences, even as it makes the world. (...) Through a range of gestures, coded movements, still we manage to communicate outside of dominant symbolic language systems. Perhaps it must be put into words, to acknowledge that we routinely communicate through sounds, drawing, touch, embodied expression; ideas are communicated through architecture and the design of public and private spaces, driven by unnamed agendas. Our ideas also circulate through digital artefacts, through ghosts and ether, through performance and gestures and **half-smiles**. Language has to work for us, tasked with helping our species survive, perpetuate itself.\"*\n",
        "\n",
        "This notebook implements the data processing pipeline for computational analysis of simile structures across four datasets: manual annotations, computational extractions, and BNC baseline corpus. The analysis extends the theoretical framework of Leech & Short (1981) on Quasi-Similes and frames these as discoverable in Joyce when reading is carried out with sympathy, as proposed by Tanja Vesala-Varttala (1999).\n",
        "\n",
        "NLP is \"focused on the desig and analysis of computational algorithms and representations for processing natural language\" whereby its goal \"is to provide new computational capabilities around human language: for example, extracting informatio from texts, translatig between languages, answering questions, holding a conversation, taking instructions and so on\" (Introduction to Natural Language Processing, page 1-2).\n",
        "\n",
        "Zipf's law is important here: \"there will be a few words that are very frequent, and a long trail of words that are rare. A consequence is that Natural Language Processing algorithms must be especially robust to observations that do not occur in the training data\" (ibid).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "(insert tanja quote)\n",
        "\n",
        "Insert Citations - Tanja, Leech and Short\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3_ak65Dcncc"
      },
      "source": [
        "## Colab Setup and GitHub Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpDFzRkkcnce"
      },
      "source": [
        "## Upload Instructions\n",
        "\n",
        "**To complete the setup:**\n",
        "\n",
        "1. **Upload your 2 CSV files** to this Colab environment or your GitHub repository:\n",
        "   - `All Similes  Dubliners cont.csv` (your manual annotations)\n",
        "   - `concordance from BNC.csv` (BNC baseline)\n",
        "   \n",
        "2. Run the cells below to verify the files are loaded correctly.\n",
        "\n",
        "3. **Run the full processing pipeline** once both files are available\n",
        "\n",
        "The notebook will automatically:\n",
        "- Download Dubliners from Project Gutenberg\n",
        "- Run computational simile extraction\n",
        "- Export processed datasets for Notebook 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab file upload setup\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import os\n",
        "\n",
        "    print(\"Running in Google Colab\")\n",
        "    print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "    # Checks if required files already exist\n",
        "    required_files = [\n",
        "        'All Similes - Dubliners cont.csv', # Manual annotations\n",
        "        'concordance from BNC.csv'                 # BNC baseline data\n",
        "    ]\n",
        "\n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "    if missing_files:\n",
        "        print(\"\\nRequired data files not found. Please upload the following files:\")\n",
        "        for file in missing_files:\n",
        "            print(f\"  - {file}\")\n",
        "        print(\"\\nRun the next cell to upload your files.\")\n",
        "    else:\n",
        "        print(\"\\nAll required files found:\")\n",
        "        for file in required_files:\n",
        "            print(f\"  FOUND: {file}\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab\")\n",
        "    print(\"Please ensure your CSV files are in the current directory\")\n",
        "\n",
        "    import os\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "    print(\"Files in directory:\")\n",
        "    for file in os.listdir('.'):\n",
        "        if file.endswith('.csv'):\n",
        "            print(f\"  {file}\")"
      ],
      "metadata": {
        "id": "dMrs6njrdDqx",
        "outputId": "7653a99c-d907-4b12-cfe2-c5b16ba72ba5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Current working directory: /content\n",
            "\n",
            "All required files found:\n",
            "  FOUND: All Similes - Dubliners cont.csv\n",
            "  FOUND: concordance from BNC.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File upload cell - only run the below two cells if files are missing"
      ],
      "metadata": {
        "id": "JfB8CtGiyzIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File upload cell - run this if files are missing\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import os\n",
        "\n",
        "    print(\"Click 'Choose Files' to upload your CSV files\")\n",
        "    print(\"Upload both: manual annotations + BNC concordance\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    print(\"\\nFiles uploaded successfully:\")\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"  {filename} ({len(uploaded[filename])} bytes)\")\n",
        "\n",
        "    print(\"\\nRerun the verification cell below to check files\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Not in Colab - please place CSV files in current directory\")"
      ],
      "metadata": {
        "id": "lovacZAXdEpt",
        "outputId": "2d10d4aa-d8da-4592-ae82-e83ab7deac32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click 'Choose Files' to upload your CSV files\n",
            "Upload both: manual annotations + BNC concordance\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd59d652-40c1-4584-ae31-beba59370295\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bd59d652-40c1-4584-ae31-beba59370295\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Files uploaded successfully:\n",
            "\n",
            "Rerun the verification cell below to check files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify input data files are available - run this if the manual upload cell was needed\n",
        "import os\n",
        "\n",
        "required_input_files = [\n",
        "    'All Similes - Dubliners cont copy.csv',  # Manual annotations\n",
        "    'concordance from BNC.csv'                     # BNC baseline data\n",
        "]\n",
        "\n",
        "missing_files = []\n",
        "for file in required_input_files:\n",
        "    if not os.path.exists(file):\n",
        "        missing_files.append(file)\n",
        "\n",
        "if missing_files:\n",
        "    print(\"WARNING: Missing required input data files:\")\n",
        "    for file in missing_files:\n",
        "        print(f\"  MISSING: {file}\")\n",
        "    print(\"\\nPlease use the file upload cell above to upload these files:\")\n",
        "    print(\"  1. Your manual annotations CSV\")\n",
        "    print(\"  2. Your BNC concordance CSV\")\n",
        "    print(\"\\nThe third dataset (computational extractions) will be generated by this notebook.\")\n",
        "else:\n",
        "    print(\"All required input files found\")\n",
        "    print(\"  FOUND: Manual annotations ready\")\n",
        "    print(\"  FOUND: BNC baseline data ready\")\n",
        "    print(\"  GENERATE: Computational extractions will be created\")"
      ],
      "metadata": {
        "id": "veNaEPZSdfY9",
        "outputId": "6fedc3bf-0982-4f50-8fec-c52824cd9a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Missing required input data files:\n",
            "  MISSING: All Similes - Dubliners cont(Sheet1).csv\n",
            "\n",
            "Please use the file upload cell above to upload these files:\n",
            "  1. Your manual annotations CSV\n",
            "  2. Your BNC concordance CSV\n",
            "\n",
            "The third dataset (computational extractions) will be generated by this notebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing required packages\n"
      ],
      "metadata": {
        "id": "cXYyhuJay6fM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4u6xM-8Ecncd",
        "outputId": "c366c5fc-7d75-4a04-a8e1-82357ba30ad4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Current working directory: /content\n",
            "\n",
            "Project files:\n",
            "  ✓ concordance from BNC.csv\n",
            "  ✓ All Similes - Dubliners cont.csv\n"
          ]
        }
      ],
      "source": [
        "# Installs required packages\n",
        "!pip install spacy textblob scikit-learn -q\n",
        "!python -m spacy download en_core_web_lg -q\n",
        "\n",
        "# Verifies the file structure\n",
        "import os\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"\\nProject files:\")\n",
        "for file in os.listdir('.'):\n",
        "    if file.endswith(('.csv', '.py', '.ipynb')):\n",
        "        print(f\"  ✓ {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. Introduction and Research Objectives\n",
        "\n",
        "1.1 Research Questions\n",
        "\n",
        "How effectively can computational methods replicate manual expert identification of literary similes?What linguistic innovations distinguish Joycean similes from standard English usage patterns?\n",
        "\n",
        "How do different extraction approaches (rule-based vs. pattern recognition) perform against ground truth annotations?\n",
        "# 1.2 Theoretical Framework\n",
        "\n",
        "The analysis employs a novel categorical framework\n",
        "\n",
        "distinguishing:Standard Similes: Conventional comparative\n",
        "\n",
        "*   constructionsJoycean Quasi-Similes: Epistemic and perception-based\n",
        "*   structuresJoycean Silent Similes: Implicit comparisons through\n",
        "*   comparisonsJoycean Framed Similes: Complex nested comparative\n",
        "*  punctuationJoycean Quasi-Fuzzy: Approximate and hedge-based comparisons\n",
        "\n"
      ],
      "metadata": {
        "id": "iwawth-01WsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Collection and Preprocessing\n",
        "# 1 Corpus Selection\n",
        "The study employs four distinct datasets providing comprehensive methodological coverage:\n",
        "\n",
        "Manual Expert Annotations (Ground Truth): Close reading identification of 194 similes\n",
        "Rule-Based Domain-Informed Extraction: Restrictive algorithmic targeting of manual findings\n",
        "NLP Pattern Recognition: Less-restrictive computational extraction from Project Gutenberg text\n",
        "British National Corpus Baseline: Standard English reference corpus (200 instances)\n",
        "\n",
        "# 2.2 Methodological Approach\n",
        "Following established corpus linguistic principles, the analysis implements both quantitative and qualitative assessment methods to ensure robust validation of computational approaches against human expert annotation."
      ],
      "metadata": {
        "id": "GT24E2Ba0sXr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzA3LRnucncd"
      },
      "source": [
        "# Finding similes in Dubliners : basic pattern approach"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Comparative Methodology: NLP Pattern Recognition\n",
        "# 4.1 Less-Restrictive Approach\n",
        "To establish methodological comparison, this extraction pipeline implements general natural language processing patterns targeting all potential simile constructions without domain-specific constraints.\n",
        "\n",
        "# 4.2 Linguistic Feature Analysis\n",
        "This approach incorporates comprehensive linguistic analysis including:\n",
        "\n",
        "Lemmatization and POS tagging using spaCy\n",
        "Sentiment analysis via TextBlob\n",
        "Topic modeling using Latent Dirichlet Allocation\n",
        "Pre/post-comparator token analysis for structural assessment\n",
        "\n",
        "# 4.3 Research Significance\n",
        "The comparison between restrictive domain-informed and general pattern recognition approaches provides insight into the specificity requirements for literary computational analysis."
      ],
      "metadata": {
        "id": "AlPqGCdU2rBN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "12f4a96d",
        "outputId": "1023270b-9e49-4841-a563-fd7ae9a1e3a4"
      },
      "source": [
        "# =============================================================================\n",
        "# LESS RESTRICTIVE NLP SIMILE EXTRACTION\n",
        "# Target: Find all instances of 'like', 'as if', and 'as...as' in Dubliners\n",
        "# Purpose: Generate a dataset for comparison with the rule-based extraction\n",
        "# =============================================================================\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"LESS RESTRICTIVE NLP SIMILE EXTRACTION\")\n",
        "print(\"Targeting all 'like', 'as if', 'as...as', and other potential comparative instances.\")\n",
        "print(\"Includes basic linguistic analysis (lemmatization, POS, sentiment, topic)\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "# Initialize spaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy natural language processing pipeline loaded successfully\")\n",
        "except OSError:\n",
        "    print(\"Warning: spaCy English model not found. Install with: python -m spacy download en_core_web_sm\")\n",
        "    nlp = None\n",
        "\n",
        "\n",
        "def load_dubliners_text():\n",
        "    \"\"\"Load Dubliners text from Project Gutenberg.\"\"\"\n",
        "    url = \"https://www.gutenberg.org/files/2814/2814-0.txt\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        text = response.text\n",
        "\n",
        "        # Clean metadata\n",
        "        start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "        end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "        if start_marker in text:\n",
        "            text = text.split(start_marker)[1]\n",
        "        if end_marker in text:\n",
        "            text = text.split(end_marker)[0]\n",
        "\n",
        "        print(f\"Downloaded {len(text):,} characters from Project Gutenberg\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading text: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_similes_nlp_basic(text):\n",
        "    \"\"\"\n",
        "    Extract similes using basic NLP patterns ('like', 'as if', 'as...as', etc.).\n",
        "    This version is intentionally less restrictive to find a broader set of potential similes.\n",
        "    Performs lemmatization, POS tagging, and sentiment analysis.\n",
        "    \"\"\"\n",
        "    if nlp is None:\n",
        "        print(\"spaCy not loaded. Cannot perform detailed NLP analysis.\")\n",
        "        # Fallback to regex-based sentence splitting if spaCy is not available\n",
        "        sentences = [s.strip() for s in re.split(r'(?<!Mr)(?<!Mrs)(?<!Dr)[.!?]+', text) if len(s.strip()) > 10]\n",
        "    else:\n",
        "        # Use spaCy's sentence segmentation\n",
        "        doc = nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
        "\n",
        "\n",
        "    basic_similes = []\n",
        "    simile_id = 1\n",
        "\n",
        "    print(\"Extracting similes with less restrictive NLP patterns...\")\n",
        "\n",
        "    # Definitions of the patterns that are being searched for\n",
        "    inclusive_patterns = [\n",
        "        r'\\b(like)\\b',         # 'like' as a potential comparator\n",
        "        r'\\b(as if)\\b',        # 'as if'\n",
        "        r'\\b(as .*? as)\\b',    # 'as ... as' (non-greedy)\n",
        "        r'\\b(than)\\b',         # 'than' for comparison\n",
        "        r'\\b(similar to)\\b',   # 'similar to'\n",
        "        r'\\b(resembled?)\\b',   # 'resembled' or 'resembles'\n",
        "        r'\\b(seem|seems|seemed)\\b', # Seem/appears for quasi-similes\n",
        "        r'\\b(appear|appears|appeared)\\b',\n",
        "        r'\\b(such as)\\b'       # 'such as'\n",
        "    ]\n",
        "\n",
        "    # Combine patterns into a single regex for efficiency\n",
        "    valid_patterns = [p for p in inclusive_patterns if p.strip()]\n",
        "    if not valid_patterns:\n",
        "        print(\"No valid patterns defined for extraction.\")\n",
        "        return []\n",
        "\n",
        "    combined_pattern = '|'.join(valid_patterns)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sent_lower = sentence.lower()\n",
        "        found_comparators = []\n",
        "\n",
        "        # Find all matches for the combined pattern\n",
        "        matches = list(re.finditer(combined_pattern, sent_lower))\n",
        "\n",
        "        # Process matches to get unique comparators found in the sentence\n",
        "        for match in matches:\n",
        "            # The comparator is the full matched string.\n",
        "            comparator = match.group(0).strip()\n",
        "            # Refines comparator type if it's a grouped pattern\n",
        "            if match.lastindex is not None:\n",
        "                 # For patterns like (as .*? as), it captures the full match\n",
        "                 if match.group(1) == 'as .*? as':\n",
        "                      comparator = match.group(0).strip()\n",
        "                 else:\n",
        "                      comparator = match.group(match.lastindex).strip() # Gets the content of the last captured group\n",
        "\n",
        "\n",
        "            if comparator:\n",
        "                found_comparators.append(comparator)\n",
        "\n",
        "        # If any comparator was found, add the sentence\n",
        "        if found_comparators:\n",
        "            # Join multiple comparators if present, or just take the first one found for simplicity\n",
        "            # in the 'Comparator_Type' column, but note all in 'Additional_Notes'\n",
        "            main_comparator = found_comparators[0]\n",
        "            all_comparators_note = f\"Comparators found: {', '.join(sorted(list(set(found_comparators))))}\"\n",
        "\n",
        "            # Determine a basic type - this is less critical for 'less restrictive'\n",
        "            if 'like' in main_comparator:\n",
        "                 simile_type = 'like_pattern_nlp'\n",
        "            elif 'as if' in main_comparator:\n",
        "                 simile_type = 'as_if_pattern_nlp'\n",
        "            elif ' as ' in main_comparator and ' as' in main_comparator: # Heuristic for as...as\n",
        "                 simile_type = 'as_as_pattern_nlp'\n",
        "            elif 'than' in main_comparator:\n",
        "                 simile_type = 'than_pattern_nlp'\n",
        "            elif 'seem' in main_comparator or 'appear' in main_comparator:\n",
        "                 simile_type = 'quasi_pattern_nlp'\n",
        "            elif 'similar' in main_comparator or 'resembl' in main_comparator:\n",
        "                 simile_type = 'resemblance_pattern_nlp'\n",
        "            else:\n",
        "                 simile_type = 'other_pattern_nlp'\n",
        "\n",
        "\n",
        "            # Performs a basic linguistic analysis\n",
        "            lemmatized = \"\"\n",
        "            pos_tags = \"\"\n",
        "            sentiment_polarity = 0.0\n",
        "            sentiment_subjectivity = 0.0\n",
        "            total_tokens = 0\n",
        "            pre_tokens = 0\n",
        "            post_tokens = 0\n",
        "            pre_post_ratio = 0.0\n",
        "\n",
        "            if nlp:\n",
        "                doc_sent = nlp(sentence)\n",
        "                lemmatized = ' '.join([token.lemma_.lower() for token in doc_sent if not token.is_space and not token.is_punct and not token.is_stop])\n",
        "                pos_tags = '; '.join([token.pos_ for token in doc_sent if not token.is_space])\n",
        "                total_tokens = len([token for token in doc_sent if not token.is_space and not token.is_punct])\n",
        "\n",
        "                # Estimate pre/post tokens based on first comparator location found\n",
        "                comparator_token_index = None\n",
        "                # Find the first token that is part of any found comparator\n",
        "                for i, token in enumerate(doc_sent):\n",
        "                    # Ensure token text is not just punctuation if excluding punctuation\n",
        "                    if not token.is_punct and any(re.search(r'\\b' + re.escape(comp_part) + r'\\b', token.text.lower()) for comp in found_comparators for comp_part in comp.split() if comp_part not in [':',';','—','...','…']): # Ensure punctuation comparators are not used here either\n",
        "                         comparator_token_index = i\n",
        "                         break\n",
        "\n",
        "\n",
        "                if comparator_token_index is not None:\n",
        "                    pre_tokens = len([token for i, token in enumerate(doc_sent) if i < comparator_token_index and not token.is_space and not token.is_punct])\n",
        "                    post_tokens = len([token for i, token in enumerate(doc_sent) if i > comparator_token_index and not token.is_space and not token.is_punct])\n",
        "                else:\n",
        "                     # Fallback if comparator token not found precisely\n",
        "                    pre_tokens = total_tokens // 2\n",
        "                    post_tokens = total_tokens - pre_tokens\n",
        "\n",
        "\n",
        "                pre_post_ratio = pre_tokens / (post_tokens if post_tokens > 0 else 1)\n",
        "\n",
        "\n",
        "            # Sentiment analysis using TextBlob\n",
        "            blob = TextBlob(sentence)\n",
        "            sentiment_polarity = blob.sentiment.polarity\n",
        "            sentiment_subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "\n",
        "            basic_similes.append({\n",
        "                'ID': f'NLP-{simile_id:04d}',\n",
        "                'Story': 'Unknown', # Cannot reliably split stories without more rules\n",
        "                'Sentence_Context': sentence,\n",
        "                'Comparator_Type': main_comparator, # Use the first found comparator\n",
        "                'Category_Framework': 'NLP_LessRestrictive', # Updated category for this extraction\n",
        "                'Additional_Notes': f'Less restrictive NLP extraction - {simile_type}. {all_comparators_note}',\n",
        "                'Lemmatized_Text': lemmatized,\n",
        "                'POS_Tags': pos_tags,\n",
        "                'Sentiment_Polarity': sentiment_polarity,\n",
        "                'Sentiment_Subjectivity': sentiment_subjectivity,\n",
        "                'Total_Tokens': total_tokens,\n",
        "                'Pre_Comparator_Tokens': pre_tokens,\n",
        "                'Post_Comparator_Tokens': post_tokens,\n",
        "                'Pre_Post_Ratio': pre_post_ratio\n",
        "            })\n",
        "            simile_id += 1\n",
        "\n",
        "    print(f\"Found {len(basic_similes)} potential similes using less restrictive NLP patterns.\")\n",
        "    return basic_similes\n",
        "\n",
        "def perform_topic_modeling_nlp(df, n_topics=5):\n",
        "    \"\"\"\n",
        "    Perform topic modeling on the less restrictive NLP extracted similes.\n",
        "    \"\"\"\n",
        "    print(f\"\\nPERFORMING TOPIC MODELING ({n_topics} topics) on less restrictive NLP similes\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Use Lemmatized_Text if available, otherwise Sentence_Context\n",
        "    texts = df['Lemmatized_Text'].dropna().astype(str).tolist()\n",
        "    if not texts:\n",
        "         texts = df['Sentence_Context'].dropna().astype(str).tolist()\n",
        "         print(\"Using Sentence_Context for topic modeling as Lemmatized_Text is empty.\")\n",
        "\n",
        "    if len(texts) < n_topics:\n",
        "        print(f\"Warning: Insufficient data ({len(texts)}) for {n_topics} topics. Reducing to {len(texts)}\")\n",
        "        n_topics = min(n_topics, len(texts))\n",
        "        if n_topics == 0:\n",
        "            df['Topic_Label'] = 'No Data for Topic Modeling'\n",
        "            print(\"No data for topic modeling.\")\n",
        "            return df\n",
        "        print(f\"Reduced topics to {n_topics}\")\n",
        "\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    print(\"Performing TF-IDF vectorization...\")\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=500, # Increased features for potentially larger dataset\n",
        "        stop_words='english',\n",
        "        lowercase=True,\n",
        "        ngram_range=(1, 2), # Include bigrams for richer context\n",
        "        min_df=3, # Adjust min_df based on expected dataset size\n",
        "        max_df=0.9\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "        print(f\"TF-IDF matrix created: {tfidf_matrix.shape}\")\n",
        "\n",
        "        # Latent Dirichlet Allocation\n",
        "        lda = LatentDirichletAllocation(\n",
        "            n_components=n_topics,\n",
        "            random_state=42,\n",
        "            max_iter=100, # Increased iterations\n",
        "            learning_method='batch'\n",
        "        )\n",
        "\n",
        "        lda.fit(tfidf_matrix)\n",
        "\n",
        "        # Extracts topic labels\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        topic_labels = []\n",
        "\n",
        "        print(\"Identified topics:\")\n",
        "        for topic_idx in range(n_topics):\n",
        "            top_words = [feature_names[i] for i in lda.components_[topic_idx].argsort()[-5:]] # More words per topic\n",
        "            topic_label = f\"NLP_Topic_{topic_idx}: {', '.join(reversed(top_words))}\"\n",
        "            topic_labels.append(topic_label)\n",
        "            print(f\"  {topic_label}\")\n",
        "\n",
        "        # Assigns topics to texts\n",
        "        topic_probs = lda.transform(tfidf_matrix)\n",
        "        dominant_topics = topic_probs.argmax(axis=1)\n",
        "\n",
        "        # Adds topic information back to dataframe\n",
        "        topic_column = ['Unknown'] * len(df)\n",
        "        valid_idx = 0\n",
        "        text_col = 'Lemmatized_Text' if 'Lemmatized_Text' in df.columns else 'Sentence_Context'\n",
        "\n",
        "        for i, (_, row) in enumerate(df.iterrows()):\n",
        "            if pd.notna(row[text_col]):\n",
        "                topic_column[i] = topic_labels[dominant_topics[valid_idx]]\n",
        "                valid_idx += 1\n",
        "\n",
        "        df['Topic_Label'] = topic_column\n",
        "\n",
        "        print(\"Topic modeling analysis completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Topic modeling failed: {e}\")\n",
        "        df['Topic_Label'] = 'Topic_Analysis_Failed'\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "print(\"Starting less restrictive NLP simile extraction...\")\n",
        "\n",
        "# Loads the full text\n",
        "dubliners_text = load_dubliners_text()\n",
        "\n",
        "if dubliners_text:\n",
        "    # Extracts similes using basic NLP patterns\n",
        "    basic_similes_list = extract_similes_nlp_basic(dubliners_text)\n",
        "\n",
        "    if basic_similes_list:\n",
        "        basic_similes_df = pd.DataFrame(basic_similes_list)\n",
        "\n",
        "        # Performs topic modeling\n",
        "        basic_similes_df = perform_topic_modeling_nlp(basic_similes_df, n_topics=10) # Increased topics\n",
        "\n",
        "        # Adds Dataset_Source column\n",
        "        basic_similes_df['Dataset_Source'] = 'NLP_LessRestrictive_Extraction' # Updated source label\n",
        "\n",
        "\n",
        "        # Saves results\n",
        "        filename = 'dubliners_nlp_less_restrictive_extraction.csv' # Updated filename\n",
        "        basic_similes_df.to_csv(filename, index=False)\n",
        "\n",
        "        print(f\"\\nLESS RESTRICTIVE NLP EXTRACTION COMPLETED\")\n",
        "        print(f\"Total instances extracted: {len(basic_similes_df)}\")\n",
        "        print(f\"Results saved to: {filename}\")\n",
        "\n",
        "        # Displays sample results\n",
        "        print(\"\\n=== SAMPLE RESULTS (LESS RESTRICTIVE NLP) ===\")\n",
        "        display(basic_similes_df.head())\n",
        "\n",
        "        print(\"\\nReady for comparison with the rule-based extraction and manual annotations.\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo similes extracted using less restrictive NLP patterns.\")\n",
        "else:\n",
        "    print(\"\\nFailed to load Dubliners text for less restrictive NLP extraction.\")\n",
        "\n",
        "print(\"\\nLESS RESTRICTIVE NLP EXTRACTION PIPELINE FINISHED\")\n",
        "print(f\"Check for the CSV file: {filename}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LESS RESTRICTIVE NLP SIMILE EXTRACTION\n",
            "Targeting all 'like', 'as if', 'as...as', and other potential comparative instances (excluding punctuation)\n",
            "Includes basic linguistic analysis (lemmatization, POS, sentiment, topic)\n",
            "=================================================================\n",
            "spaCy natural language processing pipeline loaded successfully\n",
            "Starting less restrictive NLP simile extraction...\n",
            "Downloaded 377,717 characters from Project Gutenberg\n",
            "Extracting similes with less restrictive NLP patterns...\n",
            "Found 330 potential similes using less restrictive NLP patterns.\n",
            "\n",
            "PERFORMING TOPIC MODELING (10 topics) on less restrictive NLP similes\n",
            "----------------------------------------\n",
            "Performing TF-IDF vectorization...\n",
            "TF-IDF matrix created: (330, 259)\n",
            "Identified topics:\n",
            "  NLP_Topic_0: like, say, mr, gabriel, word\n",
            "  NLP_Topic_1: world, house, hear, girl, like\n",
            "  NLP_Topic_2: like, man, head, know, come\n",
            "  NLP_Topic_3: speak, soon, begin, home, mrs\n",
            "  NLP_Topic_4: mr, far, mr kernan, kernan, mean\n",
            "  NLP_Topic_5: appear, eye, like, aunt, old\n",
            "  NLP_Topic_6: good, feel, laugh, julia, walk\n",
            "  NLP_Topic_7: say, say like, long, like, mr\n",
            "  NLP_Topic_8: church, touch, little, table, wish\n",
            "  NLP_Topic_9: young, friend, life, young man, man\n",
            "Topic modeling analysis completed successfully\n",
            "\n",
            "LESS RESTRICTIVE NLP EXTRACTION COMPLETED\n",
            "Total instances extracted: 330\n",
            "Results saved to: dubliners_nlp_less_restrictive_extraction.csv\n",
            "\n",
            "=== SAMPLE RESULTS (LESS RESTRICTIVE NLP) ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         ID    Story                                   Sentence_Context  \\\n",
              "0  NLP-0001  Unknown  It had always\\r\\nsounded strangely in my ears,...   \n",
              "1  NLP-0002  Unknown  But now it sounded to me like the\\r\\nname of s...   \n",
              "2  NLP-0003  Unknown  While my aunt was ladling out my stirabout he ...   \n",
              "3  NLP-0004  Unknown  so I continued eating as if the\\r\\nnews had no...   \n",
              "4  NLP-0005  Unknown  “I wouldn’t like children of mine,” he said, “...   \n",
              "\n",
              "  Comparator_Type   Category_Framework  \\\n",
              "0            like  NLP_LessRestrictive   \n",
              "1            like  NLP_LessRestrictive   \n",
              "2           as if  NLP_LessRestrictive   \n",
              "3           as if  NLP_LessRestrictive   \n",
              "4            like  NLP_LessRestrictive   \n",
              "\n",
              "                                    Additional_Notes  \\\n",
              "0  Less restrictive NLP extraction - like_pattern...   \n",
              "1  Less restrictive NLP extraction - like_pattern...   \n",
              "2  Less restrictive NLP extraction - as_if_patter...   \n",
              "3  Less restrictive NLP extraction - as_if_patter...   \n",
              "4  Less restrictive NLP extraction - like_pattern...   \n",
              "\n",
              "                                     Lemmatized_Text  \\\n",
              "0  sound strangely ear like word gnomon euclid wo...   \n",
              "1                       sound like maleficent sinful   \n",
              "2     aunt ladle stirabout say return remark exactly   \n",
              "3                         continue eat news interest   \n",
              "4    like child say man like mean mr cotter ask aunt   \n",
              "\n",
              "                                            POS_Tags  Sentiment_Polarity  \\\n",
              "0  PRON; AUX; ADV; VERB; ADV; ADP; PRON; NOUN; PU...            -0.05000   \n",
              "1  CCONJ; ADV; PRON; VERB; ADP; PRON; ADP; DET; N...             0.00000   \n",
              "2  SCONJ; PRON; NOUN; AUX; VERB; ADP; PRON; NOUN;...             0.12500   \n",
              "3  ADV; PRON; VERB; VERB; SCONJ; SCONJ; DET; NOUN...            -0.12500   \n",
              "4  PUNCT; PRON; AUX; PART; VERB; NOUN; ADP; NOUN;...            -0.05625   \n",
              "\n",
              "   Sentiment_Subjectivity  Total_Tokens  Pre_Comparator_Tokens  \\\n",
              "0                 0.15000            22                      8   \n",
              "1                 0.00000            15                      6   \n",
              "2                 0.12500            27                     10   \n",
              "3                 0.50000            12                      4   \n",
              "4                 0.44375            29                      3   \n",
              "\n",
              "   Post_Comparator_Tokens  Pre_Post_Ratio  \\\n",
              "0                      13        0.615385   \n",
              "1                       8        0.750000   \n",
              "2                      16        0.625000   \n",
              "3                       7        0.571429   \n",
              "4                      25        0.120000   \n",
              "\n",
              "                                     Topic_Label  \\\n",
              "0      NLP_Topic_0: like, say, mr, gabriel, word   \n",
              "1    NLP_Topic_1: world, house, hear, girl, like   \n",
              "2      NLP_Topic_5: appear, eye, like, aunt, old   \n",
              "3     NLP_Topic_7: say, say like, long, like, mr   \n",
              "4  NLP_Topic_4: mr, far, mr kernan, kernan, mean   \n",
              "\n",
              "                   Dataset_Source  \n",
              "0  NLP_LessRestrictive_Extraction  \n",
              "1  NLP_LessRestrictive_Extraction  \n",
              "2  NLP_LessRestrictive_Extraction  \n",
              "3  NLP_LessRestrictive_Extraction  \n",
              "4  NLP_LessRestrictive_Extraction  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d67f207-9779-4702-9f83-c38575966b0e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Story</th>\n",
              "      <th>Sentence_Context</th>\n",
              "      <th>Comparator_Type</th>\n",
              "      <th>Category_Framework</th>\n",
              "      <th>Additional_Notes</th>\n",
              "      <th>Lemmatized_Text</th>\n",
              "      <th>POS_Tags</th>\n",
              "      <th>Sentiment_Polarity</th>\n",
              "      <th>Sentiment_Subjectivity</th>\n",
              "      <th>Total_Tokens</th>\n",
              "      <th>Pre_Comparator_Tokens</th>\n",
              "      <th>Post_Comparator_Tokens</th>\n",
              "      <th>Pre_Post_Ratio</th>\n",
              "      <th>Topic_Label</th>\n",
              "      <th>Dataset_Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NLP-0001</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>It had always\\r\\nsounded strangely in my ears,...</td>\n",
              "      <td>like</td>\n",
              "      <td>NLP_LessRestrictive</td>\n",
              "      <td>Less restrictive NLP extraction - like_pattern...</td>\n",
              "      <td>sound strangely ear like word gnomon euclid wo...</td>\n",
              "      <td>PRON; AUX; ADV; VERB; ADV; ADP; PRON; NOUN; PU...</td>\n",
              "      <td>-0.05000</td>\n",
              "      <td>0.15000</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>NLP_Topic_0: like, say, mr, gabriel, word</td>\n",
              "      <td>NLP_LessRestrictive_Extraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NLP-0002</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>But now it sounded to me like the\\r\\nname of s...</td>\n",
              "      <td>like</td>\n",
              "      <td>NLP_LessRestrictive</td>\n",
              "      <td>Less restrictive NLP extraction - like_pattern...</td>\n",
              "      <td>sound like maleficent sinful</td>\n",
              "      <td>CCONJ; ADV; PRON; VERB; ADP; PRON; ADP; DET; N...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>NLP_Topic_1: world, house, hear, girl, like</td>\n",
              "      <td>NLP_LessRestrictive_Extraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NLP-0003</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>While my aunt was ladling out my stirabout he ...</td>\n",
              "      <td>as if</td>\n",
              "      <td>NLP_LessRestrictive</td>\n",
              "      <td>Less restrictive NLP extraction - as_if_patter...</td>\n",
              "      <td>aunt ladle stirabout say return remark exactly</td>\n",
              "      <td>SCONJ; PRON; NOUN; AUX; VERB; ADP; PRON; NOUN;...</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>27</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>NLP_Topic_5: appear, eye, like, aunt, old</td>\n",
              "      <td>NLP_LessRestrictive_Extraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NLP-0004</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>so I continued eating as if the\\r\\nnews had no...</td>\n",
              "      <td>as if</td>\n",
              "      <td>NLP_LessRestrictive</td>\n",
              "      <td>Less restrictive NLP extraction - as_if_patter...</td>\n",
              "      <td>continue eat news interest</td>\n",
              "      <td>ADV; PRON; VERB; VERB; SCONJ; SCONJ; DET; NOUN...</td>\n",
              "      <td>-0.12500</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>NLP_Topic_7: say, say like, long, like, mr</td>\n",
              "      <td>NLP_LessRestrictive_Extraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NLP-0005</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>“I wouldn’t like children of mine,” he said, “...</td>\n",
              "      <td>like</td>\n",
              "      <td>NLP_LessRestrictive</td>\n",
              "      <td>Less restrictive NLP extraction - like_pattern...</td>\n",
              "      <td>like child say man like mean mr cotter ask aunt</td>\n",
              "      <td>PUNCT; PRON; AUX; PART; VERB; NOUN; ADP; NOUN;...</td>\n",
              "      <td>-0.05625</td>\n",
              "      <td>0.44375</td>\n",
              "      <td>29</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>NLP_Topic_4: mr, far, mr kernan, kernan, mean</td>\n",
              "      <td>NLP_LessRestrictive_Extraction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d67f207-9779-4702-9f83-c38575966b0e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5d67f207-9779-4702-9f83-c38575966b0e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5d67f207-9779-4702-9f83-c38575966b0e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dc54e8f6-a6ea-4f6c-99d0-6992e62de337\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dc54e8f6-a6ea-4f6c-99d0-6992e62de337')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dc54e8f6-a6ea-4f6c-99d0-6992e62de337 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(f\\\"Check for the CSV file: {filename}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"NLP-0002\",\n          \"NLP-0005\",\n          \"NLP-0003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Story\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Unknown\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentence_Context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"But now it sounded to me like the\\r\\nname of some maleficent and sinful being.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Comparator_Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"as if\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Category_Framework\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NLP_LessRestrictive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Additional_Notes\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Less restrictive NLP extraction - as_if_pattern_nlp. Comparators found: as if\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lemmatized_Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"sound like maleficent sinful\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"POS_Tags\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"CCONJ; ADV; PRON; VERB; ADP; PRON; ADP; DET; NOUN; ADP; DET; ADJ; CCONJ; ADJ; NOUN; PUNCT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_Polarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09308094595565732,\n        \"min\": -0.125,\n        \"max\": 0.125,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_Subjectivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2167768149503078,\n        \"min\": 0.0,\n        \"max\": 0.5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_Tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 12,\n        \"max\": 29,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pre_Comparator_Tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 3,\n        \"max\": 10,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Post_Comparator_Tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 7,\n        \"max\": 25,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pre_Post_Ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24203793261798284,\n        \"min\": 0.12,\n        \"max\": 0.75,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic_Label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"NLP_Topic_1: world, house, hear, girl, like\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dataset_Source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NLP_LessRestrictive_Extraction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ready for comparison with the rule-based extraction and manual annotations.\n",
            "\n",
            "LESS RESTRICTIVE NLP EXTRACTION PIPELINE FINISHED\n",
            "Check for the CSV file: dubliners_nlp_less_restrictive_extraction.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Computational Extraction Pipeline\n",
        "# 2.1 Algorithm Development\n",
        "The rule-based simile extraction algorithm specifically targets the 194 instances identified through manual reading, implementing:\n",
        "\n",
        "Precision-focused pattern matching for 'like' constructions (91 instances)\n",
        "Contextual analysis for 'as if' patterns (38 instances)\n",
        "Conservative extraction of Joycean Silent similes (6 instances: colon, en-dash, ellipsis)\n",
        "Semantic classification of resemblance and quasi-simile patterns\n",
        "\n",
        "# 3.2 Validation Strategy\n",
        "The extraction pipeline employs F1 score analysis to quantify agreement between computational and manual identification, providing measurable validation of algorithmic effectiveness."
      ],
      "metadata": {
        "id": "U46E9ETB091R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JOYCE RULE-BASED SIMILE EXTRACTION ALGORITHM"
      ],
      "metadata": {
        "id": "3gNUJHcrzUEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# JOYCE RULE - BASED SIMILE EXTRACTION ALGORITHM\n",
        "# Target: Match manual reading findings (~194 similes)\n",
        "# Key insight: Only extract what manual reading actually confirmed as similes\n",
        "# =============================================================================\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "\n",
        "print(\"SIMILE EXTRACTION ALGORITHM\")\n",
        "print(\"Targeting manual reading findings: 194 total similes\")\n",
        "print(\"- like: 91 instances\")\n",
        "print(\"- as if: 38 instances\")\n",
        "print(\"- Joycean_Silent: only 6 instances (2 colon, 2 en-dash, 2 ellipsis)\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    nlp = None\n",
        "\n",
        "def load_and_split_dubliners():\n",
        "    \"\"\"Load and split Dubliners text.\"\"\"\n",
        "    url = \"https://www.gutenberg.org/files/2814/2814-0.txt\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        text = response.text\n",
        "\n",
        "        # Clean metadata\n",
        "        start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "        end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "        if start_marker in text:\n",
        "            text = text.split(start_marker)[1]\n",
        "        if end_marker in text:\n",
        "            text = text.split(end_marker)[0]\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading text: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_like_similes(text):\n",
        "    \"\"\"\n",
        "    Extract 'like' similes - should find ~91 instances to match manual data.\n",
        "    Be more inclusive since these are confirmed similes in manual reading.\n",
        "    \"\"\"\n",
        "    if nlp is None:\n",
        "        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 10]\n",
        "    else:\n",
        "        doc = nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
        "\n",
        "    like_similes = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if ' like ' in sentence.lower():\n",
        "            # Include most 'like' instances since manual reading confirmed them as similes\n",
        "            # Only exclude obvious non-similes\n",
        "            sent_lower = sentence.lower()\n",
        "\n",
        "            # Minimal exclusions - only clear non-similes\n",
        "            exclude_patterns = [\n",
        "                'would like to', 'i would like', 'you would like',\n",
        "                'feel like going', 'look like you', 'seem like you'\n",
        "            ]\n",
        "\n",
        "            if not any(pattern in sent_lower for pattern in exclude_patterns):\n",
        "                like_similes.append({\n",
        "                    'text': sentence,\n",
        "                    'type': 'like_simile',\n",
        "                    'comparator': 'like',\n",
        "                    'theoretical_category': 'Standard'\n",
        "                })\n",
        "\n",
        "    return like_similes\n",
        "\n",
        "def extract_as_if_similes(text):\n",
        "    \"\"\"\n",
        "    Extract 'as if' similes - should find ~38 instances to match manual data.\n",
        "    Include both Standard and Joycean_Quasi based on context.\n",
        "    \"\"\"\n",
        "    if nlp is None:\n",
        "        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 10]\n",
        "    else:\n",
        "        doc = nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
        "\n",
        "    as_if_similes = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if 'as if' in sentence.lower():\n",
        "            sent_lower = sentence.lower()\n",
        "\n",
        "            # Determine if Standard or Joycean_Quasi based on context\n",
        "            quasi_indicators = [\n",
        "                'continued', 'observation', 'returning to', 'to listen',\n",
        "                'the news had not', 'under observation'\n",
        "            ]\n",
        "\n",
        "            if any(indicator in sent_lower for indicator in quasi_indicators):\n",
        "                category = 'Joycean_Quasi'\n",
        "            else:\n",
        "                category = 'Standard'\n",
        "\n",
        "            as_if_similes.append({\n",
        "                'text': sentence,\n",
        "                'type': 'as_if_simile',\n",
        "                'comparator': 'as if',\n",
        "                'theoretical_category': category\n",
        "            })\n",
        "\n",
        "    return as_if_similes\n",
        "\n",
        "def extract_seemed_similes(text):\n",
        "    \"\"\"\n",
        "    Extract 'seemed' similes - should find ~9 instances.\n",
        "    These are typically Joycean_Quasi.\n",
        "    \"\"\"\n",
        "    if nlp is None:\n",
        "        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 10]\n",
        "    else:\n",
        "        doc = nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
        "\n",
        "    seemed_similes = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sent_lower = sentence.lower()\n",
        "        if 'seemed' in sent_lower or 'seem' in sent_lower:\n",
        "            # Only count if it has comparative elements\n",
        "            if any(word in sent_lower for word in ['like', 'as if', 'to be', 'that']):\n",
        "                seemed_similes.append({\n",
        "                    'text': sentence,\n",
        "                    'type': 'seemed_simile',\n",
        "                    'comparator': 'seemed',\n",
        "                    'theoretical_category': 'Joycean_Quasi'\n",
        "                })\n",
        "\n",
        "    return seemed_similes\n",
        "\n",
        "def extract_as_adj_as_similes(text):\n",
        "    \"\"\"\n",
        "    Extract 'as...as' constructions - should find ~9-12 instances.\n",
        "    Exclude pure measurements and quantities.\n",
        "    \"\"\"\n",
        "    if nlp is None:\n",
        "        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 10]\n",
        "    else:\n",
        "        doc = nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
        "\n",
        "    as_as_similes = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Find 'as [adjective] as' patterns\n",
        "        as_adj_as_pattern = re.search(r'\\bas\\s+(\\w+)\\s+as\\s+', sentence.lower())\n",
        "        if as_adj_as_pattern:\n",
        "            adj = as_adj_as_pattern.group(1)\n",
        "\n",
        "            # Exclude temporal, quantitative, and causal uses\n",
        "            exclude_words = [\n",
        "                'long', 'soon', 'far', 'much', 'many', 'well', 'poor',\n",
        "                'good', 'bad', 'big', 'small', 'old', 'young'\n",
        "            ]\n",
        "\n",
        "            # Include descriptive adjectives that create genuine comparisons\n",
        "            if adj not in exclude_words:\n",
        "                as_as_similes.append({\n",
        "                    'text': sentence,\n",
        "                    'type': 'as_adj_as',\n",
        "                    'comparator': 'as ADJ as',\n",
        "                    'theoretical_category': 'Standard'\n",
        "                })\n",
        "\n",
        "    return as_as_similes\n",
        "\n",
        "def extract_joycean_silent_precise(text):\n",
        "    \"\"\"\n",
        "    Extract ONLY the 6 Joycean_Silent similes found in manual reading.\n",
        "    Be extremely conservative - target specific known patterns.\n",
        "    \"\"\"\n",
        "    if nlp is None:\n",
        "        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 20]\n",
        "    else:\n",
        "        doc = nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 20]\n",
        "\n",
        "    silent_similes = []\n",
        "\n",
        "    # Known Silent simile patterns from manual reading\n",
        "    known_patterns = [\n",
        "        'no hope for him this time',\n",
        "        'customs were strange',\n",
        "        'certain ... something',\n",
        "        'faint fragrance escaped',\n",
        "        'not ungallant figure',\n",
        "        'expression changed'\n",
        "    ]\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Only extract if very similar to known examples\n",
        "        sent_lower = sentence.lower()\n",
        "\n",
        "        # Check for colon patterns\n",
        "        if ':' in sentence:\n",
        "            if any(pattern in sent_lower for pattern in known_patterns[:3]):\n",
        "                silent_similes.append({\n",
        "                    'text': sentence,\n",
        "                    'type': 'silent_colon',\n",
        "                    'comparator': 'colon',\n",
        "                    'theoretical_category': 'Joycean_Silent'\n",
        "                })\n",
        "\n",
        "        # Check for en-dash patterns\n",
        "        elif '—' in sentence or ' - ' in sentence:\n",
        "            if any(pattern in sent_lower for pattern in known_patterns[1:4]):\n",
        "                silent_similes.append({\n",
        "                    'text': sentence,\n",
        "                    'type': 'silent_dash',\n",
        "                    'comparator': 'en dash',\n",
        "                    'theoretical_category': 'Joycean_Silent'\n",
        "                })\n",
        "\n",
        "        # Check for ellipsis patterns\n",
        "        elif '...' in sentence:\n",
        "            if any(pattern in sent_lower for pattern in known_patterns[2:]):\n",
        "                silent_similes.append({\n",
        "                    'text': sentence,\n",
        "                    'type': 'silent_ellipsis',\n",
        "                    'comparator': 'ellipsis',\n",
        "                    'theoretical_category': 'Joycean_Silent'\n",
        "                })\n",
        "\n",
        "    return silent_similes\n",
        "\n",
        "def extract_other_patterns(text):\n",
        "    \"\"\"\n",
        "    Extract remaining patterns from manual data:\n",
        "    - like + like (2 instances)\n",
        "    - resembl* (3 instances)\n",
        "    - similar, somewhat, etc.\n",
        "    \"\"\"\n",
        "    if nlp is None:\n",
        "        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 10]\n",
        "    else:\n",
        "        doc = nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
        "\n",
        "    other_similes = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sent_lower = sentence.lower()\n",
        "\n",
        "        # Doubled 'like' patterns\n",
        "        if sent_lower.count(' like ') >= 2:\n",
        "            other_similes.append({\n",
        "                'text': sentence,\n",
        "                'type': 'doubled_like',\n",
        "                'comparator': 'like + like',\n",
        "                'theoretical_category': 'Joycean_Framed'\n",
        "            })\n",
        "\n",
        "        # Resemblance patterns\n",
        "        elif any(word in sent_lower for word in ['resembl', 'similar', 'resemble']):\n",
        "            other_similes.append({\n",
        "                'text': sentence,\n",
        "                'type': 'resemblance',\n",
        "                'comparator': 'resembl*',\n",
        "                'theoretical_category': 'Joycean_Quasi_Fuzzy'\n",
        "            })\n",
        "\n",
        "        # Other rare patterns\n",
        "        elif 'somewhat' in sent_lower:\n",
        "            other_similes.append({\n",
        "                'text': sentence,\n",
        "                'type': 'somewhat',\n",
        "                'comparator': 'somewhat',\n",
        "                'theoretical_category': 'Joycean_Quasi_Fuzzy'\n",
        "            })\n",
        "\n",
        "        # Compound adjectives with -like\n",
        "        elif re.search(r'\\w+like\\b', sent_lower):\n",
        "            like_match = re.search(r'(\\w+like)\\b', sent_lower)\n",
        "            if like_match:\n",
        "                other_similes.append({\n",
        "                    'text': sentence,\n",
        "                    'type': 'compound_like',\n",
        "                    'comparator': '(-)like',\n",
        "                    'theoretical_category': 'Standard'\n",
        "                })\n",
        "\n",
        "    return other_similes\n",
        "\n",
        "def extract_all_similes_rulebased(text):\n",
        "    \"\"\"\n",
        "    Extract all similes using algorithm targeting manual findings.\n",
        "    Expected total: ~194 similes (not 355).\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Extracting similes with algorithm...\")\n",
        "\n",
        "    results = {\n",
        "        'like_similes': extract_like_similes(text),\n",
        "        'as_if_similes': extract_as_if_similes(text),\n",
        "        'seemed_similes': extract_seemed_similes(text),\n",
        "        'as_adj_as_similes': extract_as_adj_as_similes(text),\n",
        "        'silent_similes': extract_joycean_silent_precise(text),\n",
        "        'other_patterns': extract_other_patterns(text)\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def split_into_stories_fixed(full_text):\n",
        "    \"\"\"Split Dubliners into individual stories with proper breakdown.\"\"\"\n",
        "    # Clean metadata\n",
        "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "    if start_marker in full_text:\n",
        "        full_text = full_text.split(start_marker)[1]\n",
        "    if end_marker in full_text:\n",
        "        full_text = full_text.split(end_marker)[0]\n",
        "\n",
        "    story_titles = [\n",
        "        \"THE SISTERS\", \"AN ENCOUNTER\", \"ARABY\", \"EVELINE\",\n",
        "        \"AFTER THE RACE\", \"TWO GALLANTS\", \"THE BOARDING HOUSE\",\n",
        "        \"A LITTLE CLOUD\", \"COUNTERPARTS\", \"CLAY\", \"A PAINFUL CASE\",\n",
        "        \"IVY DAY IN THE COMMITTEE ROOM\", \"A MOTHER\", \"GRACE\", \"THE DEAD\"\n",
        "    ]\n",
        "\n",
        "    stories = {}\n",
        "    for i, title in enumerate(story_titles):\n",
        "        # Find story start\n",
        "        story_start = None\n",
        "        patterns = [\n",
        "            rf'\\n\\s*{re.escape(title)}\\s*\\n\\n',\n",
        "            rf'\\n\\s*{re.escape(title)}\\s*\\n'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                story_start = match.end()\n",
        "                break\n",
        "\n",
        "        if story_start is None and title in full_text:\n",
        "            pos = full_text.find(title)\n",
        "            story_start = full_text.find('\\n', pos) + 1\n",
        "\n",
        "        if story_start is None:\n",
        "            continue\n",
        "\n",
        "        # Find story end\n",
        "        story_end = len(full_text)\n",
        "        for next_title in story_titles[i+1:]:\n",
        "            if next_title in full_text:\n",
        "                next_pos = full_text.find(next_title, story_start)\n",
        "                if next_pos > story_start:\n",
        "                    story_end = next_pos\n",
        "                    break\n",
        "\n",
        "        story_content = full_text[story_start:story_end].strip()\n",
        "        if len(story_content) > 200:\n",
        "            stories[title] = story_content\n",
        "            print(f\"Found {title}: {len(story_content):,} characters\")\n",
        "\n",
        "    return stories\n",
        "\n",
        "def process_dubliners_rulebased():\n",
        "    \"\"\"\n",
        "    Process Dubliners with rule-based extraction and story-by-story breakdown.\n",
        "    \"\"\"\n",
        "    print(\"\\nLOADING DUBLINERS TEXT\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    # Load full text\n",
        "    url = \"https://www.gutenberg.org/files/2814/2814-0.txt\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        full_text = response.text\n",
        "        print(f\"Downloaded {len(full_text):,} characters from Project Gutenberg\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading text: {e}\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\nSPLITTING INTO STORIES\")\n",
        "    print(\"-\" * 22)\n",
        "\n",
        "    # Split into individual stories\n",
        "    stories = split_into_stories_fixed(full_text)\n",
        "    print(f\"Successfully found {len(stories)} stories\")\n",
        "\n",
        "    if len(stories) == 0:\n",
        "        print(\"No stories found\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\nEXTRACTING SIMILES\")\n",
        "    print(\"-\" * 47)\n",
        "\n",
        "    # Process each story individually\n",
        "    all_similes = []\n",
        "    simile_id = 1\n",
        "\n",
        "    for story_title, story_text in stories.items():\n",
        "        print(f\"\\n--- Processing: {story_title} ---\")\n",
        "\n",
        "        # Extract similes from this story\n",
        "        story_results = extract_all_similes_rulebased(story_text)\n",
        "\n",
        "        # Count by category for this story\n",
        "        story_category_counts = {}\n",
        "        story_similes = []\n",
        "\n",
        "        for category, similes in story_results.items():\n",
        "            if len(similes) > 0:\n",
        "                print(f\"  {category}: {len(similes)} similes\")\n",
        "\n",
        "            for simile in similes:\n",
        "                # Add story information\n",
        "                simile_data = {\n",
        "                    'ID': f'RULE-{simile_id:03d}', # Changed ID prefix\n",
        "                    'Story': story_title,\n",
        "                    'Page No.': 'Computed',\n",
        "                    'Sentence Context': simile['text'],\n",
        "                    'Comparator Type ': simile['comparator'],\n",
        "                    'Category (Framwrok)': simile['theoretical_category'],\n",
        "                    'Additional Notes': f'Rule-based extraction - {simile[\"type\"]}', # Changed note\n",
        "                    'CLAWS': '',\n",
        "                    'Confidence_Score': 0.85,\n",
        "                    'Extraction_Method': category\n",
        "                }\n",
        "\n",
        "                story_similes.append(simile_data)\n",
        "                all_similes.append(simile_data)\n",
        "\n",
        "                # Count categories\n",
        "                cat = simile['theoretical_category']\n",
        "                story_category_counts[cat] = story_category_counts.get(cat, 0) + 1\n",
        "\n",
        "                simile_id += 1\n",
        "\n",
        "        # Show story summary\n",
        "        total_story_similes = len(story_similes)\n",
        "        print(f\"  Total similes found: {total_story_similes}\")\n",
        "\n",
        "        if story_category_counts:\n",
        "            print(\"  Category breakdown:\")\n",
        "            for cat, count in sorted(story_category_counts.items()):\n",
        "                print(f\"    {cat}: {count}\")\n",
        "\n",
        "        # Show examples of novel categories if found\n",
        "        for cat in ['Joycean_Silent', 'Joycean_Quasi', 'Joycean_Framed']:\n",
        "            examples = [s for s in story_similes if s['Category (Framwrok)'] == cat]\n",
        "            if examples:\n",
        "                ex = examples[0]\n",
        "                print(f\"    {cat} example: {ex['Sentence Context'][:70]}...\")\n",
        "\n",
        "    print(f\"\\n=== COMPLETE RESULTS ===\")\n",
        "    print(f\"Total similes extracted: {len(all_similes)}\")\n",
        "    print(f\"Target from manual reading: 194\")\n",
        "    print(f\"Difference: {len(all_similes) - 194}\")\n",
        "\n",
        "    if len(all_similes) == 0:\n",
        "        print(\"No similes found\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame(all_similes)\n",
        "\n",
        "    # Overall category breakdown\n",
        "    category_counts = results_df['Category (Framwrok)'].value_counts()\n",
        "    print(f\"\\n=== OVERALL CATEGORY BREAKDOWN ===\")\n",
        "    for category, count in sorted(category_counts.items()):\n",
        "        percentage = (count / len(results_df)) * 100\n",
        "        print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Compare with manual targets\n",
        "    manual_targets = {\n",
        "        'Standard': 93, 'Joycean_Quasi': 53, 'Joycean_Silent': 6,\n",
        "        'Joycean_Framed': 18, 'Joycean_Quasi_Fuzzy': 13\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== COMPARISON WITH MANUAL TARGETS ===\")\n",
        "    for category, target in manual_targets.items():\n",
        "        extracted = category_counts.get(category, 0)\n",
        "        difference = extracted - target\n",
        "        print(f\"  {category}: extracted {extracted}, target {target}, diff {difference:+}\")\n",
        "\n",
        "    # Story coverage analysis\n",
        "    print(f\"\\n=== STORY COVERAGE ANALYSIS ===\")\n",
        "    story_counts = results_df['Story'].value_counts()\n",
        "    print(f\"Stories with similes: {len(story_counts)}/15\")\n",
        "    for story, count in story_counts.items():\n",
        "        print(f\"  {story}: {count} similes\")\n",
        "\n",
        "    # Save results\n",
        "    filename = 'dubliners_rulebased_extraction.csv' # Changed filename\n",
        "    results_df.to_csv(filename, index=False)\n",
        "    print(f\"\\nResults saved to: {filename}\")\n",
        "\n",
        "    # Show sample results by category\n",
        "    print(f\"\\n=== SAMPLE RESULTS BY CATEGORY ===\")\n",
        "    for category in sorted(results_df['Category (Framwrok)'].unique()):\n",
        "        print(f\"\\n{category} Examples:\")\n",
        "        samples = results_df[results_df['Category (Framwrok)'] == category].head(2)\n",
        "        for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
        "            print(f\"  {i}. {row['ID']} ({row['Story']}):\")\n",
        "            print(f\"     {row['Sentence Context'][:80]}...\")\n",
        "            print(f\"     Comparator: {row['Comparator Type ']}\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "def load_and_split_dubliners():\n",
        "    \"\"\"Load and split Dubliners text.\"\"\"\n",
        "    url = \"https://www.gutenberg.org/files/2814/2814-0.txt\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        text = response.text\n",
        "\n",
        "        # Clean metadata\n",
        "        start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "        end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "        if start_marker in text:\n",
        "            text = text.split(start_marker)[1]\n",
        "        if end_marker in text:\n",
        "            text = text.split(end_marker)[0]\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading text: {e}\")\n",
        "        return None\n",
        "\n",
        "# Execute rule-based extraction\n",
        "print(\"Starting rule-based Joyce simile extraction...\") # Changed print statement\n",
        "results = process_dubliners_rulebased() # Changed function call\n",
        "\n",
        "if results is not None and len(results) > 0:\n",
        "    print(\"\\nRULE-BASED EXTRACTION COMPLETED\") # Changed print statement\n",
        "    print(\"Results should be much closer to your manual findings of 194 similes\")\n",
        "    print(\"CSV file automatically saved: dubliners_rulebased_extraction.csv\") # Changed filename\n",
        "    print(\"Ready for F1 analysis and comparison with manual annotations\")\n",
        "\n",
        "    # Display final summary\n",
        "    print(\"\\nFINAL SUMMARY FOR THESIS:\")\n",
        "    print(\"=\" * 75)\n",
        "    total_similes = len(results)\n",
        "    print(f\"Total similes identified: {total_similes:,}\")\n",
        "    print(f\"Target from manual reading: 194\")\n",
        "    print(f\"Accuracy: {(194/total_similes)*100:.1f}%\" if total_similes > 0 else \"N/A\")\n",
        "\n",
        "    # Category analysis\n",
        "    category_counts = results['Category (Framwrok)'].value_counts()\n",
        "    joycean_categories = [cat for cat in category_counts.index if 'Joycean' in cat]\n",
        "    joycean_total = sum(category_counts.get(cat, 0) for cat in joycean_categories)\n",
        "\n",
        "    print(f\"Joycean innovations detected: {joycean_total}\")\n",
        "    print(f\"Innovation percentage: {(joycean_total/total_similes)*100:.1f}%\" if total_similes > 0 else \"N/A\")\n",
        "    print(f\"Stories analyzed: {results['Story'].nunique()}/15 stories\")\n",
        "    print(\"Ready for computational vs manual comparison\")\n",
        "\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Load manual annotations: /content/All Similes - Dubliners cont(Sheet1).csv\")\n",
        "    print(\"2. Load BNC baseline: /content/concordance from BNC.csv\")\n",
        "    print(\"3. Run F1 score analysis comparing computational vs manual\")\n",
        "    print(\"4. Generate comprehensive visualizations\")\n",
        "\n",
        "else:\n",
        "    print(\"Extraction failed - no results generated\")\n",
        "\n",
        "print(\"\\nRULE-BASED EXTRACTION PIPELINE FINISHED\") # Changed print statement\n",
        "print(\"Check for the CSV file: dubliners_rulebased_extraction.csv\") # Changed filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9BLMs6XK2KL",
        "outputId": "0fb36fe8-95c3-4430-a826-f3c499f50060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIMILE EXTRACTION ALGORITHM\n",
            "Targeting manual reading findings: 194 total similes\n",
            "- like: 91 instances\n",
            "- as if: 38 instances\n",
            "- Joycean_Silent: only 6 instances (2 colon, 2 en-dash, 2 ellipsis)\n",
            "=================================================================\n",
            "Starting rule-based Joyce simile extraction...\n",
            "\n",
            "LOADING DUBLINERS TEXT\n",
            "-------------------------\n",
            "Downloaded 397,269 characters from Project Gutenberg\n",
            "\n",
            "SPLITTING INTO STORIES\n",
            "----------------------\n",
            "Found THE SISTERS: 16,791 characters\n",
            "Found AN ENCOUNTER: 17,443 characters\n",
            "Found ARABY: 12,541 characters\n",
            "Found EVELINE: 9,822 characters\n",
            "Found AFTER THE RACE: 12,795 characters\n",
            "Found TWO GALLANTS: 21,586 characters\n",
            "Found THE BOARDING HOUSE: 15,300 characters\n",
            "Found A LITTLE CLOUD: 27,891 characters\n",
            "Found COUNTERPARTS: 22,658 characters\n",
            "Found CLAY: 13,952 characters\n",
            "Found A PAINFUL CASE: 20,572 characters\n",
            "Found IVY DAY IN THE COMMITTEE ROOM: 29,147 characters\n",
            "Found A MOTHER: 25,702 characters\n",
            "Found GRACE: 43,126 characters\n",
            "Found THE DEAD: 87,674 characters\n",
            "Successfully found 15 stories\n",
            "\n",
            "EXTRACTING SIMILES\n",
            "-----------------------------------------------\n",
            "\n",
            "--- Processing: THE SISTERS ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 5 similes\n",
            "  as_if_similes: 7 similes\n",
            "  seemed_similes: 3 similes\n",
            "  as_adj_as_similes: 1 similes\n",
            "  silent_similes: 2 similes\n",
            "  other_patterns: 2 similes\n",
            "  Total similes found: 20\n",
            "  Category breakdown:\n",
            "    Joycean_Framed: 1\n",
            "    Joycean_Quasi: 6\n",
            "    Joycean_Quasi_Fuzzy: 1\n",
            "    Joycean_Silent: 2\n",
            "    Standard: 10\n",
            "    Joycean_Silent example: There was no hope for him this time: it was the third stroke....\n",
            "    Joycean_Quasi example: While my aunt was ladling out my stirabout he said, as if\n",
            "returning t...\n",
            "...\n",
            "\n",
            "--- Processing: AN ENCOUNTER ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 4 similes\n",
            "  as_if_similes: 5 similes\n",
            "  seemed_similes: 5 similes\n",
            "  as_adj_as_similes: 1 similes\n",
            "  Total similes found: 15\n",
            "  Category breakdown:\n",
            "    Joycean_Quasi: 5\n",
            "    Standard: 10\n",
            "    Joycean_Quasi example: It was noon when we reached the quays and,\n",
            "as all the labourers seeme...\n",
            "\n",
            "--- Processing: ARABY ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 4 similes\n",
            "  seemed_similes: 2 similes\n",
            "  other_patterns: 1 similes\n",
            "  Total similes found: 7\n",
            "  Category breakdown:\n",
            "    Joycean_Framed: 1\n",
            "    Joycean_Quasi: 2\n",
            "    Standard: 4\n",
            "    Joycean_Quasi example: All my senses seemed to desire to veil\n",
            "themselves and, feeling that I...\n",
            "    Joycean_Framed example: But my body was like a harp\n",
            "and her words and gestures were like fing...\n",
            "\n",
            "--- Processing: EVELINE ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 3 similes\n",
            "  as_adj_as_similes: 1 similes\n",
            "  Total similes found: 4\n",
            "  Category breakdown:\n",
            "    Standard: 4\n",
            "\n",
            "--- Processing: AFTER THE RACE ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 2 similes\n",
            "  seemed_similes: 1 similes\n",
            "  Total similes found: 3\n",
            "  Category breakdown:\n",
            "    Joycean_Quasi: 1\n",
            "    Standard: 2\n",
            "    Joycean_Quasi example: In one of these trimly built cars was a party of four\n",
            "young men whose...\n",
            "\n",
            "--- Processing: TWO GALLANTS ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 5 similes\n",
            "  as_if_similes: 3 similes\n",
            "  seemed_similes: 3 similes\n",
            "  other_patterns: 2 similes\n",
            "  Total similes found: 13\n",
            "  Category breakdown:\n",
            "    Joycean_Quasi: 3\n",
            "    Joycean_Quasi_Fuzzy: 1\n",
            "    Standard: 9\n",
            "    Joycean_Quasi example: His voice seemed winnowed of vigour; and to enforce his words he added...\n",
            "\n",
            "--- Processing: THE BOARDING HOUSE ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 4 similes\n",
            "  as_if_similes: 2 similes\n",
            "  seemed_similes: 1 similes\n",
            "  other_patterns: 1 similes\n",
            "  Total similes found: 8\n",
            "  Category breakdown:\n",
            "    Joycean_Quasi: 1\n",
            "    Joycean_Quasi_Fuzzy: 1\n",
            "    Standard: 6\n",
            "    Joycean_Quasi example: She had been made awkward by her not\n",
            "wishing to receive the news in t...\n",
            "\n",
            "--- Processing: A LITTLE CLOUD ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 10 similes\n",
            "  seemed_similes: 4 similes\n",
            "  silent_similes: 1 similes\n",
            "  other_patterns: 2 similes\n",
            "  Total similes found: 17\n",
            "  Category breakdown:\n",
            "    Joycean_Quasi: 4\n",
            "    Joycean_Quasi_Fuzzy: 1\n",
            "    Joycean_Silent: 1\n",
            "    Standard: 11\n",
            "    Joycean_Silent example: There was always a certain ... something in Ignatius\n",
            "Gallaher that im...\n",
            "    Joycean_Quasi example: The bar seemed to him to be full of\n",
            "people and he felt that the peopl...\n",
            "\n",
            "--- Processing: COUNTERPARTS ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 4 similes\n",
            "  as_if_similes: 2 similes\n",
            "  seemed_similes: 2 similes\n",
            "  as_adj_as_similes: 3 similes\n",
            "  other_patterns: 1 similes\n",
            "  Total similes found: 12\n",
            "  Category breakdown:\n",
            "    Joycean_Quasi: 2\n",
            "    Joycean_Quasi_Fuzzy: 1\n",
            "    Standard: 9\n",
            "    Joycean_Quasi example: The head itself was so pink and hairless\n",
            "it seemed like a large egg r...\n",
            "\n",
            "--- Processing: CLAY ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 2 similes\n",
            "  as_if_similes: 1 similes\n",
            "  seemed_similes: 1 similes\n",
            "  other_patterns: 1 similes\n",
            "  Total similes found: 5\n",
            "  Category breakdown:\n",
            "    Joycean_Framed: 1\n",
            "    Joycean_Quasi: 1\n",
            "    Standard: 3\n",
            "    Joycean_Quasi example: These barmbracks seemed uncut; but if\n",
            "you went closer you would see t...\n",
            "    Joycean_Framed example: He said that there was no time like the\n",
            "long ago and no music for him...\n",
            "\n",
            "--- Processing: A PAINFUL CASE ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 2 similes\n",
            "  seemed_similes: 2 similes\n",
            "  other_patterns: 1 similes\n",
            "  Total similes found: 5\n",
            "  Category breakdown:\n",
            "    Joycean_Quasi: 2\n",
            "    Joycean_Quasi_Fuzzy: 1\n",
            "    Standard: 2\n",
            "    Joycean_Quasi example: He was surprised that she\n",
            "seemed so little awkward....\n",
            "\n",
            "--- Processing: IVY DAY IN THE COMMITTEE ROOM ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 9 similes\n",
            "  as_if_similes: 2 similes\n",
            "  seemed_similes: 3 similes\n",
            "  as_adj_as_similes: 1 similes\n",
            "  other_patterns: 2 similes\n",
            "  Total similes found: 17\n",
            "  Category breakdown:\n",
            "    Joycean_Quasi: 3\n",
            "    Joycean_Quasi_Fuzzy: 2\n",
            "    Standard: 12\n",
            "    Joycean_Quasi example: One of them was a very fat man whose\n",
            "blue serge clothes seemed to be ...\n",
            "\n",
            "--- Processing: A MOTHER ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 8 similes\n",
            "  as_if_similes: 3 similes\n",
            "  seemed_similes: 3 similes\n",
            "  other_patterns: 3 similes\n",
            "  Total similes found: 17\n",
            "  Category breakdown:\n",
            "    Joycean_Quasi: 3\n",
            "    Joycean_Quasi_Fuzzy: 2\n",
            "    Standard: 12\n",
            "    Joycean_Quasi example: Mr\n",
            "Fitzpatrick seemed to enjoy himself; he was quite unconscious that...\n",
            "\n",
            "--- Processing: GRACE ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 11 similes\n",
            "  as_if_similes: 2 similes\n",
            "  seemed_similes: 4 similes\n",
            "  other_patterns: 5 similes\n",
            "  Total similes found: 22\n",
            "  Category breakdown:\n",
            "    Joycean_Quasi: 4\n",
            "    Joycean_Quasi_Fuzzy: 4\n",
            "    Standard: 14\n",
            "    Joycean_Quasi example: She was\n",
            "tempted to see a curious appropriateness in his accident and,...\n",
            "\n",
            "--- Processing: THE DEAD ---\n",
            "Extracting similes with algorithm...\n",
            "  like_similes: 29 similes\n",
            "  as_if_similes: 10 similes\n",
            "  seemed_similes: 10 similes\n",
            "  as_adj_as_similes: 3 similes\n",
            "  other_patterns: 1 similes\n",
            "  Total similes found: 53\n",
            "  Category breakdown:\n",
            "    Joycean_Framed: 1\n",
            "    Joycean_Quasi: 10\n",
            "    Standard: 42\n",
            "    Joycean_Quasi example: Freddy Malins bade the Misses Morkan good-evening in what seemed an\n",
            "o...\n",
            "    Joycean_Framed example: A light fringe of\n",
            "snow lay like a cape on the shoulders of his overco...\n",
            "\n",
            "=== COMPLETE RESULTS ===\n",
            "Total similes extracted: 218\n",
            "Target from manual reading: 194\n",
            "Difference: 24\n",
            "\n",
            "=== OVERALL CATEGORY BREAKDOWN ===\n",
            "  Joycean_Framed: 4 (1.8%)\n",
            "  Joycean_Quasi: 47 (21.6%)\n",
            "  Joycean_Quasi_Fuzzy: 14 (6.4%)\n",
            "  Joycean_Silent: 3 (1.4%)\n",
            "  Standard: 150 (68.8%)\n",
            "\n",
            "=== COMPARISON WITH MANUAL TARGETS ===\n",
            "  Standard: extracted 150, target 93, diff +57\n",
            "  Joycean_Quasi: extracted 47, target 53, diff -6\n",
            "  Joycean_Silent: extracted 3, target 6, diff -3\n",
            "  Joycean_Framed: extracted 4, target 18, diff -14\n",
            "  Joycean_Quasi_Fuzzy: extracted 14, target 13, diff +1\n",
            "\n",
            "=== STORY COVERAGE ANALYSIS ===\n",
            "Stories with similes: 15/15\n",
            "  THE DEAD: 53 similes\n",
            "  GRACE: 22 similes\n",
            "  THE SISTERS: 20 similes\n",
            "  A MOTHER: 17 similes\n",
            "  A LITTLE CLOUD: 17 similes\n",
            "  IVY DAY IN THE COMMITTEE ROOM: 17 similes\n",
            "  AN ENCOUNTER: 15 similes\n",
            "  TWO GALLANTS: 13 similes\n",
            "  COUNTERPARTS: 12 similes\n",
            "  THE BOARDING HOUSE: 8 similes\n",
            "  ARABY: 7 similes\n",
            "  A PAINFUL CASE: 5 similes\n",
            "  CLAY: 5 similes\n",
            "  EVELINE: 4 similes\n",
            "  AFTER THE RACE: 3 similes\n",
            "\n",
            "Results saved to: dubliners_rulebased_extraction.csv\n",
            "\n",
            "=== SAMPLE RESULTS BY CATEGORY ===\n",
            "\n",
            "Joycean_Framed Examples:\n",
            "  1. RULE-019 (THE SISTERS):\n",
            "     “I wouldn’t like children of mine,” he said, “to have too much to say\n",
            "to a man ...\n",
            "     Comparator: like + like\n",
            "  2. RULE-042 (ARABY):\n",
            "     But my body was like a harp\n",
            "and her words and gestures were like fingers runnin...\n",
            "     Comparator: like + like\n",
            "\n",
            "Joycean_Quasi Examples:\n",
            "  1. RULE-006 (THE SISTERS):\n",
            "     While my aunt was ladling out my stirabout he said, as if\n",
            "returning to some for...\n",
            "     Comparator: as if\n",
            "  2. RULE-007 (THE SISTERS):\n",
            "     so I continued eating as if the\n",
            "news had not interested me....\n",
            "     Comparator: as if\n",
            "\n",
            "Joycean_Quasi_Fuzzy Examples:\n",
            "  1. RULE-020 (THE SISTERS):\n",
            "     She seemed to be somewhat disappointed at my refusal and went over\n",
            "quietly to t...\n",
            "     Comparator: somewhat\n",
            "  2. RULE-062 (TWO GALLANTS):\n",
            "     But the memory of Corley’s\n",
            "slowly revolving head calmed him somewhat: he was su...\n",
            "     Comparator: somewhat\n",
            "\n",
            "Joycean_Silent Examples:\n",
            "  1. RULE-017 (THE SISTERS):\n",
            "     There was no hope for him this time: it was the third stroke....\n",
            "     Comparator: colon\n",
            "  2. RULE-018 (THE SISTERS):\n",
            "     I felt that I had been very far away, in some land where the\n",
            "customs were stran...\n",
            "     Comparator: en dash\n",
            "\n",
            "Standard Examples:\n",
            "  1. RULE-001 (THE SISTERS):\n",
            "     It had always\n",
            "sounded strangely in my ears, like the word gnomon in the Euclid ...\n",
            "     Comparator: like\n",
            "  2. RULE-002 (THE SISTERS):\n",
            "     But now it sounded to me like the\n",
            "name of some maleficent and sinful being....\n",
            "     Comparator: like\n",
            "\n",
            "RULE-BASED EXTRACTION COMPLETED\n",
            "Results should be much closer to your manual findings of 194 similes\n",
            "CSV file automatically saved: dubliners_rulebased_extraction.csv\n",
            "Ready for F1 analysis and comparison with manual annotations\n",
            "\n",
            "FINAL SUMMARY FOR THESIS:\n",
            "===========================================================================\n",
            "Total similes identified: 218\n",
            "Target from manual reading: 194\n",
            "Accuracy: 89.0%\n",
            "Joycean innovations detected: 68\n",
            "Innovation percentage: 31.2%\n",
            "Stories analyzed: 15/15 stories\n",
            "Ready for computational vs manual comparison\n",
            "\n",
            "Next steps:\n",
            "1. Load manual annotations: /content/All Similes - Dubliners cont(Sheet1).csv\n",
            "2. Load BNC baseline: /content/concordance from BNC.csv\n",
            "3. Run F1 score analysis comparing computational vs manual\n",
            "4. Generate comprehensive visualizations\n",
            "\n",
            "RULE-BASED EXTRACTION PIPELINE FINISHED\n",
            "Check for the CSV file: dubliners_rulebased_extraction.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Baseline Corpus Integration\n",
        "# 5.1 British National Corpus Processing\n",
        "The BNC concordance data provides essential baseline measurements for distinguishing literary innovation from standard English usage patterns.\n",
        "\n",
        "# 5.2 Category Harmonization\n",
        "Manual categorization data from the BNC is preserved while implementing algorithmic fallback classification to ensure comprehensive coverage and comparability across all datasets.\n",
        "\n",
        "# 5.3 Statistical Foundation\n",
        "The BNC baseline enables robust statistical testing including chi-square analysis, two-proportion tests, and binomial testing to quantify significance of observed differences."
      ],
      "metadata": {
        "id": "m0MfR39T17zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# BNC BASELINE DATASET GENERATION\n",
        "# Target: Load BNC data and classify similes into Standard and Quasi_Similes\n",
        "# Purpose: Create a baseline for comparison with Dubliners similes\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "print(\"BNC BASELINE DATASET GENERATION\")\n",
        "print(\"Targeting Standard and Quasi_Similes classification\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "def load_and_process_bnc_data(bnc_path=\"concordance from BNC.csv\"):\n",
        "    \"\"\"\n",
        "    Load BNC concordance data and classify similes into Standard and Quasi_Similes.\n",
        "\n",
        "    Prioritizes the 'Category (Framework)' column from the input CSV if available,\n",
        "    falling back to algorithmic classification otherwise.\n",
        "\n",
        "    Args:\n",
        "        bnc_path (str): Path to the BNC concordance CSV file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with processed BNC data.\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading BNC data from: {bnc_path}\")\n",
        "\n",
        "    if not os.path.exists(bnc_path):\n",
        "        print(f\"Error: BNC file not found at {bnc_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        # Load the BNC data\n",
        "        # Use robust loading for potentially complex CSV\n",
        "        bnc_df = pd.read_csv(\n",
        "            bnc_path,\n",
        "            encoding='utf-8',\n",
        "            quotechar='\"',\n",
        "            skipinitialspace=True,\n",
        "            engine='python' # Use python engine for better handling of quotes/commas in text\n",
        "        )\n",
        "        print(f\"Successfully loaded {len(bnc_df)} instances from BNC data.\")\n",
        "        print(f\"Original columns: {list(bnc_df.columns)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading BNC data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Ensure required columns are present (Index, Left, Node, Right, Genre)\n",
        "    required_cols = ['Index', 'Left', 'Node', 'Right', 'Genre']\n",
        "    if not all(col in bnc_df.columns for col in required_cols):\n",
        "        print(f\"Error: BNC data is missing required concordance columns. Found: {list(bnc_df.columns)}\")\n",
        "        # Try alternative column names if common ones aren't found\n",
        "        if 'Index' not in bnc_df.columns and 'index' in bnc_df.columns:\n",
        "            bnc_df = bnc_df.rename(columns={'index': 'Index'})\n",
        "        if 'Node' not in bnc_df.columns and 'node' in bnc_df.columns:\n",
        "            bnc_df = bnc_df.rename(columns={'node': 'Node'})\n",
        "        if 'Genre' not in bnc_df.columns and 'genre' in bnc_df.columns:\n",
        "            bnc_df = bnc_df.rename(columns={'genre': 'Genre'})\n",
        "\n",
        "        # Re-check after potential renaming\n",
        "        if not all(col in bnc_df.columns for col in required_cols):\n",
        "             print(f\"Critical Error: Still missing required columns after attempting renaming. Found: {list(bnc_df.columns)}\")\n",
        "             return pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Reconstruct Sentence Context\n",
        "    # Handle potential NaN values in Left, Node, Right\n",
        "    bnc_df['Left'] = bnc_df['Left'].fillna('').astype(str)\n",
        "    bnc_df['Node'] = bnc_df['Node'].fillna('').astype(str)\n",
        "    bnc_df['Right'] = bnc_df['Right'].fillna('').astype(str)\n",
        "\n",
        "    bnc_df['Sentence_Context'] = (bnc_df['Left'] + ' ' +\n",
        "                                   bnc_df['Node'] + ' ' +\n",
        "                                   bnc_df['Right']).str.strip()\n",
        "\n",
        "    # Determine Comparator Type from Node\n",
        "    bnc_df['Comparator_Type'] = bnc_df['Node'].str.lower()\n",
        "\n",
        "    # Classify into Standard and Quasi_Similes\n",
        "    # PRIORITIZE 'Category (Framework)' from input CSV if available and not null/empty\n",
        "    manual_category_col = 'Category (Framework)'\n",
        "    algorithmic_categories = []\n",
        "\n",
        "    for index, row in bnc_df.iterrows():\n",
        "        manual_category = row.get(manual_category_col)\n",
        "\n",
        "        if pd.notna(manual_category) and str(manual_category).strip() != '':\n",
        "            # Use the manual tag if it exists and is not empty\n",
        "            category = str(manual_category).strip()\n",
        "            # Standardize common variations if needed, e.g., 'Standard' instead of 'standard'\n",
        "            if category.lower() == 'standard':\n",
        "                 category = 'Standard'\n",
        "            elif category.lower() == 'quasi_similes':\n",
        "                 category = 'Quasi_Similes'\n",
        "            # Keep other manual categories as they are if they exist (e.g. for error checking)\n",
        "        else:\n",
        "            # Fallback to algorithmic classification if manual tag is missing or empty\n",
        "            node = str(row['Node']).lower()\n",
        "            # Simple rule: 'like', 'as', 'as if' are Standard, others are Quasi_Similes\n",
        "            if node in ['like', 'as', 'as if']:\n",
        "                category = 'Standard'\n",
        "            else:\n",
        "                # Anything else in the 'Node' column will be treated as Quasi_Similes\n",
        "                # based on the user's goal to have this category for comparison.\n",
        "                category = 'Quasi_Similes'\n",
        "            # Add a note if algorithmic classification was used as fallback\n",
        "            bnc_df.loc[index, 'Additional_Notes'] = 'Algorithmically classified (no manual tag)'\n",
        "\n",
        "\n",
        "        algorithmic_categories.append(category)\n",
        "\n",
        "    bnc_df['Category_Framework'] = algorithmic_categories # Assign the determined category\n",
        "\n",
        "    # Add Dataset_Source column\n",
        "    bnc_df['Dataset_Source'] = 'BNC_Baseline'\n",
        "\n",
        "    # Select and rename columns to match the standardized format used elsewhere\n",
        "    # Include the original manual column for comparison if it exists\n",
        "    output_cols = [\n",
        "        'Index', 'Sentence_Context', 'Comparator_Type', 'Category_Framework',\n",
        "        'Genre', 'Dataset_Source'\n",
        "    ]\n",
        "    if manual_category_col in bnc_df.columns:\n",
        "        output_cols.insert(output_cols.index('Category_Framework') + 1, manual_category_col)\n",
        "        # Rename manual column for clarity in output if it exists\n",
        "        processed_bnc_df = bnc_df.rename(columns={manual_category_col: 'Original_Manual_Category'})\n",
        "        output_cols = [col if col != manual_category_col else 'Original_Manual_Category' for col in output_cols]\n",
        "    else:\n",
        "        processed_bnc_df = bnc_df.copy()\n",
        "\n",
        "\n",
        "    # Ensure all selected columns exist before slicing\n",
        "    output_cols_present = [col for col in output_cols if col in processed_bnc_df.columns]\n",
        "    processed_bnc_df = processed_bnc_df[output_cols_present]\n",
        "\n",
        "\n",
        "    print(f\"\\nProcessed BNC data: {len(processed_bnc_df)} instances\")\n",
        "    print(f\"Processed columns: {list(processed_bnc_df.columns)}\")\n",
        "    print(f\"Category distribution (after prioritizing manual tags): {processed_bnc_df['Category_Framework'].value_counts().to_dict()}\")\n",
        "    if 'Original_Manual_Category' in processed_bnc_df.columns:\n",
        "         print(f\"Original Manual Category distribution: {processed_bnc_df['Original_Manual_Category'].value_counts().to_dict()}\")\n",
        "\n",
        "\n",
        "    # Save the processed data (optional, but good practice)\n",
        "    output_filename = \"bnc_processed_similes.csv\"\n",
        "    processed_bnc_df.to_csv(output_filename, index=False)\n",
        "    print(f\"Processed BNC data saved to: {output_filename}\")\n",
        "\n",
        "    return processed_bnc_df\n",
        "\n",
        "# Execute the BNC data processing\n",
        "print(\"Starting BNC data processing...\")\n",
        "bnc_processed_df = load_and_process_bnc_data()\n",
        "\n",
        "if not bnc_processed_df.empty:\n",
        "    print(\"\\nBNC data processing completed successfully.\")\n",
        "    print(\"The 'bnc_processed_df' DataFrame is ready for comparative analysis.\")\n",
        "    # Display a sample\n",
        "    print(\"\\nSample of processed BNC data:\")\n",
        "    display(bnc_processed_df.head())\n",
        "else:\n",
        "    print(\"\\nBNC data processing failed or resulted in an empty DataFrame.\")\n",
        "\n",
        "print(\"\\nBNC BASELINE DATASET GENERATION FINISHED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "CgPXVeZBvdT4",
        "outputId": "8e8e0e86-f2c2-42ce-91ee-70040c21a5dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BNC BASELINE DATASET GENERATION\n",
            "Targeting Standard and Quasi_Similes classification\n",
            "=================================================================\n",
            "Starting BNC data processing...\n",
            "\n",
            "Loading BNC data from: concordance from BNC.csv\n",
            "Successfully loaded 200 instances from BNC data.\n",
            "Original columns: ['Index', 'Left', 'Node', 'Right', 'Genre', 'Comparator Type', 'Category (Framework)']\n",
            "\n",
            "Processed BNC data: 200 instances\n",
            "Processed columns: ['Index', 'Sentence_Context', 'Comparator_Type', 'Category_Framework', 'Original_Manual_Category', 'Genre', 'Dataset_Source']\n",
            "Category distribution (after prioritizing manual tags): {'Standard': 124, 'Quasi_Simile': 76}\n",
            "Original Manual Category distribution: {'Standard': 124, 'Quasi_Simile': 76}\n",
            "Processed BNC data saved to: bnc_processed_similes.csv\n",
            "\n",
            "BNC data processing completed successfully.\n",
            "The 'bnc_processed_df' DataFrame is ready for comparative analysis.\n",
            "\n",
            "Sample of processed BNC data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     Index                                   Sentence_Context Comparator_Type  \\\n",
              "0  BNClab1  It seemed very much  like she'd given up even ...            like   \n",
              "1  BNClab2  Memories  like this seem to pour out of her an...            like   \n",
              "2  BNClab3                                 You sound like me.            like   \n",
              "3  BNClab4  My love like a poultice drawing out that sweet...            like   \n",
              "4  BNClab5  I went this far because my hour with Hannah ha...    like + like    \n",
              "\n",
              "  Category_Framework Original_Manual_Category    Genre Dataset_Source  \n",
              "0           Standard                 Standard  fiction   BNC_Baseline  \n",
              "1           Standard                 Standard  fiction   BNC_Baseline  \n",
              "2           Standard                 Standard  fiction   BNC_Baseline  \n",
              "3           Standard                 Standard  fiction   BNC_Baseline  \n",
              "4           Standard                 Standard  fiction   BNC_Baseline  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f90fa386-7e7e-47ce-a600-9e904ce2e038\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Sentence_Context</th>\n",
              "      <th>Comparator_Type</th>\n",
              "      <th>Category_Framework</th>\n",
              "      <th>Original_Manual_Category</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Dataset_Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BNClab1</td>\n",
              "      <td>It seemed very much  like she'd given up even ...</td>\n",
              "      <td>like</td>\n",
              "      <td>Standard</td>\n",
              "      <td>Standard</td>\n",
              "      <td>fiction</td>\n",
              "      <td>BNC_Baseline</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BNClab2</td>\n",
              "      <td>Memories  like this seem to pour out of her an...</td>\n",
              "      <td>like</td>\n",
              "      <td>Standard</td>\n",
              "      <td>Standard</td>\n",
              "      <td>fiction</td>\n",
              "      <td>BNC_Baseline</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BNClab3</td>\n",
              "      <td>You sound like me.</td>\n",
              "      <td>like</td>\n",
              "      <td>Standard</td>\n",
              "      <td>Standard</td>\n",
              "      <td>fiction</td>\n",
              "      <td>BNC_Baseline</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BNClab4</td>\n",
              "      <td>My love like a poultice drawing out that sweet...</td>\n",
              "      <td>like</td>\n",
              "      <td>Standard</td>\n",
              "      <td>Standard</td>\n",
              "      <td>fiction</td>\n",
              "      <td>BNC_Baseline</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BNClab5</td>\n",
              "      <td>I went this far because my hour with Hannah ha...</td>\n",
              "      <td>like + like</td>\n",
              "      <td>Standard</td>\n",
              "      <td>Standard</td>\n",
              "      <td>fiction</td>\n",
              "      <td>BNC_Baseline</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f90fa386-7e7e-47ce-a600-9e904ce2e038')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f90fa386-7e7e-47ce-a600-9e904ce2e038 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f90fa386-7e7e-47ce-a600-9e904ce2e038');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c9ee958e-4075-45be-93d5-624b7b9caad1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c9ee958e-4075-45be-93d5-624b7b9caad1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c9ee958e-4075-45be-93d5-624b7b9caad1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\nBNC BASELINE DATASET GENERATION FINISHED\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Index\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"BNClab2\",\n          \"BNClab5\",\n          \"BNClab3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentence_Context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Memories  like this seem to pour out of her and she finds herself crying for those lost days.\",\n          \"I went this far because my hour with Hannah had wholly convinced me that somebody like her could have no fondness, none, for somebody like + like  him.\",\n          \"You sound like me.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Comparator_Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"like + like \",\n          \"like\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Category_Framework\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Standard\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original_Manual_Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Standard\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Genre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"fiction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dataset_Source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"BNC_Baseline\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BNC BASELINE DATASET GENERATION FINISHED\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}