# =============================================================================
# COMPUTATIONAL ANALYSIS OF SIMILE STRUCTURES IN JOYCE'S DUBLINERS
# Notebook 1: Data Processing and Linguistic Analysis
# 
# Master's Dissertation Research - University College London
# Author: [Your Name]
# Supervisor: [Supervisor Name]
# 
# This notebook implements the data processing pipeline for computational 
# analysis of simile structures across three datasets: manual annotations,
# computational extractions, and BNC baseline corpus. The analysis extends
# the theoretical framework of Leech & Short (1981) with novel Joycean
# categories.
#
# Repository: https://github.com/[username]/joyce-dubliners-similes-analysis
# =============================================================================

# COLAB SETUP AND GITHUB INTEGRATION
# =============================================================================

# Mount Google Drive for file access (if using Colab)
try:
    from google.colab import drive
    drive.mount('/content/drive')
    print("Google Drive mounted successfully")
    
    # Set working directory to your project folder
    import os
    os.chdir('/content/drive/MyDrive/joyce-dubliners-similes-analysis')
    print("Working directory set to project folder")
    
except ImportError:
    print("Not running in Colab - skipping Drive mount")

# Clone GitHub repository (run once to get initial files)
# Uncomment the following lines on first run:
# !git clone https://github.com/[username]/joyce-dubliners-similes-analysis.git
# %cd joyce-dubliners-similes-analysis

# For subsequent runs, pull latest changes:
# !git pull origin main

# Install required packages
!pip install spacy textblob scikit-learn -q
!python -m spacy download en_core_web_lg -q

# Verify file structure
import os
print("Current working directory:", os.getcwd())
print("\nProject files:")
for file in os.listdir('.'):
    if file.endswith(('.csv', '.py', '.ipynb')):
        print(f"  ✓ {file}")

# =============================================================================
# CORE IMPORTS AND CONFIGURATION
# =============================================================================

import pandas as pd
import numpy as np
import spacy
import logging
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import re
import warnings
from pathlib import Path
import requests
from typing import Dict, List, Tuple, Optional

# Configure academic logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('simile_analysis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Suppress non-critical warnings for cleaner output
warnings.filterwarnings('ignore', category=UserWarning)

# Initialize spaCy language model for linguistic processing
try:
    nlp = spacy.load("en_core_web_lg")
    logger.info("SpaCy English language model initialized successfully")
except OSError:
    logger.error("SpaCy model not found. Install with: python -m spacy download en_core_web_lg")
    raise

# =============================================================================
# DATASET LOADING AND STANDARDIZATION MODULE
# =============================================================================

class SimileDataProcessor:
    """
    Handles loading, cleaning, and standardization of simile datasets for
    comparative analysis. Implements consistent data structures across
    manual annotations, computational extractions, and BNC baseline corpus.
    """
    
    def __init__(self):
        """Initialize the data processor with configuration parameters."""
        self.nlp = nlp
        self.processed_datasets = {}
        self.dataset_metadata = {}
        
        # Define theoretical framework categories for consistency
        self.theoretical_categories = {
            'standard': ['Standard', 'standard'],
            'quasi': ['Quasi', 'Joycean_Quasi', 'quasi'],
            'joycean_silent': ['Joycean_Silent', 'joycean_silent'],
            'joycean_hybrid': ['Joycean_Hybrid', 'Joycean_Quasi_Fuzzy', 'joycean_hybrid'],
            'joycean_framed': ['Joycean_Framed', 'joycean_framed'],
            'joycean_complex': ['Joycean', 'joycean']
        }
        
        logger.info("SimileDataProcessor initialized with theoretical framework categories")
    
    def load_manual_annotations(self, filepath: str) -> pd.DataFrame:
        """
        Load and process manual annotations from close reading analysis.
        
        This dataset serves as the gold standard for algorithmic validation,
        containing expert-annotated similes with theoretical categorization
        based on extended Leech & Short framework.
        
        Args:
            filepath (str): Path to manual annotations CSV file
            
        Returns:
            pd.DataFrame: Processed manual annotations dataset
        """
        logger.info("Loading manual annotations dataset")
        
        try:
            # Load with appropriate encoding for academic text
            manual_df = pd.read_csv(filepath, encoding='cp1252')
            
            # Standardize column names for consistency
            column_mapping = {
                'Category (Framwrok)': 'Category_Framework',  # Fix typo in original
                'Comparator Type ': 'Comparator_Type',  # Remove trailing space
                'Sentence Context': 'Sentence_Context',
                'Additional Notes': 'Additional_Notes',
                'Page No.': 'Page_Number'
            }
            manual_df = manual_df.rename(columns=column_mapping)
            
            # Clean and validate data
            manual_df = self._clean_manual_annotations(manual_df)
            
            # Add dataset identifier
            manual_df['Dataset_Source'] = 'Manual_Annotation'
            manual_df['Analysis_Method'] = 'Close_Reading'
            
            # Store metadata
            self.dataset_metadata['manual'] = {
                'total_instances': len(manual_df),
                'stories_covered': manual_df['Story'].nunique(),
                'categories_found': manual_df['Category_Framework'].nunique(),
                'date_processed': pd.Timestamp.now()
            }
            
            logger.info(f"Manual annotations loaded: {len(manual_df)} instances across {manual_df['Story'].nunique()} stories")
            
            return manual_df
            
        except Exception as e:
            logger.error(f"Failed to load manual annotations: {str(e)}")
            raise
    
    def load_computational_extractions(self, filepath: str = None) -> pd.DataFrame:
        """
        Load computational simile extractions from NLP pipeline.
        
        If no filepath provided, downloads fresh results from Gutenberg processing.
        This dataset represents algorithmic detection using the enhanced
        theoretical framework implementation.
        
        Args:
            filepath (str, optional): Path to computational results CSV
            
        Returns:
            pd.DataFrame: Processed computational extractions dataset
        """
        logger.info("Loading computational extractions dataset")
        
        if filepath and Path(filepath).exists():
            comp_df = pd.read_csv(filepath)
            logger.info(f"Loaded existing computational results from {filepath}")
        else:
            logger.info("No existing file found, running fresh computational extraction")
            comp_df = self._run_computational_extraction()
        
        # Standardize column structure
        comp_df = self._standardize_computational_data(comp_df)
        
        # Add dataset identifier
        comp_df['Dataset_Source'] = 'Computational_Extraction'
        comp_df['Analysis_Method'] = 'NLP_Pipeline'
        
        # Store metadata
        self.dataset_metadata['computational'] = {
            'total_instances': len(comp_df),
            'stories_covered': comp_df['Story'].nunique() if 'Story' in comp_df.columns else 15,
            'categories_found': comp_df['Category_Framework'].nunique(),
            'confidence_mean': comp_df['Confidence_Score'].mean() if 'Confidence_Score' in comp_df.columns else None,
            'date_processed': pd.Timestamp.now()
        }
        
        logger.info(f"Computational extractions processed: {len(comp_df)} instances")
        
        return comp_df
    
    def load_bnc_concordances(self, filepath: str) -> pd.DataFrame:
        """
        Load and process BNC concordance data for baseline comparison.
        
        Reconstructs full sentences from concordance format and applies
        the same theoretical framework for comparative analysis against
        Joyce's innovations.
        
        Args:
            filepath (str): Path to BNC concordance CSV file
            
        Returns:
            pd.DataFrame: Processed BNC baseline dataset
        """
        logger.info("Loading BNC concordance dataset for baseline comparison")
        
        try:
            bnc_df = pd.read_csv(filepath, encoding='utf-8')
            
            # Reconstruct full sentences from concordance format
            bnc_df['Sentence_Context'] = bnc_df.apply(
                lambda row: f"{row['Left']} {row['Node']} {row['Right']}".strip(), 
                axis=1
            )
            
            # Extract comparator from node
            bnc_df['Comparator_Type'] = bnc_df['Node'].str.lower()
            
            # Apply theoretical framework categorization
            bnc_df['Category_Framework'] = bnc_df.apply(
                self._categorize_bnc_simile, axis=1
            )
            
            # Add dataset identifiers
            bnc_df['Dataset_Source'] = 'BNC_Baseline'
            bnc_df['Analysis_Method'] = 'Corpus_Extraction'
            bnc_df['Story'] = 'BNC_Fiction'  # Standardize for comparison
            
            # Clean and standardize
            bnc_df = self._clean_bnc_data(bnc_df)
            
            # Store metadata
            self.dataset_metadata['bnc'] = {
                'total_instances': len(bnc_df),
                'genres_covered': bnc_df['Genre'].nunique() if 'Genre' in bnc_df.columns else 1,
                'categories_found': bnc_df['Category_Framework'].nunique(),
                'search_terms': bnc_df['Comparator_Type'].value_counts().to_dict(),
                'date_processed': pd.Timestamp.now()
            }
            
            logger.info(f"BNC concordances processed: {len(bnc_df)} instances from {bnc_df['Genre'].nunique() if 'Genre' in bnc_df.columns else 1} genres")
            
            return bnc_df
            
        except Exception as e:
            logger.error(f"Failed to load BNC concordances: {str(e)}")
            raise

# =============================================================================
# LINGUISTIC PROCESSING MODULE
# =============================================================================

class LinguisticAnalyzer:
    """
    Implements comprehensive linguistic analysis across all datasets.
    
    Provides uniform processing for POS tagging, lemmatization, token counting,
    sentiment analysis, and topic modeling to enable robust cross-dataset
    comparison and statistical analysis.
    """
    
    def __init__(self):
        """Initialize linguistic analyzer with spaCy model and configurations."""
        self.nlp = nlp
        self.topic_model = None
        self.vectorizer = None
        
        logger.info("LinguisticAnalyzer initialized for cross-dataset processing")
    
    def process_dataset_linguistics(self, df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:
        """
        Apply comprehensive linguistic processing to simile dataset.
        
        Processes each simile instance through the complete linguistic pipeline
        including morphological analysis, syntactic parsing, semantic analysis,
        and positional calculations relative to comparator elements.
        
        Args:
            df (pd.DataFrame): Input dataset with simile instances
            dataset_name (str): Dataset identifier for logging
            
        Returns:
            pd.DataFrame: Dataset enhanced with linguistic features
        """
        logger.info(f"Beginning linguistic processing for {dataset_name} dataset ({len(df)} instances)")
        
        # Initialize result columns
        linguistic_features = []
        
        for idx, row in df.iterrows():
            if idx % 50 == 0:
                logger.info(f"Processing instance {idx + 1}/{len(df)} for {dataset_name}")
            
            sentence_text = row['Sentence_Context']
            
            # Process with spaCy for comprehensive linguistic analysis
            doc = self.nlp(sentence_text)
            
            # Extract linguistic features
            features = self._extract_linguistic_features(doc, row)
            
            # Calculate positional features relative to comparator
            positional_features = self._calculate_positional_features(doc, row)
            
            # Combine all features
            combined_features = {**features, **positional_features}
            linguistic_features.append(combined_features)
        
        # Convert to DataFrame and merge with original
        features_df = pd.DataFrame(linguistic_features)
        
        # Merge linguistic features with original dataset
        result_df = pd.concat([df.reset_index(drop=True), features_df], axis=1)
        
        logger.info(f"Linguistic processing completed for {dataset_name}: {len(features_df.columns)} features extracted")
        
        return result_df
    
    def _extract_linguistic_features(self, doc, row: pd.Series) -> Dict:
        """
        Extract comprehensive linguistic features from spaCy document.
        
        Implements detailed morphological, syntactic, and semantic analysis
        following computational linguistics best practices for literary text
        processing.
        
        Args:
            doc: spaCy processed document
            row: Original data row for context
            
        Returns:
            Dict: Comprehensive linguistic feature set
        """
        # Token-level analysis
        tokens = [token for token in doc if not token.is_punct and not token.is_space]
        
        # POS tag distribution
        pos_tags = [token.pos_ for token in tokens]
        pos_distribution = {
            'noun_count': pos_tags.count('NOUN'),
            'verb_count': pos_tags.count('VERB'),
            'adj_count': pos_tags.count('ADJ'),
            'adv_count': pos_tags.count('ADV'),
            'pron_count': pos_tags.count('PRON')
        }
        
        # Dependency parsing analysis
        dependencies = [token.dep_ for token in tokens]
        syntactic_features = {
            'subject_count': dependencies.count('nsubj') + dependencies.count('nsubjpass'),
            'object_count': dependencies.count('dobj') + dependencies.count('pobj'),
            'modifier_count': dependencies.count('amod') + dependencies.count('advmod'),
            'compound_count': dependencies.count('compound')
        }
        
        # Lemmatization and lexical diversity
        lemmas = [token.lemma_.lower() for token in tokens if token.is_alpha]
        lexical_features = {
            'total_tokens': len(tokens),
            'unique_lemmas': len(set(lemmas)),
            'lexical_diversity': len(set(lemmas)) / len(lemmas) if lemmas else 0,
            'average_word_length': np.mean([len(token.text) for token in tokens]) if tokens else 0
        }
        
        # Named entity recognition
        entities = [(ent.text, ent.label_) for ent in doc.ents]
        entity_features = {
            'entity_count': len(entities),
            'person_entities': len([ent for ent in entities if ent[1] == 'PERSON']),
            'place_entities': len([ent for ent in entities if ent[1] in ['GPE', 'LOC']]),
            'time_entities': len([ent for ent in entities if ent[1] in ['DATE', 'TIME']])
        }
        
        # Sentiment analysis using TextBlob
        blob = TextBlob(doc.text)
        sentiment_features = {
            'sentiment_polarity': blob.sentiment.polarity,
            'sentiment_subjectivity': blob.sentiment.subjectivity
        }
        
        # Combine all linguistic features
        return {
            **pos_distribution,
            **syntactic_features,
            **lexical_features,
            **entity_features,
            **sentiment_features
        }
    
    def _calculate_positional_features(self, doc, row: pd.Series) -> Dict:
        """
        Calculate token positions relative to comparator elements.
        
        Implements precise positional analysis to measure syntactic balance
        and structural characteristics of comparative constructions, following
        the methodological framework established in corpus stylistics research.
        
        Args:
            doc: spaCy processed document
            row: Original data row with comparator information
            
        Returns:
            Dict: Positional feature measurements
        """
        comparator_type = row.get('Comparator_Type', '').lower()
        tokens = [token for token in doc if not token.is_punct and not token.is_space]
        
        # Locate comparator position
        comparator_position = self._find_comparator_position(tokens, comparator_type)
        
        if comparator_position is None:
            # Fallback to sentence midpoint if comparator not found
            comparator_position = len(tokens) // 2
        
        # Calculate pre/post-comparator metrics
        pre_comparator_tokens = tokens[:comparator_position]
        post_comparator_tokens = tokens[comparator_position + 1:]
        
        # Token counts
        pre_count = len(pre_comparator_tokens)
        post_count = len(post_comparator_tokens)
        
        # Positional ratios
        total_tokens = len(tokens)
        pre_ratio = pre_count / total_tokens if total_tokens > 0 else 0
        post_ratio = post_count / total_tokens if total_tokens > 0 else 0
        balance_ratio = pre_count / post_count if post_count > 0 else float('inf')
        
        # Semantic density analysis
        pre_content_words = len([t for t in pre_comparator_tokens if not t.is_stop])
        post_content_words = len([t for t in post_comparator_tokens if not t.is_stop])
        
        return {
            'pre_comparator_tokens': pre_count,
            'post_comparator_tokens': post_count,
            'total_tokens': total_tokens,
            'pre_post_ratio': balance_ratio,
            'pre_proportion': pre_ratio,
            'post_proportion': post_ratio,
            'comparator_position': comparator_position,
            'pre_content_density': pre_content_words / pre_count if pre_count > 0 else 0,
            'post_content_density': post_content_words / post_count if post_count > 0 else 0
        }
    
    def _find_comparator_position(self, tokens: List, comparator_type: str) -> Optional[int]:
        """
        Locate the position of the comparator within the token sequence.
        
        Implements robust pattern matching for various comparator types
        including explicit markers (like, as), epistemic markers (seem),
        and punctuation-based comparators (colon, semicolon).
        
        Args:
            tokens: List of spaCy tokens
            comparator_type: Type of comparator to locate
            
        Returns:
            Optional[int]: Position index of comparator, or None if not found
        """
        # Handle different comparator types
        if comparator_type in ['like', 'as']:
            for i, token in enumerate(tokens):
                if token.text.lower() == comparator_type:
                    return i
        
        elif comparator_type == 'as if':
            for i in range(len(tokens) - 1):
                if (tokens[i].text.lower() == 'as' and 
                    tokens[i + 1].text.lower() == 'if'):
                    return i
        
        elif comparator_type in ['seemed', 'seem', 'seem(ed)']:
            for i, token in enumerate(tokens):
                if token.lemma_.lower() == 'seem':
                    return i
        
        elif comparator_type in ['colon', 'semicolon', ':', ';']:
            # For punctuation comparators, look in original sentence
            sentence = ' '.join([t.text for t in tokens])
            if ':' in sentence:
                # Approximate position based on text structure
                return len(tokens) // 2
            elif ';' in sentence:
                return len(tokens) // 2
        
        # Pattern matching for complex comparators
        elif 'as' in comparator_type and 'as' in comparator_type:
            # Handle "as ADJ as" patterns
            for i in range(len(tokens) - 2):
                if (tokens[i].text.lower() == 'as' and 
                    tokens[i + 2].text.lower() == 'as'):
                    return i + 1  # Return position of middle element
        
        return None
    
    def perform_topic_modeling(self, datasets: Dict[str, pd.DataFrame], n_topics: int = 8) -> Dict:
        """
        Apply topic modeling across all datasets for thematic analysis.
        
        Implements Latent Dirichlet Allocation to identify thematic patterns
        in simile usage across Joyce's work and BNC baseline, enabling
        semantic comparison beyond structural features.
        
        Args:
            datasets: Dictionary of processed datasets
            n_topics: Number of topics for LDA model
            
        Returns:
            Dict: Topic model results and assignments
        """
        logger.info(f"Performing topic modeling analysis with {n_topics} topics")
        
        # Combine all sentence contexts for comprehensive topic discovery
        all_texts = []
        dataset_indices = []
        
        for dataset_name, df in datasets.items():
            texts = df['Sentence_Context'].tolist()
            all_texts.extend(texts)
            dataset_indices.extend([dataset_name] * len(texts))
        
        # Vectorization with academic text preprocessing
        self.vectorizer = TfidfVectorizer(
            max_features=200,
            stop_words='english',
            lowercase=True,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )
        
        text_matrix = self.vectorizer.fit_transform(all_texts)
        
        # Topic modeling with optimized parameters
        self.topic_model = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            max_iter=100,
            learning_method='batch'
        )
        
        topic_assignments = self.topic_model.fit_transform(text_matrix)
        dominant_topics = topic_assignments.argmax(axis=1)
        
        # Generate topic labels
        feature_names = self.vectorizer.get_feature_names_out()
        topic_labels = []
        
        for topic_idx in range(n_topics):
            top_words = [
                feature_names[i] 
                for i in self.topic_model.components_[topic_idx].argsort()[-4:]
            ]
            topic_labels.append(f"Topic_{topic_idx}: {', '.join(reversed(top_words))}")
        
        # Assign topics back to datasets
        start_idx = 0
        for dataset_name, df in datasets.items():
            end_idx = start_idx + len(df)
            dataset_topics = dominant_topics[start_idx:end_idx]
            df['Topic_Assignment'] = [topic_labels[topic] for topic in dataset_topics]
            df['Topic_Probability'] = topic_assignments[start_idx:end_idx].max(axis=1)
            start_idx = end_idx
        
        logger.info("Topic modeling completed successfully")
        
        return {
            'model': self.topic_model,
            'vectorizer': self.vectorizer,
            'topic_labels': topic_labels,
            'n_topics': n_topics
        }

# =============================================================================
# DATA CLEANING AND STANDARDIZATION UTILITIES
# =============================================================================

class DataStandardizer:
    """
    Implements data cleaning and standardization procedures for cross-dataset
    consistency. Ensures uniform data structures and category mappings for
    robust comparative analysis.
    """
    
    @staticmethod
    def standardize_categories(df: pd.DataFrame, category_column: str = 'Category_Framework') -> pd.DataFrame:
        """
        Standardize theoretical framework categories across datasets.
        
        Maps variant category names to consistent framework labels,
        enabling robust cross-dataset comparison and statistical analysis.
        
        Args:
            df: Input dataset
            category_column: Name of category column to standardize
            
        Returns:
            pd.DataFrame: Dataset with standardized categories
        """
        category_mapping = {
            # Standard similes
            'Standard': 'Standard',
            'standard': 'Standard',
            
            # Quasi similes
            'Quasi': 'Quasi',
            'Joycean_Quasi': 'Quasi',
            'quasi': 'Quasi',
            
            # Joycean innovations
            'Joycean_Silent': 'Joycean_Silent',
            'joycean_silent': 'Joycean_Silent',
            
            'Joycean_Hybrid': 'Joycean_Hybrid',
            'Joycean_Quasi_Fuzzy': 'Joycean_Hybrid',
            'joycean_hybrid': 'Joycean_Hybrid',
            
            'Joycean_Framed': 'Joycean_Framed',
            'joycean_framed': 'Joycean_Framed',
            
            'Joycean': 'Joycean_Complex',
            'joycean': 'Joycean_Complex'
        }
        
        df[category_column] = df[category_column].map(category_mapping).fillna(df[category_column])
        
        return df
    
    @staticmethod
    def clean_text_data(df: pd.DataFrame, text_column: str = 'Sentence_Context') -> pd.DataFrame:
        """
        Clean and normalize text data for consistent processing.
        
        Implements standard text preprocessing while preserving literary
        features essential for stylistic analysis.
        
        Args:
            df: Input dataset
            text_column: Name of text column to clean
            
        Returns:
            pd.DataFrame: Dataset with cleaned text
        """
        # Remove excessive whitespace while preserving sentence structure
        df[text_column] = df[text_column].str.replace(r'\s+', ' ', regex=True)
        df[text_column] = df[text_column].str.strip()
        
        # Remove empty or invalid entries
        df = df[df[text_column].notna()]
        df = df[df[text_column].str.len() > 10]  # Minimum viable sentence length
        
        return df

# =============================================================================
# MAIN PROCESSING PIPELINE
# =============================================================================

def main_processing_pipeline():
    """
    Execute the complete data processing pipeline for simile analysis.
    
    Orchestrates data loading, linguistic processing, and standardization
    across all three datasets (manual, computational, BNC), preparing
    standardized datasets for statistical analysis and visualization.
    
    Returns:
        Dict: Processed datasets ready for analysis
    """
    logger.info("Initiating comprehensive data processing pipeline")
    
    # Initialize processors
    data_processor = SimileDataProcessor()
    linguistic_analyzer = LinguisticAnalyzer()
    
    # Load and process all datasets
    logger.info("Phase 1: Dataset Loading and Initial Processing")
    
    # Load manual annotations (gold standard)
    manual_df = data_processor.load_manual_annotations('All Similes  Dubliners contSheet1.csv')
    
    # Load computational extractions
    computational_df = data_processor.load_computational_extractions('dubliners_complete_enhanced_similes.csv')
    
    # Load BNC baseline
    bnc_df = data_processor.load_bnc_concordances('concordance from BNC.csv')
    
    # Standardize data structures
    logger.info("Phase 2: Data Standardization and Cleaning")
    
    datasets = {
        'manual': DataStandardizer.standardize_categories(manual_df),
        'computational': DataStandardizer.standardize_categories(computational_df),
        'bnc': DataStandardizer.standardize_categories(bnc_df)
    }
    
    # Clean text data
    for name, df in datasets.items():
        datasets[name] = DataStandardizer.clean_text_data(df)
    
    # Apply linguistic processing
    logger.info("Phase 3: Comprehensive Linguistic Analysis")
    
    for dataset_name, df in datasets.items():
        datasets[dataset_name] = linguistic_analyzer.process_dataset_linguistics(df, dataset_name)
    
    # Perform topic modeling
    logger.info("Phase 4: Cross-Dataset Topic Modeling")
    
    topic_results = linguistic_analyzer.perform_topic_modeling(datasets)
    
    # Export processed datasets
    logger.info("Phase 5: Dataset Export and Metadata Generation")
    
    for dataset_name, df in datasets.items():
        output_filename = f'processed_{dataset_name}_dataset.csv'
        df.to_csv(output_filename, index=False)
        logger.info(f"Exported processed dataset: {output_filename}")
    
    # Generate processing summary
    summary = {
        'datasets': datasets,
        'topic_modeling': topic_results,
        'metadata': data_processor.dataset_metadata,
        'processing_timestamp': pd.Timestamp.now(),
        'total_instances': sum(len(df) for df in datasets.values())
    }
    
    logger.info(f"Data processing pipeline completed successfully")
    logger.info(f"Total instances processed: {summary['total_instances']}")
    
    return summary

# =============================================================================
# EXECUTION
# =============================================================================

if __name__ == "__main__":
    # Verify input data files are available (only the 2 we need to upload)
    required_input_files = [
        'All Similes  Dubliners contSheet1.csv',  # Manual annotations
        'concordance from BNC.csv'                 # BNC baseline data
    ]
    
    missing_files = []
    for file in required_input_files:
        if not os.path.exists(file):
            missing_files.append(file)
    
    if missing_files:
        print("WARNING: Missing required input data files:")
        for file in missing_files:
            print(f"  ❌ {file}")
        print("\nPlease upload these files to your repository:")
        print("  1. Your manual annotations CSV")
        print("  2. Your BNC concordance CSV")
        print("\nThe third dataset (computational extractions) will be generated by this notebook.")
    else:
        print("All required input files found ✓")
        print("  ✓ Manual annotations ready")
        print("  ✓ BNC baseline data ready") 
        print("  → Computational extractions will be generated")
        
        # Execute the complete processing pipeline
        results = main_processing_pipeline()
        
        # Display processing summary
        print("\n" + "="*80)
        print("DATA PROCESSING PIPELINE SUMMARY")
        print("="*80)
        
        for dataset_name, metadata in results['metadata'].items():
            print(f"\n{dataset_name.upper()} DATASET:")
            print(f"  Total instances: {metadata['total_instances']}")
            if 'stories_covered' in metadata:
                print(f"  Stories covered: {metadata['stories_covered']}")
            print(f"  Categories found: {metadata['categories_found']}")
        
        print(f"\nTopic modeling completed with {results['topic_modeling']['n_topics']} topics")
        print(f"Processing completed at: {results['processing_timestamp']}")
        
        print("\nProcessed datasets exported:")
        print("  - processed_manual_dataset.csv")
        print("  - processed_computational_dataset.csv") 
        print("  - processed_bnc_dataset.csv")
        
        print("\nReady for statistical analysis and F1 validation (Notebook 2)")
        
        # Save results for next notebook
        import pickle
        with open('processing_results.pkl', 'wb') as f:
            pickle.dump(results, f)
        print("\nProcessing results saved for Notebook 2")

# =============================================================================
# COLAB FILE MANAGEMENT UTILITIES
# =============================================================================

def upload_to_github():
    """
    Utility function to commit and push results to GitHub repository.
    Run this after processing to save your results.
    """
    print("Uploading results to GitHub repository...")
    
    # Configure git (replace with your details)
    !git config --global user.email "your.email@ucl.ac.uk"
    !git config --global user.name "Your Name"
    
    # Add processed files
    !git add processed_*.csv processing_results.pkl simile_analysis.log
    
    # Commit with timestamp
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    !git commit -m "Data processing completed - {timestamp}"
    
    # Push to repository
    !git push origin main
    
    print("Results uploaded to GitHub successfully!")

def download_from_github():
    """
    Utility function to download latest data files from GitHub repository.
    Run this to get the most recent datasets.
    """
    print("Downloading latest files from GitHub...")
    !git pull origin main
    print("Files updated from repository!")

# Uncomment to use these utilities:
# upload_to_github()  # After processing
# download_from_github()  # To get latest files
