{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Analysis of Simile Structures in Joyce's Dubliners\n",
    "## Notebook 1: Data Processing and Linguistic Analysis\n",
    "\n",
    "**Master's Dissertation Research - University College London**\n",
    "\n",
    "This notebook implements the data processing pipeline for computational analysis of simile structures across three datasets: manual annotations, computational extractions, and BNC baseline corpus. The analysis extends the theoretical framework of Leech & Short (1981) with novel Joycean categories.\n",
    "\n",
    "**Repository:** https://github.com/[username]/joyce-dubliners-similes-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup and GitHub Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for file access (if using Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully\")\n",
    "    \n",
    "    # Set working directory to your project folder\n",
    "    import os\n",
    "    os.chdir('/content/drive/MyDrive/joyce-dubliners-similes-analysis')\n",
    "    print(\"Working directory set to project folder\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Not running in Colab - skipping Drive mount\")\n",
    "\n",
    "# For GitHub integration (run once to get initial files)\n",
    "# Uncomment the following lines on first run:\n",
    "# !git clone https://github.com/[username]/joyce-dubliners-similes-analysis.git\n",
    "# %cd joyce-dubliners-similes-analysis\n",
    "\n",
    "# For subsequent runs, pull latest changes:\n",
    "# !git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install spacy textblob scikit-learn -q\n",
    "!python -m spacy download en_core_web_lg -q\n",
    "\n",
    "# Verify file structure\n",
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nProject files:\")\n",
    "for file in os.listdir('.'):\n",
    "    if file.endswith(('.csv', '.py', '.ipynb')):\n",
    "        print(f\"  ✓ {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import logging\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Configure academic logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('simile_analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress non-critical warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Initialize spaCy language model for linguistic processing\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    logger.info(\"SpaCy English language model initialized successfully\")\n",
    "except OSError:\n",
    "    logger.error(\"SpaCy model not found. Install with: python -m spacy download en_core_web_lg\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Standardization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimileDataProcessor:\n",
    "    \"\"\"\n",
    "    Handles loading, cleaning, and standardization of simile datasets for\n",
    "    comparative analysis. Implements consistent data structures across\n",
    "    manual annotations, computational extractions, and BNC baseline corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the data processor with configuration parameters.\"\"\"\n",
    "        self.nlp = nlp\n",
    "        self.processed_datasets = {}\n",
    "        self.dataset_metadata = {}\n",
    "        \n",
    "        # Define theoretical framework categories for consistency\n",
    "        self.theoretical_categories = {\n",
    "            'standard': ['Standard', 'standard'],\n",
    "            'quasi': ['Quasi', 'Joycean_Quasi', 'quasi'],\n",
    "            'joycean_silent': ['Joycean_Silent', 'joycean_silent'],\n",
    "            'joycean_hybrid': ['Joycean_Hybrid', 'Joycean_Quasi_Fuzzy', 'joycean_hybrid'],\n",
    "            'joycean_framed': ['Joycean_Framed', 'joycean_framed'],\n",
    "            'joycean_complex': ['Joycean', 'joycean']\n",
    "        }\n",
    "        \n",
    "        logger.info(\"SimileDataProcessor initialized with theoretical framework categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load_manual_annotations(self, filepath: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load and process manual annotations from close reading analysis.\n",
    "        \n",
    "        This dataset serves as the gold standard for algorithmic validation,\n",
    "        containing expert-annotated similes with theoretical categorization\n",
    "        based on extended Leech & Short framework.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to manual annotations CSV file\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Processed manual annotations dataset\n",
    "        \"\"\"\n",
    "        logger.info(\"Loading manual annotations dataset\")\n",
    "        \n",
    "        try:\n",
    "            # Load with appropriate encoding for academic text\n",
    "            manual_df = pd.read_csv(filepath, encoding='cp1252')\n",
    "            \n",
    "            # Standardize column names for consistency\n",
    "            column_mapping = {\n",
    "                'Category (Framwrok)': 'Category_Framework',  # Fix typo in original\n",
    "                'Comparator Type ': 'Comparator_Type',  # Remove trailing space\n",
    "                'Sentence Context': 'Sentence_Context',\n",
    "                'Additional Notes': 'Additional_Notes',\n",
    "                'Page No.': 'Page_Number'\n",
    "            }\n",
    "            manual_df = manual_df.rename(columns=column_mapping)\n",
    "            \n",
    "            # Clean and validate data\n",
    "            manual_df = self._clean_manual_annotations(manual_df)\n",
    "            \n",
    "            # Add dataset identifier\n",
    "            manual_df['Dataset_Source'] = 'Manual_Annotation'\n",
    "            manual_df['Analysis_Method'] = 'Close_Reading'\n",
    "            \n",
    "            # Store metadata\n",
    "            self.dataset_metadata['manual'] = {\n",
    "                'total_instances': len(manual_df),\n",
    "                'stories_covered': manual_df['Story'].nunique(),\n",
    "                'categories_found': manual_df['Category_Framework'].nunique(),\n",
    "                'date_processed': pd.Timestamp.now()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Manual annotations loaded: {len(manual_df)} instances across {manual_df['Story'].nunique()} stories\")\n",
    "            \n",
    "            return manual_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load manual annotations: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _clean_manual_annotations(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and validate manual annotations data.\"\"\"\n",
    "        # Remove empty rows\n",
    "        df = df.dropna(subset=['Sentence_Context'])\n",
    "        \n",
    "        # Clean text fields\n",
    "        df['Sentence_Context'] = df['Sentence_Context'].str.strip()\n",
    "        \n",
    "        # Validate categories\n",
    "        df = df[df['Category_Framework'].notna()]\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Verification and Main Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify input data files are available (only the 2 we need to upload)\n",
    "required_input_files = [\n",
    "    'All Similes  Dubliners contSheet1.csv',  # Manual annotations\n",
    "    'concordance from BNC.csv'                 # BNC baseline data\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for file in required_input_files:\n",
    "    if not os.path.exists(file):\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"WARNING: Missing required input data files:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"  ❌ {file}\")\n",
    "    print(\"\\nPlease upload these files to your repository:\")\n",
    "    print(\"  1. Your manual annotations CSV\")\n",
    "    print(\"  2. Your BNC concordance CSV\")\n",
    "    print(\"\\nThe third dataset (computational extractions) will be generated by this notebook.\")\n",
    "else:\n",
    "    print(\"All required input files found ✓\")\n",
    "    print(\"  ✓ Manual annotations ready\")\n",
    "    print(\"  ✓ BNC baseline data ready\") \n",
    "    print(\"  → Computational extractions will be generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test - Load Your Manual Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to load and examine your manual annotations\n",
    "if os.path.exists('All Similes  Dubliners contSheet1.csv'):\n",
    "    try:\n",
    "        # Initialize processor\n",
    "        processor = SimileDataProcessor()\n",
    "        \n",
    "        # Load manual annotations\n",
    "        manual_data = processor.load_manual_annotations('All Similes  Dubliners contSheet1.csv')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MANUAL ANNOTATIONS DATASET SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Total similes: {len(manual_data)}\")\n",
    "        print(f\"Stories covered: {manual_data['Story'].nunique()}\")\n",
    "        print(f\"Categories found: {manual_data['Category_Framework'].nunique()}\")\n",
    "        \n",
    "        print(\"\\nCategory distribution:\")\n",
    "        category_counts = manual_data['Category_Framework'].value_counts()\n",
    "        for category, count in category_counts.items():\n",
    "            percentage = (count / len(manual_data)) * 100\n",
    "            print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(\"\\nSample entries:\")\n",
    "        for i, (_, row) in enumerate(manual_data.head(3).iterrows()):\n",
    "            print(f\"\\n{i+1}. {row['ID']} ({row['Story']})\")\n",
    "            print(f\"   Text: {row['Sentence_Context'][:80]}...\")\n",
    "            print(f\"   Category: {row['Category_Framework']}\")\n",
    "            print(f\"   Comparator: {row['Comparator_Type']}\")\n",
    "        \n",
    "        print(\"\\n✅ Manual annotations loaded successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading manual annotations: {e}\")\n",
    "else:\n",
    "    print(\"❌ Manual annotations file not found\")\n",
    "    print(\"Please upload 'All Similes  Dubliners contSheet1.csv' to your repository\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Instructions\n",
    "\n",
    "**To complete the setup:**\n",
    "\n",
    "1. **Upload your 2 CSV files** to this Colab environment or your GitHub repository:\n",
    "   - `All Similes  Dubliners contSheet1.csv` (your manual annotations)\n",
    "   - `concordance from BNC.csv` (your BNC baseline data)\n",
    "\n",
    "2. **Re-run the cells above** to verify the files are loaded correctly\n",
    "\n",
    "3. **Continue with the full processing pipeline** once both files are available\n",
    "\n",
    "The notebook will automatically:\n",
    "- Download Dubliners from Project Gutenberg\n",
    "- Run computational simile extraction \n",
    "- Process all three datasets with linguistic analysis\n",
    "- Export processed datasets for Notebook 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
