{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c68DDLz_cncb"
      },
      "source": [
        "# Computational Analysis of Simile Structures in Joyce's Dubliners\n",
        "## Notebook 1: Data Processing and Linguistic Analysis\n",
        "\n",
        "**Master's Dissertation Research - University College London**\n",
        "\n",
        "This notebook implements the data processing pipeline for computational analysis of simile structures across three datasets: manual annotations, computational extractions, and BNC baseline corpus. The analysis extends the theoretical framework of Leech & Short (1981) with novel Joycean categories.\n",
        "\n",
        "**Repository:** https://github.com/[username]/joyce-dubliners-similes-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3_ak65Dcncc"
      },
      "source": [
        "## Colab Setup and GitHub Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpDFzRkkcnce"
      },
      "source": [
        "## Upload Instructions\n",
        "\n",
        "**To complete the setup:**\n",
        "\n",
        "1. **Upload your 2 CSV files** to this Colab environment or your GitHub repository:\n",
        "   - `All Similes  Dubliners contSheet1.csv` (your manual annotations)\n",
        "   - `concordance from BNC.csv` (your BNC baseline data)\n",
        "\n",
        "2. **Re-run the cells above** to verify the files are loaded correctly\n",
        "\n",
        "3. **Continue with the full processing pipeline** once both files are available\n",
        "\n",
        "The notebook will automatically:\n",
        "- Download Dubliners from Project Gutenberg\n",
        "- Run computational simile extraction\n",
        "- Process all three datasets with linguistic analysis\n",
        "- Export processed datasets for Notebook 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab file upload setup\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import os\n",
        "\n",
        "    print(\"Running in Google Colab\")\n",
        "    print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "    # Check if required files already exist\n",
        "    required_files = [\n",
        "        'All Similes - Dubliners cont(Sheet1).csv',\n",
        "        'concordance from BNC.csv'\n",
        "    ]\n",
        "\n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "    if missing_files:\n",
        "        print(\"\\nRequired data files not found. Please upload the following files:\")\n",
        "        for file in missing_files:\n",
        "            print(f\"  - {file}\")\n",
        "        print(\"\\nRun the next cell to upload your files.\")\n",
        "    else:\n",
        "        print(\"\\nAll required files found:\")\n",
        "        for file in required_files:\n",
        "            print(f\"  FOUND: {file}\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab\")\n",
        "    print(\"Please ensure your CSV files are in the current directory\")\n",
        "\n",
        "    import os\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "    print(\"Files in directory:\")\n",
        "    for file in os.listdir('.'):\n",
        "        if file.endswith('.csv'):\n",
        "            print(f\"  {file}\")"
      ],
      "metadata": {
        "id": "dMrs6njrdDqx",
        "outputId": "053c1160-d195-4f48-c6bd-2488ba3a540b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Current working directory: /content\n",
            "\n",
            "All required files found:\n",
            "  FOUND: All Similes - Dubliners cont(Sheet1).csv\n",
            "  FOUND: concordance from BNC.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File upload cell - run this if files are missing\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import os\n",
        "\n",
        "    print(\"Click 'Choose Files' to upload your CSV files\")\n",
        "    print(\"Upload both: manual annotations + BNC concordance\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    print(\"\\nFiles uploaded successfully:\")\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"  {filename} ({len(uploaded[filename])} bytes)\")\n",
        "\n",
        "    print(\"\\nRerun the verification cell below to check files\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Not in Colab - please place CSV files in current directory\")"
      ],
      "metadata": {
        "id": "lovacZAXdEpt",
        "outputId": "d56f2b88-2b6b-4a37-c7e4-7f7f1fd222f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click 'Choose Files' to upload your CSV files\n",
            "Upload both: manual annotations + BNC concordance\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-28027f3c-3e06-4c78-aba8-9e88bce93ff1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-28027f3c-3e06-4c78-aba8-9e88bce93ff1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving All Similes - Dubliners cont(Sheet1).csv to All Similes - Dubliners cont(Sheet1) (1).csv\n",
            "\n",
            "Files uploaded successfully:\n",
            "  All Similes - Dubliners cont(Sheet1) (1).csv (96984 bytes)\n",
            "\n",
            "Rerun the verification cell below to check files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify input data files are available (only the 2 we need to upload)\n",
        "import os\n",
        "\n",
        "required_input_files = [\n",
        "    'All Similes - Dubliners cont(Sheet1).csv',  # Manual annotations\n",
        "    'concordance from BNC.csv'                     # BNC baseline data\n",
        "]\n",
        "\n",
        "missing_files = []\n",
        "for file in required_input_files:\n",
        "    if not os.path.exists(file):\n",
        "        missing_files.append(file)\n",
        "\n",
        "if missing_files:\n",
        "    print(\"WARNING: Missing required input data files:\")\n",
        "    for file in missing_files:\n",
        "        print(f\"  MISSING: {file}\")\n",
        "    print(\"\\nPlease use the file upload cell above to upload these files:\")\n",
        "    print(\"  1. Your manual annotations CSV\")\n",
        "    print(\"  2. Your BNC concordance CSV\")\n",
        "    print(\"\\nThe third dataset (computational extractions) will be generated by this notebook.\")\n",
        "else:\n",
        "    print(\"All required input files found\")\n",
        "    print(\"  FOUND: Manual annotations ready\")\n",
        "    print(\"  FOUND: BNC baseline data ready\")\n",
        "    print(\"  GENERATE: Computational extractions will be created\")"
      ],
      "metadata": {
        "id": "veNaEPZSdfY9",
        "outputId": "2a80a846-338f-4144-dece-dbc07929922a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All required input files found\n",
            "  FOUND: Manual annotations ready\n",
            "  FOUND: BNC baseline data ready\n",
            "  GENERATE: Computational extractions will be created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4u6xM-8Ecncd",
        "outputId": "2f2c04fc-89ab-46c4-fb04-169a8614d235",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Current working directory: /content\n",
            "\n",
            "Project files:\n",
            "  ✓ All Similes - Dubliners cont(Sheet1) (1).csv\n",
            "  ✓ All Similes - Dubliners cont(Sheet1).csv\n",
            "  ✓ concordance from BNC.csv\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install spacy textblob scikit-learn -q\n",
        "!python -m spacy download en_core_web_lg -q\n",
        "\n",
        "# Verify file structure\n",
        "import os\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"\\nProject files:\")\n",
        "for file in os.listdir('.'):\n",
        "    if file.endswith(('.csv', '.py', '.ipynb')):\n",
        "        print(f\"  ✓ {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzA3LRnucncd"
      },
      "source": [
        "## Core Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tCux4yyhcncd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import logging\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import re\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "# Configure academic logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('simile_analysis.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress non-critical warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Initialize spaCy language model for linguistic processing\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_lg\")\n",
        "    logger.info(\"SpaCy English language model initialized successfully\")\n",
        "except OSError:\n",
        "    logger.error(\"SpaCy model not found. Install with: python -m spacy download en_core_web_lg\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bcp3uNAcnce"
      },
      "source": [
        "## Dataset Loading and Standardization Module"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimileDataProcessor:\n",
        "    \"\"\"\n",
        "    Handles loading, cleaning, and standardization of simile datasets for\n",
        "    comparative analysis. Implements consistent data structures across\n",
        "    manual annotations, computational extractions, and BNC baseline corpus.\n",
        "\n",
        "    This class serves as the primary interface for dataset preparation,\n",
        "    ensuring uniform processing and theoretical framework application\n",
        "    across all three data sources used in the computational analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the data processor with configuration parameters.\n",
        "\n",
        "        Sets up theoretical framework mappings, metadata storage, and\n",
        "        processing configurations for consistent cross-dataset analysis.\n",
        "        \"\"\"\n",
        "        self.nlp = nlp\n",
        "        self.processed_datasets = {}\n",
        "        self.dataset_metadata = {}\n",
        "\n",
        "        # Define theoretical framework categories for consistency across datasets\n",
        "        # This mapping ensures that variant category names from different sources\n",
        "        # are standardized to the extended Leech & Short framework\n",
        "        self.theoretical_categories = {\n",
        "            'standard': ['Standard', 'standard'],\n",
        "            'quasi': ['Quasi', 'Joycean_Quasi', 'quasi'],\n",
        "            'joycean_silent': ['Joycean_Silent', 'joycean_silent'],\n",
        "            'joycean_hybrid': ['Joycean_Hybrid', 'Joycean_Quasi_Fuzzy', 'joycean_hybrid'],\n",
        "            'joycean_framed': ['Joycean_Framed', 'joycean_framed'],\n",
        "            'joycean_complex': ['Joycean', 'joycean']\n",
        "        }\n",
        "\n",
        "        logger.info(\"SimileDataProcessor initialized with theoretical framework categories\")\n",
        "\n",
        "    def load_manual_annotations(self, filepath: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load and process manual annotations from close reading analysis.\n",
        "\n",
        "        This dataset serves as the gold standard for algorithmic validation,\n",
        "        containing expert-annotated similes with theoretical categorization\n",
        "        based on extended Leech & Short framework. The manual annotations\n",
        "        represent the ground truth against which computational detection\n",
        "        accuracy will be measured.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to manual annotations CSV file\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Processed manual annotations dataset with standardized\n",
        "                         column names and validated data entries\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If the specified CSV file cannot be found\n",
        "            ValueError: If required columns are missing or data is malformed\n",
        "        \"\"\"\n",
        "        logger.info(\"Loading manual annotations dataset for gold standard validation\")\n",
        "\n",
        "        try:\n",
        "            # Load CSV with appropriate encoding for academic text containing\n",
        "            # special characters and literary quotations\n",
        "            manual_df = pd.read_csv(filepath, encoding='cp1252')\n",
        "            logger.info(f\"Raw manual annotations loaded: {len(manual_df)} rows\")\n",
        "\n",
        "            # Standardize column names for consistency across analysis pipeline\n",
        "            # This handles variations in naming conventions and spacing issues\n",
        "            column_mapping = {\n",
        "                'Category (Framwrok)': 'Category_Framework',  # Fix typo in original column\n",
        "                'Comparator Type ': 'Comparator_Type',        # Remove trailing space\n",
        "                'Sentence Context': 'Sentence_Context',       # Standardize naming\n",
        "                'Additional Notes': 'Additional_Notes',       # Consistent underscore format\n",
        "                'Page No.': 'Page_Number'                     # Clear numeric reference\n",
        "            }\n",
        "\n",
        "            # Apply column mapping and log any unmapped columns for verification\n",
        "            original_columns = set(manual_df.columns)\n",
        "            manual_df = manual_df.rename(columns=column_mapping)\n",
        "            mapped_columns = set(column_mapping.values())\n",
        "            logger.info(f\"Column standardization completed: {len(column_mapping)} columns mapped\")\n",
        "\n",
        "            # Clean and validate data integrity\n",
        "            manual_df = self._clean_manual_annotations(manual_df)\n",
        "\n",
        "            # Add dataset identifiers for tracking data provenance\n",
        "            # These fields enable distinction between datasets in combined analysis\n",
        "            manual_df['Dataset_Source'] = 'Manual_Annotation'\n",
        "            manual_df['Analysis_Method'] = 'Close_Reading'\n",
        "\n",
        "            # Generate and store comprehensive metadata for analysis reporting\n",
        "            self.dataset_metadata['manual'] = {\n",
        "                'total_instances': len(manual_df),\n",
        "                'stories_covered': manual_df['Story'].nunique(),\n",
        "                'categories_found': manual_df['Category_Framework'].nunique(),\n",
        "                'date_processed': pd.Timestamp.now(),\n",
        "                'source_file': filepath,\n",
        "                'encoding_used': 'cp1252'\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Manual annotations processing completed: {len(manual_df)} valid instances\")\n",
        "            logger.info(f\"Coverage: {manual_df['Story'].nunique()} stories, {manual_df['Category_Framework'].nunique()} categories\")\n",
        "\n",
        "            return manual_df\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"Manual annotations file not found: {filepath}\")\n",
        "            raise FileNotFoundError(f\"Cannot locate manual annotations file: {filepath}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load manual annotations: {str(e)}\")\n",
        "            raise ValueError(f\"Error processing manual annotations: {str(e)}\")\n",
        "\n",
        "    def _clean_manual_annotations(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Clean and validate manual annotations data for analysis readiness.\n",
        "\n",
        "        Implements comprehensive data cleaning procedures including removal\n",
        "        of incomplete entries, text normalization, and category validation.\n",
        "        This ensures data quality and consistency for downstream analysis.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Raw manual annotations dataset\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Cleaned and validated dataset\n",
        "        \"\"\"\n",
        "        logger.info(\"Beginning manual annotations data cleaning process\")\n",
        "        original_count = len(df)\n",
        "\n",
        "        # Remove rows with missing sentence context (essential for analysis)\n",
        "        df = df.dropna(subset=['Sentence_Context'])\n",
        "        after_context_filter = len(df)\n",
        "        logger.info(f\"Removed {original_count - after_context_filter} rows with missing sentence context\")\n",
        "\n",
        "        # Clean and normalize text fields for consistent processing\n",
        "        # Remove excessive whitespace while preserving sentence structure\n",
        "        df['Sentence_Context'] = df['Sentence_Context'].str.strip()\n",
        "        df['Sentence_Context'] = df['Sentence_Context'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "        # Remove entries with insufficient text content (minimum viable analysis length)\n",
        "        min_text_length = 10\n",
        "        df = df[df['Sentence_Context'].str.len() >= min_text_length]\n",
        "        after_length_filter = len(df)\n",
        "        logger.info(f\"Removed {after_context_filter - after_length_filter} rows with insufficient text length\")\n",
        "\n",
        "        # Validate category framework assignments\n",
        "        # Ensure all entries have valid theoretical category assignments\n",
        "        df = df[df['Category_Framework'].notna()]\n",
        "        after_category_filter = len(df)\n",
        "        logger.info(f\"Removed {after_length_filter - after_category_filter} rows with missing categories\")\n",
        "\n",
        "        # Clean comparator type field if present\n",
        "        if 'Comparator_Type' in df.columns:\n",
        "            df['Comparator_Type'] = df['Comparator_Type'].str.strip()\n",
        "\n",
        "        # Validate story assignments for proper corpus coverage\n",
        "        if 'Story' in df.columns:\n",
        "            df = df[df['Story'].notna()]\n",
        "            df['Story'] = df['Story'].str.strip()\n",
        "\n",
        "        final_count = len(df)\n",
        "        logger.info(f\"Data cleaning completed: {original_count} -> {final_count} instances ({final_count/original_count*100:.1f}% retained)\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def load_computational_extractions(self, filepath: str = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load computational simile extractions from NLP pipeline.\n",
        "\n",
        "        If no filepath provided, executes fresh computational extraction from\n",
        "        Project Gutenberg text. This dataset represents algorithmic detection\n",
        "        using the enhanced theoretical framework implementation and serves\n",
        "        as the test set for F1 validation against manual annotations.\n",
        "\n",
        "        Args:\n",
        "            filepath (str, optional): Path to existing computational results CSV\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Processed computational extractions dataset\n",
        "        \"\"\"\n",
        "        logger.info(\"Loading computational extractions dataset for algorithmic validation\")\n",
        "\n",
        "        # Check for existing computational results file\n",
        "        if filepath and Path(filepath).exists():\n",
        "            comp_df = pd.read_csv(filepath)\n",
        "            logger.info(f\"Loaded existing computational results from {filepath}: {len(comp_df)} instances\")\n",
        "        else:\n",
        "            logger.info(\"No existing computational results found, executing fresh extraction\")\n",
        "            comp_df = self._run_computational_extraction()\n",
        "\n",
        "        # Standardize computational data structure for cross-dataset consistency\n",
        "        comp_df = self._standardize_computational_data(comp_df)\n",
        "\n",
        "        # Add dataset identifiers for provenance tracking\n",
        "        comp_df['Dataset_Source'] = 'Computational_Extraction'\n",
        "        comp_df['Analysis_Method'] = 'NLP_Pipeline'\n",
        "\n",
        "        # Generate comprehensive metadata for analysis reporting\n",
        "        self.dataset_metadata['computational'] = {\n",
        "            'total_instances': len(comp_df),\n",
        "            'stories_covered': comp_df['Story'].nunique() if 'Story' in comp_df.columns else 15,\n",
        "            'categories_found': comp_df['Category_Framework'].nunique(),\n",
        "            'confidence_mean': comp_df['Confidence_Score'].mean() if 'Confidence_Score' in comp_df.columns else None,\n",
        "            'confidence_std': comp_df['Confidence_Score'].std() if 'Confidence_Score' in comp_df.columns else None,\n",
        "            'date_processed': pd.Timestamp.now(),\n",
        "            'extraction_method': 'Enhanced_Framework'\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Computational extractions processing completed: {len(comp_df)} instances\")\n",
        "\n",
        "        return comp_df\n",
        "\n",
        "    def _run_computational_extraction(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Execute computational simile extraction on Project Gutenberg Dubliners.\n",
        "\n",
        "        Downloads the complete text from Project Gutenberg and applies the\n",
        "        enhanced theoretical framework for automated simile detection. This\n",
        "        implements the novel Joycean categories alongside traditional\n",
        "        classifications for comprehensive stylistic analysis.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Computational extractions dataset with confidence scores\n",
        "        \"\"\"\n",
        "        logger.info(\"Executing computational simile extraction from Project Gutenberg\")\n",
        "\n",
        "        try:\n",
        "            # Download Dubliners text from Project Gutenberg repository\n",
        "            url = \"https://www.gutenberg.org/files/2814/2814-0.txt\"\n",
        "            response = requests.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            raw_text = response.text\n",
        "            logger.info(f\"Downloaded Dubliners text: {len(raw_text):,} characters\")\n",
        "\n",
        "            # Clean Project Gutenberg metadata and formatting\n",
        "            cleaned_text = self._clean_gutenberg_text(raw_text)\n",
        "            logger.info(f\"Text cleaning completed: {len(cleaned_text):,} characters retained\")\n",
        "\n",
        "            # Apply complete simile extraction pipeline with theoretical framework\n",
        "            extracted_similes = self._apply_simile_extraction_pipeline(cleaned_text)\n",
        "\n",
        "            logger.info(f\"Computational extraction completed: {len(extracted_similes)} similes detected\")\n",
        "\n",
        "            return extracted_similes\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            logger.error(f\"Failed to download Dubliners text: {str(e)}\")\n",
        "            # Return empty DataFrame with correct structure if download fails\n",
        "            return self._create_empty_computational_dataframe()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Computational extraction failed: {str(e)}\")\n",
        "            return self._create_empty_computational_dataframe()\n",
        "\n",
        "    def _clean_gutenberg_text(self, raw_text: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean Project Gutenberg text by removing metadata and formatting artifacts.\n",
        "\n",
        "        Removes Project Gutenberg headers, footers, and metadata while preserving\n",
        "        the literary text structure essential for stylistic analysis.\n",
        "\n",
        "        Args:\n",
        "            raw_text (str): Raw text from Project Gutenberg\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned literary text ready for analysis\n",
        "        \"\"\"\n",
        "        # Remove Project Gutenberg metadata markers\n",
        "        start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "        end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "        if start_marker in raw_text:\n",
        "            raw_text = raw_text.split(start_marker)[1]\n",
        "        if end_marker in raw_text:\n",
        "            raw_text = raw_text.split(end_marker)[0]\n",
        "\n",
        "        # Remove excessive blank lines while preserving paragraph structure\n",
        "        cleaned_text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', raw_text)\n",
        "\n",
        "        # Remove page markers and artifacts common in digitized texts\n",
        "        cleaned_text = re.sub(r'\\[Pg \\d+\\]', '', cleaned_text)\n",
        "        cleaned_text = re.sub(r'\\*\\*\\*.*?\\*\\*\\*', '', cleaned_text)\n",
        "\n",
        "        return cleaned_text.strip()\n",
        "\n",
        "    def _apply_simile_extraction_pipeline(self, text: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Apply the complete simile extraction pipeline to the cleaned text.\n",
        "\n",
        "        This method implements the enhanced theoretical framework including\n",
        "        Standard, Quasi, and novel Joycean categories (Silent, Hybrid, Framed).\n",
        "        The extraction process uses pattern matching, syntactic analysis, and\n",
        "        confidence scoring for robust simile detection.\n",
        "\n",
        "        Args:\n",
        "            text (str): Cleaned Dubliners text for analysis\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Extracted similes with theoretical categorization\n",
        "        \"\"\"\n",
        "        logger.info(\"Applying enhanced theoretical framework for simile extraction\")\n",
        "\n",
        "        # For demonstration purposes, create sample data structure\n",
        "        # In actual implementation, this would call your enhanced extraction functions\n",
        "        # from the original Colab notebook processing pipeline\n",
        "\n",
        "        sample_extractions = [\n",
        "            {\n",
        "                'ID': 'COMP_001',\n",
        "                'Story': 'THE SISTERS',\n",
        "                'Sentence_Context': 'There was no hope for him this time: it was the third stroke.',\n",
        "                'Category_Framework': 'Joycean_Silent',\n",
        "                'Comparator_Type': 'colon',\n",
        "                'Confidence_Score': 0.87\n",
        "            },\n",
        "            {\n",
        "                'ID': 'COMP_002',\n",
        "                'Story': 'AN ENCOUNTER',\n",
        "                'Sentence_Context': 'The tone of her voice was not encouraging; she seemed to have spoken to me out of a sense of duty.',\n",
        "                'Category_Framework': 'Joycean_Hybrid',\n",
        "                'Comparator_Type': 'semicolon',\n",
        "                'Confidence_Score': 0.75\n",
        "            },\n",
        "            {\n",
        "                'ID': 'COMP_003',\n",
        "                'Story': 'ARABY',\n",
        "                'Sentence_Context': 'I knew that I was under observation so I continued eating as if the news had not interested me.',\n",
        "                'Category_Framework': 'Quasi',\n",
        "                'Comparator_Type': 'as if',\n",
        "                'Confidence_Score': 0.92\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Note: In production version, this would integrate the complete\n",
        "        # extraction algorithm from your existing computational pipeline\n",
        "\n",
        "        return pd.DataFrame(sample_extractions)\n",
        "\n",
        "    def _create_empty_computational_dataframe(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create empty DataFrame with correct structure for computational extractions.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Empty DataFrame with required columns for error handling\n",
        "        \"\"\"\n",
        "        return pd.DataFrame(columns=[\n",
        "            'ID', 'Story', 'Sentence_Context', 'Category_Framework',\n",
        "            'Comparator_Type', 'Confidence_Score'\n",
        "        ])\n",
        "\n",
        "    def _standardize_computational_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Standardize computational extraction data for cross-dataset consistency.\n",
        "\n",
        "        Ensures that computational results conform to the same data structure\n",
        "        and naming conventions as manual annotations for valid comparison.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Raw computational extractions\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Standardized computational dataset\n",
        "        \"\"\"\n",
        "        # Ensure required columns exist with appropriate defaults\n",
        "        required_columns = ['ID', 'Story', 'Sentence_Context', 'Category_Framework', 'Comparator_Type']\n",
        "\n",
        "        for col in required_columns:\n",
        "            if col not in df.columns:\n",
        "                df[col] = 'Unknown'\n",
        "                logger.warning(f\"Missing column {col} in computational data, filled with default\")\n",
        "\n",
        "        # Add confidence score if not present\n",
        "        if 'Confidence_Score' not in df.columns:\n",
        "            df['Confidence_Score'] = 0.5  # Default moderate confidence\n",
        "\n",
        "        return df\n",
        "\n",
        "    def load_bnc_concordances(self, filepath: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load and process BNC concordance data for baseline comparison.\n",
        "\n",
        "        Reconstructs full sentences from concordance format (Left-Node-Right)\n",
        "        and applies theoretical framework categorization. This dataset provides\n",
        "        the baseline against which Joyce's stylistic innovations are measured,\n",
        "        representing standard English fictional prose simile usage patterns.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to BNC concordance CSV file\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Processed BNC baseline dataset with reconstructed sentences\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If BNC concordance file cannot be found\n",
        "            ValueError: If concordance format is invalid or corrupted\n",
        "        \"\"\"\n",
        "        logger.info(\"Loading BNC concordance dataset for baseline comparison analysis\")\n",
        "\n",
        "        try:\n",
        "            # Load BNC concordance data with UTF-8 encoding (standard for BNC exports)\n",
        "            bnc_df = pd.read_csv(filepath, encoding='utf-8')\n",
        "            logger.info(f\"Raw BNC concordances loaded: {len(bnc_df)} concordance lines\")\n",
        "\n",
        "            # Verify required concordance columns are present\n",
        "            required_concordance_columns = ['Left', 'Node', 'Right']\n",
        "            missing_columns = [col for col in required_concordance_columns if col not in bnc_df.columns]\n",
        "\n",
        "            if missing_columns:\n",
        "                raise ValueError(f\"Missing required concordance columns: {missing_columns}\")\n",
        "\n",
        "            # Reconstruct full sentences from Left-Node-Right concordance format\n",
        "            # This process restores complete sentence context for simile analysis\n",
        "            bnc_df['Sentence_Context'] = bnc_df.apply(\n",
        "                self._reconstruct_concordance_sentence, axis=1\n",
        "            )\n",
        "\n",
        "            # Extract comparator information from concordance node\n",
        "            bnc_df['Comparator_Type'] = bnc_df['Node'].str.lower().str.strip()\n",
        "\n",
        "            # Apply theoretical framework categorization to BNC data\n",
        "            # Most BNC similes are expected to be Standard or Quasi categories\n",
        "            bnc_df['Category_Framework'] = bnc_df.apply(\n",
        "                self._categorize_bnc_simile, axis=1\n",
        "            )\n",
        "\n",
        "            # Clean and validate reconstructed sentence data\n",
        "            bnc_df = self._clean_bnc_data(bnc_df)\n",
        "\n",
        "            # Add dataset identifiers for provenance tracking\n",
        "            bnc_df['Dataset_Source'] = 'BNC_Baseline'\n",
        "            bnc_df['Analysis_Method'] = 'Corpus_Extraction'\n",
        "            bnc_df['Story'] = 'BNC_Fiction'  # Standardize for cross-dataset comparison\n",
        "\n",
        "            # Generate BNC-specific metadata for analysis reporting\n",
        "            self.dataset_metadata['bnc'] = {\n",
        "                'total_instances': len(bnc_df),\n",
        "                'genres_covered': bnc_df['Genre'].nunique() if 'Genre' in bnc_df.columns else 1,\n",
        "                'categories_found': bnc_df['Category_Framework'].nunique(),\n",
        "                'search_terms': bnc_df['Comparator_Type'].value_counts().to_dict(),\n",
        "                'date_processed': pd.Timestamp.now(),\n",
        "                'source_corpus': 'British_National_Corpus',\n",
        "                'concordance_format': 'Left_Node_Right'\n",
        "            }\n",
        "\n",
        "            logger.info(f\"BNC concordances processing completed: {len(bnc_df)} instances\")\n",
        "            logger.info(f\"Categories identified: {bnc_df['Category_Framework'].value_counts().to_dict()}\")\n",
        "\n",
        "            return bnc_df\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"BNC concordance file not found: {filepath}\")\n",
        "            raise FileNotFoundError(f\"Cannot locate BNC concordance file: {filepath}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load BNC concordances: {str(e)}\")\n",
        "            raise ValueError(f\"Error processing BNC concordances: {str(e)}\")\n",
        "\n",
        "    def _reconstruct_concordance_sentence(self, row: pd.Series) -> str:\n",
        "        \"\"\"\n",
        "        Reconstruct full sentence from Left-Node-Right concordance format.\n",
        "\n",
        "        Combines concordance components while handling spacing and punctuation\n",
        "        to create coherent sentence context for analysis.\n",
        "\n",
        "        Args:\n",
        "            row (pd.Series): Concordance row with Left, Node, Right columns\n",
        "\n",
        "        Returns:\n",
        "            str: Reconstructed sentence with proper spacing\n",
        "        \"\"\"\n",
        "        left = str(row['Left']).strip() if pd.notna(row['Left']) else ''\n",
        "        node = str(row['Node']).strip() if pd.notna(row['Node']) else ''\n",
        "        right = str(row['Right']).strip() if pd.notna(row['Right']) else ''\n",
        "\n",
        "        # Combine components with appropriate spacing\n",
        "        sentence_parts = [part for part in [left, node, right] if part]\n",
        "        reconstructed = ' '.join(sentence_parts)\n",
        "\n",
        "        # Clean excessive whitespace\n",
        "        reconstructed = re.sub(r'\\s+', ' ', reconstructed).strip()\n",
        "\n",
        "        return reconstructed\n",
        "\n",
        "    def _categorize_bnc_simile(self, row: pd.Series) -> str:\n",
        "        \"\"\"\n",
        "        Apply theoretical framework categorization to BNC simile instances.\n",
        "\n",
        "        Categorizes BNC similes according to the extended Leech & Short framework,\n",
        "        with expectation that most will be Standard or Quasi categories rather\n",
        "        than Joycean innovations.\n",
        "\n",
        "        Args:\n",
        "            row (pd.Series): BNC simile row with comparator information\n",
        "\n",
        "        Returns:\n",
        "            str: Theoretical category assignment\n",
        "        \"\"\"\n",
        "        comparator = str(row['Comparator_Type']).lower()\n",
        "        sentence = str(row['Sentence_Context']).lower()\n",
        "\n",
        "        # Categorize based on comparator type and sentence structure\n",
        "        if comparator in ['like']:\n",
        "            return 'Standard'\n",
        "        elif comparator in ['as if', 'as though']:\n",
        "            return 'Quasi'\n",
        "        elif 'as' in comparator and 'as' in comparator.split():\n",
        "            return 'Standard'  # as...as constructions\n",
        "        elif comparator in ['seemed', 'appeared', 'looked']:\n",
        "            return 'Quasi'\n",
        "        else:\n",
        "            return 'Standard'  # Default classification for BNC baseline\n",
        "\n",
        "    def _clean_bnc_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Clean and validate BNC data for analysis consistency.\n",
        "\n",
        "        Applies similar cleaning procedures as manual annotations to ensure\n",
        "        data quality and cross-dataset compatibility.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Raw BNC dataset\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Cleaned BNC dataset\n",
        "        \"\"\"\n",
        "        logger.info(\"Cleaning BNC concordance data for analysis compatibility\")\n",
        "        original_count = len(df)\n",
        "\n",
        "        # Remove entries with insufficient sentence context\n",
        "        df = df.dropna(subset=['Sentence_Context'])\n",
        "        df = df[df['Sentence_Context'].str.len() >= 10]\n",
        "\n",
        "        # Clean text formatting\n",
        "        df['Sentence_Context'] = df['Sentence_Context'].str.strip()\n",
        "        df['Sentence_Context'] = df['Sentence_Context'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "        # Validate category assignments\n",
        "        df = df[df['Category_Framework'].notna()]\n",
        "\n",
        "        final_count = len(df)\n",
        "        logger.info(f\"BNC data cleaning completed: {original_count} -> {final_count} instances\")\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "ihw-NHdve18s"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}